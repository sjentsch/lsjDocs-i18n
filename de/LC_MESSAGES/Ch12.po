# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi)  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi)  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.po (Learning statistics with jamovi )  #-#-#-#-#
# #-#-#-#-#  Ch12.pot (Learning statistics with jamovi )  #-#-#-#-#
# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-12-11 16:44+0100\n"
"PO-Revision-Date: 2022-12-13 15:55+0000\n"
"Last-Translator: Sebastian Jentschke <sebastian.jentschke@uib.no>\n"
"Language-Team: German <https://hosted.weblate.org/projects/lsjdocs/ch12/de/>"
"\n"
"Language: de\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=n != 1;\n"
"X-Generator: Weblate 4.15-dev\n"
"Generated-By: Babel 2.10.3\n"

#: ../../Ch12/Ch12_Regression.rst:4
msgid "Correlation and linear regression"
msgstr "Korrelation und lineare Regression"

#: ../../Ch12/Ch12_Regression.rst:24
msgid ""
"The goal in this chapter is to introduce **correlation** and **linear "
"regression**. These are the standard tools that statisticians rely on when "
"analysing the relationship between continuous predictors |continuous| and "
"continuous outcomes |continuous|."
msgstr ""
"Ziel dieses Kapitels ist es, die **Korrelation** und die **lineare "
"Regression** vorzustellen. Dies sind die Standardinstrumente, auf die sich "
"Statistiker bei der Analyse der Beziehung zwischen kontinuierlichen "
"Prädiktoren und kontinuierlichen Ergebnissen verlassen."

#: ../../Ch12/Ch12_Regression.rst:31 ../../Ch12/Ch12_Regression_01.rst:420
msgid "continuous"
msgstr "continuous"

#: ../../Ch12/Ch12_Regression_01.rst:4
msgid "Correlations"
msgstr "Korrelationen"

#: ../../Ch12/Ch12_Regression_01.rst:6
msgid ""
"In this section we’ll talk about how to describe the relationships *between* "
"variables in the data. To do that, we want to talk mostly about the "
"**correlation** between variables. But first, we need some data."
msgstr ""
"In diesem Abschnitt werden wir darüber sprechen, wie die Beziehungen "
"*zwischen* Variablen in den Daten beschrieben werden können. Zu diesem Zweck "
"wollen wir vor allem über die **Korrelation** zwischen den Variablen "
"sprechen. Aber zuerst brauchen wir einige Daten."

#: ../../Ch12/Ch12_Regression_01.rst:12
msgid "The data"
msgstr "Die Daten"

#: ../../Ch12/Ch12_Regression_01.rst:14
msgid "Descriptive statistics for the |parenthood|_ data set."
msgstr "Deskriptivstatistik für den Datensatz |parenthood|_."

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "Variable"
msgstr "Variable"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "min"
msgstr "Min."

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "max"
msgstr "Max."

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "mean"
msgstr "Mittelwert"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "median"
msgstr "Median"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "std. dev"
msgstr "Std.-abw."

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "IQR"
msgstr "IQR"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "**Dani’s grumpiness**"
msgstr "**Danis Miesepetrigkeit**"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "41"
msgstr "41"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "91"
msgstr "91"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "63.71"
msgstr "63.71"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "62"
msgstr "62"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "10.05"
msgstr "10.05"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "14"
msgstr "14"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "**Dani’s hours slept**"
msgstr "**Dani's Stunden mit Schlaf**"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "4.84"
msgstr "4.84"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "9.00"
msgstr "9.00"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "6.97"
msgstr "6.97"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "7.03"
msgstr "7.03"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "1.02"
msgstr "1.02"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "1.45"
msgstr "1.45"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "**Dani’s son’s hours slept**"
msgstr "**Stunden mit Schlaf von Dani's Sohn**"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "3.25"
msgstr "3.25"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "12.07"
msgstr "12.07"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "8.05"
msgstr "8.05"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "7.95"
msgstr "7.95"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "2.07"
msgstr "2.07"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "3.21"
msgstr "3.21"

#: ../../Ch12/Ch12_Regression_01.rst:27
msgid ""
"Let’s turn to a topic close to every parent’s heart: sleep. The data set "
"we’ll use is fictitious, but based on real events. Suppose I’m curious to "
"find out how much my infant son’s sleeping habits affect my mood. Let’s say "
"that I can rate my grumpiness very precisely, on a scale from 0 (not at all "
"grumpy) to 100 (grumpy as a very, very grumpy old man or woman). And lets "
"also assume that I’ve been measuring my grumpiness, my sleeping patterns and "
"my son’s sleeping patterns for 100 days. The data are stored in the |"
"parenthood|_ data set, that contains four variables ``dani.sleep``, ``baby."
"sleep``, ``dani.grump`` and ``day``."
msgstr ""
"Wenden wir uns einem Thema zu, das allen Eltern am Herzen liegt: dem Schlaf. "
"Der Datensatz, den wir verwenden werden, ist fiktiv, basiert aber auf realen "
"Ereignissen. Angenommen, ich möchte herausfinden, wie sehr die "
"Schlafgewohnheiten meines kleinen Sohnes meine Stimmung beeinflussen. Nehmen "
"wir an, ich kann meine Miesepetrigkeit auf einer Skala von 0 (überhaupt "
"nicht miesepetrig) bis 100 (so miesepetrig wie sehr, sehr miesepetrige "
"ältere Menschen) sehr genau einschätzen. Und nehmen wir weiter an, dass ich "
"meine Miesepetrigkeit, mein Schlafverhalten und das Schlafverhalten meines "
"Sohnes 100 Tage lang gemessen habe. Die Daten sind im Datensatz |parenthood|"
"_ gespeichert, der vier Variablen enthält: ``dani.sleep``, ``baby.sleep``, "
"``dani.grump`` und ``day``."

#: ../../Ch12/Ch12_Regression_01.rst:37
msgid ""
"Next, I’ll take a look at some basic descriptive statistics and, to give a "
"graphical depiction of what each of the three interesting variables looks "
"like, :numref:`fig-grumpHist` plots histograms. One thing to note: just "
"because jamovi can calculate dozens of different statistics doesn’t mean you "
"should report all of them. If I were writing this up for a report, I’d "
"probably pick out those statistics that are of most interest to me (and to "
"my readership), and then put them into a nice, simple table like the one in :"
"numref:`tab-parenthood`.\\ [#]_ Notice that when I put it into a table, I "
"gave everything “human readable” names. This is always good practice. Notice "
"also that I’m not getting enough sleep. This isn’t good practice, but other "
"parents tell me that it’s pretty standard."
msgstr ""
"Als Nächstes werfe ich einen Blick auf einige grundlegende deskriptive "
"Statistiken und, um eine grafische Darstellung zu geben, wie jede der drei "
"interessanten Variablen aussieht, zeigt :numref:`fig-grumpHist` Histogramme. "
"Eine Anmerkung: Nur weil jamovi Dutzende verschiedener Statistiken berechnen "
"kann, heißt das nicht, dass man sie alle ausgeben sollte. Würde ich über "
"diese Daten berichten wollen, würde ich wahrscheinlich die Statistiken "
"auswählen, die für mich (und meine angenommene Leserschaft) am "
"interessantesten sind, und sie dann in eine einfache Tabelle wie :numref:"
"`tab-parenthood`einfügen.\\ [#]_ Das ist immer eine gute Vorgehensweise. "
"Beachten Sie auch, dass ich nicht genug Schlaf bekomme. Das ist nicht "
"empfehlenswert, aber andere Eltern sagen mir, dass das ziemlich normal ist."

#: ../../Ch12/Ch12_Regression_01.rst:55
msgid "Histograms for three variables from the |parenthood| data set"
msgstr "Histogramme für drei Variablen aus dem Datensatz |parenthood|"

#: ../../Ch12/Ch12_Regression_01.rst:55
msgid ""
"Histograms for the three interesting variables in the |parenthood|_ data set"
msgstr ""
"Histogramme für die drei interessierenden Variablen aus dem Datensatz |"
"parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:61
msgid "The strength and direction of a relationship"
msgstr "Die Stärke und Richtung einer Beziehung"

#: ../../Ch12/Ch12_Regression_01.rst:63
msgid ""
"We can draw scatterplots to give us a general sense of how closely related "
"two variables are. Ideally though, we might want to say a bit more about it "
"than that. For instance, let’s compare the relationship between ``dani."
"sleep`` and ``dani.grump`` (:numref:`fig-grumpCor1`, left) with that between "
"``baby.sleep`` and ``dani.grump`` (:numref:`fig-grumpCor1`, right). When "
"looking at these two plots side by side, it’s clear that the relationship is "
"*qualitatively* the same in both cases: more sleep equals less grump! "
"However, it’s also pretty obvious that the relationship between ``dani."
"sleep`` and ``dani.grump`` is *stronger* than the relationship between "
"``baby.sleep`` and ``dani.grump``. The plot on the left is “neater” than the "
"one on the right. What it feels like is that if you want to predict what my "
"mood is, it’d help you a little bit to know how many hours my son slept, but "
"it’d be more helpful to know how many hours I slept."
msgstr ""
"Wir können Streudiagramme zeichnen, die uns einen allgemeinen Eindruck davon "
"vermitteln, wie eng zwei Variablen miteinander zusammenhängen. Idealerweise "
"sollten wir jedoch etwas mehr darüber aussagen, als das. Vergleichen wir zum "
"Beispiel die Beziehung zwischen ``dani.sleep`` und ``dani.grump`` (:numref:"
"`fig-grumpCor1`, links) mit der zwischen ``baby.sleep`` und ``dani.grump`` (:"
"numref:`fig-grumpCor1`, rechts). Wenn man sich diese beiden Diagramme "
"nebeneinander ansieht, wird deutlich, dass die Beziehung *qualitativ* in "
"beiden Fällen die gleiche ist: mehr Schlaf bedeutet weniger Miesepetrigkeit! "
"Es ist jedoch auch ziemlich offensichtlich, dass die Beziehung zwischen "
"``dani.sleep`` und ``dani.grump`` *stärker* ist als die Beziehung zwischen "
"``baby.sleep`` und ``dani.grump``. Das Diagramm auf der linken Seite ist "
"„ordentlicher“ als das auf der rechten Seite. Wenn Sie meine Stimmung "
"vorhersagen wollen, hilft es Ihnen ein wenig, wenn Sie wissen, wie viele "
"Stunden mein Sohn geschlafen hat. Es ist stattdessen hilfreicher, wenn Sie "
"wissen, wie viele Stunden ich geschlafen habe."

#: ../../Ch12/Ch12_Regression_01.rst:84
msgid ""
"Scatterplots between ``dani.sleep`` and ``baby.sleep`` to ``dani.grump``"
msgstr ""
"Streudiagramme zwischen ``dani.sleep`` bzw. ``baby.sleep`` und ``dani.grump``"

#: ../../Ch12/Ch12_Regression_01.rst:84
msgid ""
"Scatterplots showing the relationship between ``dani.sleep`` and ``dani."
"grump`` (left panel) and the relationship between ``baby.sleep`` and ``dani."
"grump`` (right panel)."
msgstr ""
"Streudiagramme, die das Verhältnis zwischen ``dani.sleep`` und ``dani."
"grump`` (links) und das Verhältnis zwischen ``baby.sleep`` und ``dani."
"grump`` (rechts) zeigen."

#: ../../Ch12/Ch12_Regression_01.rst:90
msgid ""
"In contrast, let’s consider the two scatterplots shown in :numref:`fig-"
"grumpCor2`. If we compare the scatterplot of ``baby.sleep`` vs. ``dani."
"grump`` (left) to the scatterplot of ``baby.sleep`` vs. ``dani.sleep`` "
"(right), the overall strength of the relationship is the same, but the "
"direction is different. That is, if my son sleeps more, I get *more* sleep "
"(positive relationship, right hand side), but if he sleeps more then I get "
"*less* grumpy (negative relationship, left hand side)."
msgstr ""
"Betrachten wir im Gegensatz dazu die beiden in :numref:`fig-grumpCor2` "
"dargestellten Streudiagramme. Wenn wir das Streudiagramm von ``baby.sleep`` "
"vs. ``dani.grump`` (links) mit dem Streudiagramm von ``baby.sleep`` vs. "
"``dani.sleep`` (rechts) vergleichen, ist die Gesamtstärke der Beziehung die "
"gleiche, aber die Richtung ist anders. Das heißt, wenn mein Sohn mehr "
"schläft, bekomme ich *mehr* Schlaf (positive Beziehung, rechts). "
"Gleichzeitig bin ich *weniger* miesepetrig, wenn er mehr schläft (negative "
"Beziehung, links)."

#: ../../Ch12/Ch12_Regression_01.rst:105
msgid "Scatterplots between baby.sleep to dani.grump and dani.sleep"
msgstr ""
"Streudiagramme zwischen ``baby.sleep`` zu ``dani.grump`` bzw. ``dani.sleep``"

#: ../../Ch12/Ch12_Regression_01.rst:105
msgid ""
"Scatterplots showing the relationship between ``baby.sleep`` and ``dani."
"grump`` (left panel) and the relationship between ``baby.sleep`` and ``dani."
"sleep`` (right panel)."
msgstr ""
"Streudiagramme, die das Verhältnis zwischen ``baby.sleep`` und ``dani."
"grump`` (links) sowie das Verhältnis zwischen ``baby.sleep`` und ``dani."
"sleep`` (rechts) zeigen."

#: ../../Ch12/Ch12_Regression_01.rst:112
msgid "The correlation coefficient"
msgstr "Der Korrelationskoeffizient"

#: ../../Ch12/Ch12_Regression_01.rst:114
msgid ""
"We can make these ideas a bit more explicit by introducing the idea of a "
"**correlation coefficient** (or, more specifically, Pearson’s correlation "
"coefficient), which is traditionally denoted as *r*. The correlation "
"coefficient between two variables *X* and *Y* (sometimes denoted r\\ :sub:"
"`XY`), which we’ll define more precisely in the next section, is a measure "
"that varies from -1 to 1. When *r* = -1 it means that we have a perfect "
"negative relationship, and when *r* = 1 it means we have a perfect positive "
"relationship. When *r* = 0, there’s no relationship at all. If you look at :"
"numref:`fig-corr`, you can see several plots showing what different "
"correlations look like."
msgstr ""
"Wir können diese Ideen etwas deutlicher machen, indem wir die Idee eines "
"**Korrelationskoeffizienten** (oder, genauer gesagt, des Pearsonschen "
"Korrelationskoeffizienten) einführen, der traditionell als *r* bezeichnet "
"wird. Der Korrelationskoeffizient zwischen zwei Variablen *X* und *Y* "
"(manchmal als r\\ :sub:`XY` bezeichnet), den wir im nächsten Abschnitt "
"genauer definieren werden, ist ein Maß, das von -1 bis 1 variiert. Wenn *r* "
"= -1 ist, bedeutet dies, dass wir eine perfekte negative Beziehung haben, "
"und wenn *r* = 1 ist, bedeutet dies, dass wir eine perfekte positive "
"Beziehung haben. Wenn *r* = 0 ist, gibt es überhaupt keine Beziehung. Unter :"
"numref:`fig-corr` finden Sie mehrere Diagramme, die zeigen, wie die "
"verschiedenen Korrelationen aussehen können."

#: ../../Ch12/Ch12_Regression_01.rst:130
msgid "Effect of varying the strength and direction of a correlation"
msgstr "Auswirkung der Variation von Stärke und Richtung einer Korrelation"

#: ../../Ch12/Ch12_Regression_01.rst:130
msgid ""
"Illustration of the effect of varying the strength and direction of a "
"correlation. In the left hand column, the correlations are 0.00, 0.33, 0.67 "
"and 1.00 In the right hand column, the correlations are 0.00, -0.33, -0.67 "
"and -1.00."
msgstr ""
"Veranschaulichung der Auswirkungen einer Änderung der Stärke und Richtung "
"einer Korrelation. In der linken Spalte sind die Korrelationen 0,00, 0,33, "
"0,67 und 1,00 dargestellt, in der rechten Spalte die Korrelationen 0,00, "
"-0,33, -0,67 und -1,00."

#: ../../Ch12/Ch12_Regression_01.rst:137
msgid ""
"The formula for the Pearson’s correlation coefficient can be written in "
"several different ways. I think the simplest way to write down the formula "
"is to break it into two steps. Firstly, let’s introduce the idea of a "
"**covariance**. The covariance between two variables *X* and *Y* is a "
"generalisation of the notion of the variance amd is a mathematically simple "
"way of describing the relationship between two variables that isn’t terribly "
"informative to humans"
msgstr ""
"Die Formel für den Pearsonschen Korrelationskoeffizienten kann auf "
"verschiedene Arten geschrieben werden. Meiner Meinung nach ist es am "
"einfachsten, die Formel in zwei Schritte aufzuteilen. Führen wir zunächst "
"die Idee einer **Kovarianz** ein. Die Kovarianz zwischen zwei Variablen *X* "
"und *Y* ist eine Verallgemeinerung des Begriffs der Varianz und stellt eine "
"mathematisch einfache Möglichkeit dar, die Beziehung zwischen zwei Variablen "
"zu beschreiben, die für den Menschen nicht sehr informativ ist"

#: ../../Ch12/Ch12_Regression_01.rst:145
msgid ""
"\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(X_i - \\bar{X} "
"\\right) \\left(Y_i - \\bar{Y} \\right)\n"
"\n"
msgstr ""
"\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(X_i - \\bar{X} "
"\\right) \\left(Y_i - \\bar{Y} \\right)\n"
"\n"

#: ../../Ch12/Ch12_Regression_01.rst:147
msgid ""
"Because we’re multiplying (i.e., taking the “product” of) a quantity that "
"depends on *X* by a quantity that depends on *Y* and then averaging,\\ [#]_ "
"you can think of the formula for the covariance as an “average cross "
"product” between *X* and *Y*."
msgstr ""
"Da wir eine Größe, die von *X* abhängt, mit einer Größe, die von *Y* "
"abhängt, multiplizieren (d. h. das „Produkt“ davon nehmen) und dann den "
"Mittelwert bilden, [#]_ können Sie sich die Formel für die Kovarianz als "
"„durchschnittliches Kreuzprodukt“ zwischen *X* und *Y* vorstellen."

#: ../../Ch12/Ch12_Regression_01.rst:152
msgid ""
"The covariance has the nice property that, if *X* and *Y* are entirely "
"unrelated, then the covariance is exactly zero. If the relationship between "
"them is positive (in the sense shown in :numref:`fig-corr`) then the "
"covariance is also positive, and if the relationship is negative then the "
"covariance is also negative. In other words, the covariance captures the "
"basic qualitative idea of correlation. Unfortunately, the raw magnitude of "
"the covariance isn’t easy to interpret as it depends on the units in which "
"*X* and *Y* are expressed and, worse yet, the actual units that the "
"covariance itself is expressed in are really weird. For instance, if *X* "
"refers to the ``dani.sleep`` variable (units: hours) and *Y* refers to the "
"``dani.grump`` variable (units: grumps), then the units for their covariance "
"are “hours × grumps”. And I have no freaking idea what that would even mean."
msgstr ""
"Die Kovarianz hat die nützliche Eigenschaft, dass die Kovarianz genau Null "
"ist, wenn *X* und *Y* völlig unabhängig voneinander sind. Wenn die Beziehung "
"zwischen ihnen positiv ist (im Sinne von :numref:`fig-corr`), dann ist auch "
"die Kovarianz positiv, und wenn die Beziehung negativ ist, dann ist auch die "
"Kovarianz negativ. Mit anderen Worten: Die Kovarianz gibt die grundlegende "
"qualitative Idee der Korrelation wieder. Leider ist die Kovarianz für "
"Rohdaten nicht einfach zu interpretieren, da sie von den Einheiten abhängt, "
"in denen *X* und *Y* ausgedrückt werden. Schlimmer ist, dass die "
"tatsächlichen Einheiten, in denen die Kovarianz ausgedrückt wird, wirklich "
"seltsam sind. Wenn sich zum Beispiel *X* auf die Variable ``dani.sleep`` "
"(Einheit: Stunden) und *Y* auf die Variable ``dani.grump`` (Einheit: "
"Miesepeter) bezieht, dann sind die Einheiten für ihre Kovarianz \"Stunden × "
"Miesepeter\". Und ich habe keinen blassen Schimmer, was das überhaupt "
"bedeuten soll."

#: ../../Ch12/Ch12_Regression_01.rst:165
msgid ""
"The Pearson correlation coefficient *r* fixes this interpretation problem by "
"standardising the covariance, in pretty much the exact same way that the *z*-"
"score standardises a raw score, by dividing by the standard deviation. "
"However, because we have two variables that contribute to the covariance, "
"the standardisation only works if we divide by both standard deviations.\\ "
"[#]_ In other words, the correlation between *X* and *Y* can be written as "
"follows:"
msgstr ""
"Der Pearson-Korrelationskoeffizient *r* behebt dieses "
"Interpretationsproblem, indem er die Kovarianz standardisiert, und zwar auf "
"ziemlich genau dieselbe Weise, wie der *z*-Score eine Rohdaten-Variable "
"standardisiert, indem er durch die Standardabweichung dividiert wird. Da wir "
"jedoch zwei Variablen haben, die zur Kovarianz beitragen, funktioniert die "
"Standardisierung nur, wenn wir durch beide Standardabweichungen dividieren."
"\\ [#]_ Das bedeutet, dass die Korrelation zwischen *X* und *Y* wie folgt "
"geschrieben werden kann:"

#: ../../Ch12/Ch12_Regression_01.rst:173
msgid ""
"r_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n"
"\n"
msgstr ""
"r_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n"
"\n"

#: ../../Ch12/Ch12_Regression_01.rst:175
msgid ""
"By standardising the covariance, not only do we keep all of the nice "
"properties of the covariance discussed earlier, but the actual values of *r* "
"are on a meaningful scale: *r* = 1 implies a perfect positive relationship "
"and *r* = -1 implies a perfect negative relationship. I’ll expand a little "
"more on this point later, in section :doc:`Interpreting a correlation <../"
"Ch12/Ch12_Regression_01>`. But before I do, let’s look at how to calculate "
"correlations in jamovi."
msgstr ""
"Durch die Standardisierung der Kovarianz bleiben nicht nur die zuvor "
"besprochenen nützlichen Eigenschaften der Kovarianz erhalten, sondern die "
"tatsächlichen Werte für *r* liegen auf einer sinnvollen Skala: *r* = 1 "
"bedeutet eine perfekte positive Beziehung und *r* = -1 bedeutet eine "
"perfekte negative Beziehung. Auf diesen Punkt werde ich später im Abschnitt :"
"doc:`Interpretation einer Korrelation <../Ch12/Ch12_Regression_01>` noch "
"etwas näher eingehen. Zuvor wollen wir uns jedoch ansehen, wie Korrelationen "
"in jamovi berechnet werden."

#: ../../Ch12/Ch12_Regression_01.rst:183
msgid "Calculating correlations in jamovi"
msgstr "Berechnen von Korrelationen in jamovi"

#: ../../Ch12/Ch12_Regression_01.rst:185
msgid ""
"Calculating correlations in jamovi can be done by clicking on the "
"``Regression`` → ``Correlation Matrix`` button. Transfer all four continuous "
"variables |continuous| across into the box on the right to get the output "
"in :numref:`fig-correlations`."
msgstr ""
"Das Berechnen von Korrelationen in jamovi erfolgt durch Klicken auf die "
"Schaltfläche ``Regression`` → ``Correlation Matrix``. Übertragen Sie alle "
"vier kontinuierlichen Variablen |continuous| in das Feld auf der rechten "
"Seite, um die Ausgabe in :numref:`fig-correlations` zu erhalten."

#: ../../Ch12/Ch12_Regression_01.rst:196
msgid "jamovi screenshot with correlations in the |parenthood| data set"
msgstr "jamovi-Screenshot mit Korrelationen aus dem Datensatz |parenthood|"

#: ../../Ch12/Ch12_Regression_01.rst:196
msgid ""
"jamovi screenshot showing correlations between variables in the |parenthood|"
"_ data set"
msgstr ""
"jamovi-Screenshot mit den Korrelationen zwischen den Variablen im Datensatz |"
"parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:202
msgid "Interpreting a correlation"
msgstr "Interpretieren einer Korrelation"

#: ../../Ch12/Ch12_Regression_01.rst:204
msgid ""
"Naturally, in real life you don’t see many correlations of 1. So how should "
"you interpret a correlation of, say, *r* = 0.4? The honest answer is that it "
"really depends on what you want to use the data for, and on how strong the "
"correlations in your field tend to be. A friend of mine in engineering once "
"argued that any correlation less than 0.95 is completely useless (I think he "
"was exaggerating, even for engineering). On the other hand, there are real "
"cases, even in psychology, where you should really expect correlations that "
"strong. For instance, one of the benchmark data sets used to test theories "
"of how people judge similarities is so clean that any theory that can’t "
"achieve a correlation of at least 0.9 really isn’t deemed to be successful. "
"However, when looking for (say) elementary correlates of intelligence (e.g., "
"inspection time, response time), if you get a correlation above 0.3 you’re "
"doing very very well. In short, the interpretation of a correlation depends "
"a lot on the context. That said, the rough guide in :numref:`tab-"
"interpretcorrelations` is pretty typical."
msgstr ""
"Natürlich gibt es im wirklichen Leben nicht viele Korrelationen von 1. Wie "
"sollten Sie also eine Korrelation von, sagen wir, *r* = 0,4 interpretieren? "
"Die ehrliche Antwort ist, dass es wirklich davon abhängt, wofür Sie die "
"Daten verwenden wollen und wie stark die Korrelationen in Ihrem "
"Arbeitsbereich typischerweise sind. Ein befreundeter Ingenieur behauptete "
"einmal, dass jede Korrelation unter 0,95 völlig unbrauchbar ist (ich glaube, "
"er hat übertrieben, selbst für den Ingenieurbereich). Andererseits gibt es "
"selbst in der Psychologie Fälle, in denen man wirklich so starke "
"Korrelationen erwarten sollte. So ist zum Beispiel einer der Benchmark-"
"Datensätze, mit denen Theorien darüber getestet werden, wie Menschen "
"Ähnlichkeiten beurteilen, so sauber, dass jede Theorie, die nicht mindestens "
"eine Korrelation von 0,9 erreicht, als nicht erfolgreich gilt. Wenn man "
"jedoch nach (sagen wir) elementaren Korrelaten der Intelligenz sucht (z. B. "
"Inspektionszeit, Reaktionszeit), ist man sehr gut dran, wenn man eine "
"Korrelation von über 0,3 erreicht. Kurz gesagt, die Interpretation einer "
"Korrelation hängt stark vom Kontext ab. Die groben Richtwerte in :numref:"
"`tab-interpretcorrelations` sind jedoch ein guter Ausgangspunkt."

#: ../../Ch12/Ch12_Regression_01.rst:220
msgid ""
"A rough guide to interpreting correlations. Note that I say a *rough* guide. "
"There aren’t hard and fast rules for what counts as strong or weak "
"relationships. It depends on the context."
msgstr ""
"Ein grober Leitfaden für die Interpretation von Korrelationen. Beachten Sie, "
"dass ich sage, ein *grober* Leitfaden. Es gibt keine festen Regeln dafür, "
"was als starke oder schwache Beziehung gilt. Es kommt auf den Kontext an."

#: ../../Ch12/Ch12_Regression_01.rst:226
msgid "Correlation"
msgstr "Korrelation"

#: ../../Ch12/Ch12_Regression_01.rst:226
msgid "Strength"
msgstr "Stärke"

#: ../../Ch12/Ch12_Regression_01.rst:226
msgid "Direction"
msgstr "Richtung"

#: ../../Ch12/Ch12_Regression_01.rst:228
msgid "-1.0 to -0.9"
msgstr "-1,0 bis -0,9"

#: ../../Ch12/Ch12_Regression_01.rst:228 ../../Ch12/Ch12_Regression_01.rst:246
msgid "Very strong"
msgstr "Sehr stark"

#: ../../Ch12/Ch12_Regression_01.rst:228 ../../Ch12/Ch12_Regression_01.rst:230
#: ../../Ch12/Ch12_Regression_01.rst:232 ../../Ch12/Ch12_Regression_01.rst:234
#: ../../Ch12/Ch12_Regression_01.rst:236
msgid "Negative"
msgstr "Negativ"

#: ../../Ch12/Ch12_Regression_01.rst:230
msgid "-0.9 to -0.7"
msgstr "-0,9 bis -0,7"

#: ../../Ch12/Ch12_Regression_01.rst:230 ../../Ch12/Ch12_Regression_01.rst:244
msgid "Strong"
msgstr "Stark"

#: ../../Ch12/Ch12_Regression_01.rst:232
msgid "-0.7 to -0.4"
msgstr "-0,7 bis -0,4"

#: ../../Ch12/Ch12_Regression_01.rst:232 ../../Ch12/Ch12_Regression_01.rst:242
msgid "Moderate"
msgstr "Mäßig"

#: ../../Ch12/Ch12_Regression_01.rst:234
msgid "-0.4 to -0.2"
msgstr "-0,4 bis -0,2"

#: ../../Ch12/Ch12_Regression_01.rst:234 ../../Ch12/Ch12_Regression_01.rst:240
msgid "Weak"
msgstr "Schwach"

#: ../../Ch12/Ch12_Regression_01.rst:236
msgid "-0.2 to  0.0"
msgstr "-0,2 bis 0,0"

#: ../../Ch12/Ch12_Regression_01.rst:236 ../../Ch12/Ch12_Regression_01.rst:238
msgid "Negligible"
msgstr "Unbedeutend"

#: ../../Ch12/Ch12_Regression_01.rst:238
msgid "0.0 to  0.2"
msgstr "0,0 bis 0,2"

#: ../../Ch12/Ch12_Regression_01.rst:238 ../../Ch12/Ch12_Regression_01.rst:240
#: ../../Ch12/Ch12_Regression_01.rst:242 ../../Ch12/Ch12_Regression_01.rst:244
#: ../../Ch12/Ch12_Regression_01.rst:246
msgid "Positive"
msgstr "Positiv"

#: ../../Ch12/Ch12_Regression_01.rst:240
msgid "0.2 to  0.4"
msgstr "0,2 bis 0,4"

#: ../../Ch12/Ch12_Regression_01.rst:242
msgid "0.4 to  0.7"
msgstr "0,4 bis 0,7"

#: ../../Ch12/Ch12_Regression_01.rst:244
msgid "0.7 to  0.9"
msgstr "0,7 bis 0,9"

#: ../../Ch12/Ch12_Regression_01.rst:246
msgid "0.9 to  1.0"
msgstr "0,9 bis 1,0"

#: ../../Ch12/Ch12_Regression_01.rst:256
msgid "Anscombe’s quartet"
msgstr "Anscombes Quartett"

#: ../../Ch12/Ch12_Regression_01.rst:256
msgid ""
"Anscombe’s quartet: All four of these data sets have a Pearson correlation "
"of *r* = 0.816, but they are qualitatively different from one another."
msgstr ""
"Das Anscombe-Quartett: Alle vier Variablenpaare weisen eine Pearson-"
"Korrelation von *r* = 0,816 auf, unterscheiden sich aber qualitativ deutlich "
"voneinander."

#: ../../Ch12/Ch12_Regression_01.rst:261
msgid ""
"However, something that can never be stressed enough is that you should "
"*always* look at the scatterplot before attaching any interpretation to the "
"data. A correlation might not mean what you think it means. The classic "
"illustration of this is “Anscombe’s Quartet” (`Anscombe, 1973 <../Other/"
"References.html#anscombe-1973>`__\\ ), a collection of four data sets. Each "
"data set has two variables, an *X* and a *Y*. For all four data sets the "
"mean value for *X* is 9 and the mean for *Y* is 7.5. The standard deviations "
"for all *X* variables are almost identical, as are those for the *Y* "
"variables. And in each case the correlation between *X* and *Y* is *r* = "
"0.816`. You can verify this yourself, since I happen to have saved it as a "
"dataset called |anscombe|_."
msgstr ""
"Man kann nicht oft genug betonen, dass man sich *immer* das Streudiagramm "
"ansehen sollte, bevor man seine Daten interpretiert. Eine Korrelation "
"bedeutet möglicherweise nicht das, was Sie denken. Das klassische Beispiel "
"hierfür ist das „Anscombe-Quartett“ (`Anscombe, 1973 <../Other/References."
"html#anscombe-1973>`__\\ ), eine Sammlung von vier Variablenpaaren, jeweils "
"*X* und *Y*. Für alle vier Variablenpaare beträgt der Mittelwert für *X* = 9 "
"und der Mittelwert für *Y* = 7,5. Die Standardabweichung ist für alle *X*-"
"Variablen fast identisch, ebenso wie die für die *Y* Variablen. Und in jedem "
"Fall beträgt die Korrelation zwischen *X* und *Y* * r* = 0,816`. Sie können "
"dies selbst überprüfen, indem Sie den Datensatz |anscombe|_ benutzen."

#: ../../Ch12/Ch12_Regression_01.rst:272
msgid ""
"You’d think that these four data sets would look pretty similar to one "
"another. They do not. If we draw scatterplots of *X* against *Y* for all "
"four variables, as shown in :numref:`fig-anscombe`, we see that all four of "
"these are *spectacularly* different to each other. The lesson here, which so "
"very many people seem to forget in real life, is *always graph your raw "
"data* (chapter :doc:`Drawing graphs <../Ch05/Ch05_Graphics>`)."
msgstr ""
"Man sollte meinen, dass diese vier Datensätze einander ziemlich ähnlich "
"sehen. Das tun sie aber nicht. Wenn wir die Streudiagramme *X* gegen *Y* für "
"alle vier Variablenpaare zeichnen, wie in :numref:`fig-anscombe` gezeigt, "
"sehen wir, dass sich alle vier *deutlich* voneinander unterscheiden. Die "
"Lektion hier, die so viele Leute im wirklichen Leben zu vergessen scheinen, "
"ist *Ihre Rohdaten immer grafisch darzustellen* (Kapitel :doc:`Diagramme "
"erstellen <../Ch05/Ch05_Graphics>`)."

#: ../../Ch12/Ch12_Regression_01.rst:280
msgid "Spearman’s rank correlations"
msgstr "Rangkorrelationen nach Spearman"

#: ../../Ch12/Ch12_Regression_01.rst:282
msgid ""
"The Pearson correlation coefficient is useful for a lot of things, but it "
"does have shortcomings. One issue in particular stands out: what it actually "
"measures is the strength of the *linear* relationship between two variables. "
"In other words, what it gives you is a measure of the extent to which the "
"data all tend to fall on a single, perfectly straight line. Often, this is a "
"pretty good approximation to what we mean when we say “relationship”, and so "
"the Pearson correlation is a good thing to calculate. Sometimes though, it "
"isn’t."
msgstr ""
"Der Pearson-Korrelationskoeffizient ist in vielerlei Hinsicht nützlich, hat "
"aber auch seine Schwächen. Ein Problem fällt besonders auf: Er misst die "
"Stärke der *linearen* Beziehung zwischen zwei Variablen. Mit anderen Worten, "
"es ist ein Maß dafür, inwieweit die Daten alle auf eine einzige, perfekt "
"gerade Linie fallen. Oft ist dies eine ziemlich gute Annäherung an das, was "
"wir meinen, wenn wir von „Beziehung“ sprechen, und daher ist das Berechnen "
"einer Pearson-Korrelation oft eine gute Idee. Manchmal ist dies jedoch nicht "
"der Fall."

#: ../../Ch12/Ch12_Regression_01.rst:291
msgid ""
"One very common situation where the Pearson correlation isn’t quite the "
"right thing to use arises when an increase in one variable *X* really is "
"reflected in an increase in another variable *Y*, but the nature of the "
"relationship isn’t necessarily linear. An example of this might be the "
"relationship between effort and reward when studying for an exam. If you put "
"zero effort (*X*) into learning a subject then you should expect a grade of "
"0\\% (*Y*). However, a little bit of effort will cause a *massive* "
"improvement. Just turning up to lectures means that you learn a fair bit, "
"and if you just turn up to classes and scribble a few things down your grade "
"might rise to 35\\%, all without a lot of effort. However, you just don’t "
"get the same effect at the other end of the scale. As everyone knows, it "
"takes *a lot* more effort to get a grade of 90\\% than it takes to get a "
"grade of 55\\%. What this means is that, if I’ve got data looking at study "
"effort and grades, there’s a pretty good chance that Pearson correlations "
"will be misleading."
msgstr ""
"Eine sehr häufige Situation, in der die Pearson-Korrelation nicht ganz das "
"Richtige ist, ergibt sich, wenn ein Anstieg einer Variablen *X* sich "
"tatsächlich in einem Anstieg einer anderen Variablen *Y* widerspiegelt, aber "
"die Art der Beziehung nicht unbedingt linear ist. Ein Beispiel hierfür wäre "
"die Beziehung zwischen Aufwand und Belohnung beim Lernen für eine Prüfung. "
"Wenn Sie sich überhaupt nicht anstrengen (*X*), um ein Fach zu lernen, "
"sollten Sie 0 % als Note erwarten (*Y*). Ein wenig Anstrengung wird jedoch "
"eine *massive* Verbesserung bewirken. Wenn Sie einfach nur zu den "
"Vorlesungen kommen, lernen Sie schon eine ganze Menge, und wenn Sie einfach "
"nur zum Unterricht erscheinen und ein paar Dinge aufschreiben, könnte Ihre "
"Note auf 35 % steigen, und das alles ohne große Anstrengung. Am anderen Ende "
"der Skala hat man jedoch nicht denselben Effekt. Wie jeder weiß, braucht es "
"*viel* mehr Aufwand, um eine Note von 90 % zu erreichen, als für eine Note "
"von 55 %. Das bedeutet, dass bei Daten, die sich mit Lernaufwand und Noten "
"befassen, die Wahrscheinlichkeit groß ist, dass Pearson-Korrelationen "
"irreführend sind."

#: ../../Ch12/Ch12_Regression_01.rst:308
msgid ""
"To illustrate, consider the data plotted in :numref:`fig-"
"ordinalRelationship`, showing the relationship between hours worked and "
"grade received for 10 students taking some class. The curious thing about "
"this (highly fictitious) data set is that increasing your effort *always* "
"increases your grade. It might be by a lot or it might be by a little, but "
"increasing effort will never decrease your grade. If we run a standard "
"Pearson correlation, it shows a strong relationship between hours worked and "
"grade received, with a correlation coefficient of **0.91**. However, this "
"doesn’t actually capture the observation that increasing hours worked "
"*always* increases the grade. There’s a sense here in which we want to be "
"able to say that the correlation is *perfect* but for a somewhat different "
"notion of what a “relationship” is. What we’re looking for is something that "
"captures the fact that there is a perfect **ordinal relationship** here. "
"That is, if student 1 works more hours than student 2, then we can guarantee "
"that student 1 will get the better grade. That’s not what a correlation of "
"*r* = 0.91 says at all."
msgstr ""
"Betrachten Sie zur Veranschaulichung die in :numref:`fig-"
"ordinalRelationship` gezeigten Daten, die für 10 Studenten das Verhältnis "
"zwischen dem Arbeitseinsatz für einen Kurs (in h) und der Note zeigen. Das "
"Kuriose an diesem (fiktiven) Datensatz ist, dass eine Erhöhung des Aufwands "
"*immer* die Note erhöht. Das kann sehr viel oder wenig sein, aber eine "
"Erhöhung des Arbeitsaufwands führt nie zu einer Verschlechterung der Note. "
"Wenn wir eine gewöhnliche Pearson-Korrelation durchführen, zeigt sich eine "
"starke Beziehung zwischen dem Arbeitseinsatz und der Note, mit einem "
"Korrelationskoeffizienten von **0,91**. Dies entspricht jedoch nicht der "
"Beobachtung, dass eine Erhöhung der geleisteten Arbeitsstunden *immer zu "
"einer Verbesserung der Note* führt. In gewissem Sinne möchten wir sagen "
"können, dass die Korrelation *perfekt ist*, aber wir haben eine etwas andere "
"Vorstellung davon, was eine „Beziehung“ ist. Was wir suchen, ist etwas, das "
"die Tatsache festhält, dass es hier eine perfekte **ordinale Beziehung** "
"gibt. Das heißt, wenn Student 1 mehr Stunden arbeitet als Student 2, dann "
"können wir garantieren, dass Student 1 die bessere Note erhält. Das sagt "
"eine Korrelation von *r* = 0,91 aber überhaupt nicht aus."

#: ../../Ch12/Ch12_Regression_01.rst:331
msgid "relationship between hours worked and grade received"
msgstr ""
"Verhältnis zwischen dem geleisteten Arbeitseinsatz (in h) und der erhaltenen "
"Note"

#: ../../Ch12/Ch12_Regression_01.rst:331
msgid ""
"The relationship between hours worked and grade received for a toy data set "
"consisting of only 10 students (each circle corresponds to one student). The "
"dashed line through the middle shows the linear relationship between the two "
"variables. This produces a strong Pearson correlation of *r* = 0.91. "
"However, the interesting thing to note here is that there’s actually a "
"perfect monotonic relationship between the two variables. In this toy "
"example, increasing the hours worked always increases the grade received, as "
"illustrated by the solid line. This is reflected in a Spearman correlation "
"of ρ = 1.00. With such a small data set, however, it’s an open question as "
"to which version better describes the actual relationship involved."
msgstr ""
"Die Beziehung zwischen den geleisteten Arbeitsstunden und der erhaltenen "
"Note für einen fiktiven Datensatz mit 10 Studenten (jeder Kreis entspricht "
"einem Studenten). Die gestrichelte Linie in der Mitte zeigt die lineare "
"Beziehung zwischen den beiden Variablen mit einer starken Pearson-"
"Korrelation von *r* = 0,91. Interessant ist jedoch, dass es tatsächlich eine "
"perfekte monotone Beziehung zwischen den beiden Variablen gibt. Wie die "
"durchgezogene Linie zeigt, steigt in diesem Beispiel mit zunehmender Zahl "
"der geleisteten Arbeitsstunden immer auch die erhaltene Note. Dies spiegelt "
"sich in einer Spearman-Korrelation von ρ = 1,00 wider. Bei einem so kleinen "
"Datensatz ist es jedoch eine offene Frage, welche der Korrelationen die "
"tatsächliche Beziehung besser beschreibt."

#: ../../Ch12/Ch12_Regression_01.rst:345
msgid ""
"How should we address this? Actually, it’s really easy. If we’re looking for "
"ordinal relationships all we have to do is treat the data as if it were "
"ordinal scale |ordinal|! So, instead of measuring effort in terms of “hours "
"worked”, lets rank all 10 of our students in order of hours worked. That is, "
"student 1 did the least work out of anyone (2 hours) so they get the lowest "
"rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of "
"work over the whole semester, so they get the next lowest rank (rank = 2). "
"Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday "
"language we talk about “rank = 1” to mean “top rank” rather than “bottom "
"rank”. So be careful, you can rank “from smallest value to largest value” (i."
"e., small equals rank 1) or you can rank “from largest value to smallest "
"value” (i.e., large equals rank 1). In this case, I’m ranking from smallest "
"to largest, but as it’s really easy to forget which way you set things up "
"you have to put a bit of effort into remembering!"
msgstr ""
"Wie sollten wir das angehen? Eigentlich ist es ganz einfach. Wenn wir nach "
"ordinalen Beziehungen suchen, müssen wir die Daten nur so behandeln, als "
"wären sie ordinalskaliert |ordinal|! Anstatt also den Arbeitseinsatz in Form "
"von „Arbeitsstunden“ zu messen, sollten wir alle 10 Studenten entsprechend "
"der Rangfolge ihrer Arbeitsstunden einordnen. Das heißt, Student 1 hat von "
"allen am wenigsten gearbeitet (2 Stunden) und erhält daher den niedrigsten "
"Rang (Rang = 1). Student 4 war etwas fleißiger, hat aber während des "
"gesamten Semesters nur 6 Stunden Arbeit geleistet, und erhält daher den "
"nächsthöheren Rang (Rang = 2). Beachten Sie, dass ich „Rang = 1“ im Sinne "
"von „niedriger Rang“ verwende. In der Alltagssprache sprechen wir manchmal "
"von „Rang = 1“, um den „höchsten Rang“ und nicht den „niedrigsten Rang“ zu "
"meinen. Seien Sie also vorsichtig, Sie können eine Rangfolge „vom kleinsten "
"Wert zum größten Wert“ (d. h. klein ist gleich Rang 1) oder „vom größten "
"Wert zum kleinsten Wert“ (d. h. groß ist gleich Rang 1) erstellen. In diesem "
"Fall rangiere ich vom kleinsten zum größten Wert, aber da man sehr leicht "
"vergessen kann, wie man die Rangfolge einstellt, muss man sich schon ein "
"wenig Mühe geben, sich daran zu erinnern!"

#: ../../Ch12/Ch12_Regression_01.rst:423
msgid "ordinal"
msgstr "ordinal"

#: ../../Ch12/Ch12_Regression_01.rst:360
msgid ""
"Okay, so let’s have a look at our students when we rank them from worst to "
"best in terms of effort and reward:"
msgstr ""
"Schauen wir uns also unsere Studenten an, wenn wir sie in Bezug auf Aufwand "
"und Belohnung von der schlechtesten bis zur besten Leistung einstufen:"

#: ../../Ch12/Ch12_Regression_01.rst:364
msgid "rank (hours worked)"
msgstr "Rang (geleistete Arbeitsstunden)"

#: ../../Ch12/Ch12_Regression_01.rst:364
msgid "rank (grade received)"
msgstr "Rang (erhaltene Note)"

#: ../../Ch12/Ch12_Regression_01.rst:366
msgid "**student 1**"
msgstr "**Student 1**"

#: ../../Ch12/Ch12_Regression_01.rst:366
msgid "1"
msgstr "1"

#: ../../Ch12/Ch12_Regression_01.rst:368
msgid "**student 2**"
msgstr "**Student 2**"

#: ../../Ch12/Ch12_Regression_01.rst:368
msgid "10"
msgstr "10"

#: ../../Ch12/Ch12_Regression_01.rst:370
msgid "**student 3**"
msgstr "**Student 3**"

#: ../../Ch12/Ch12_Regression_01.rst:370
msgid "6"
msgstr "6"

#: ../../Ch12/Ch12_Regression_01.rst:372
msgid "**student 4**"
msgstr "**Student 4**"

#: ../../Ch12/Ch12_Regression_01.rst:372
msgid "2"
msgstr "2"

#: ../../Ch12/Ch12_Regression_01.rst:374
msgid "**student 5**"
msgstr "**Student 5**"

#: ../../Ch12/Ch12_Regression_01.rst:374
msgid "3"
msgstr "3"

#: ../../Ch12/Ch12_Regression_01.rst:376
msgid "**student 6**"
msgstr "**Student 6**"

#: ../../Ch12/Ch12_Regression_01.rst:376
msgid "5"
msgstr "5"

#: ../../Ch12/Ch12_Regression_01.rst:378
msgid "**student 7**"
msgstr "**Student 7**"

#: ../../Ch12/Ch12_Regression_01.rst:378
msgid "4"
msgstr "4"

#: ../../Ch12/Ch12_Regression_01.rst:380
msgid "**student 8**"
msgstr "**Student 8**"

#: ../../Ch12/Ch12_Regression_01.rst:380
msgid "8"
msgstr "8"

#: ../../Ch12/Ch12_Regression_01.rst:382
msgid "**student 9**"
msgstr "**Student 9**"

#: ../../Ch12/Ch12_Regression_01.rst:382
msgid "7"
msgstr "7"

#: ../../Ch12/Ch12_Regression_01.rst:384
msgid "**student 10**"
msgstr "**Student 10**"

#: ../../Ch12/Ch12_Regression_01.rst:384
msgid "9"
msgstr "9"

#: ../../Ch12/Ch12_Regression_01.rst:387
msgid ""
"Hmm. These are *identical*. The student who put in the most effort got the "
"best grade, the student with the least effort got the worst grade, etc. As "
"the table above shows, these two rankings are identical, so if we now "
"correlate them we get a perfect relationship, with a correlation of **1.0**."
msgstr ""
"Die Ränge sind *identisch*. Der Student, der sich am meisten angestrengt "
"hat, hat die beste Note bekommen, der Student mit der geringsten Anstrengung "
"hat die schlechteste Note bekommen usw. Wie die obige Tabelle zeigt, sind "
"die Ränge jeweils identisch. Wenn wir sie nun zueinander in Beziehung "
"setzen, erhalten wir eine perfekte Beziehung, mit einer Korrelation von "
"**1,0**."

#: ../../Ch12/Ch12_Regression_01.rst:393
msgid ""
"What we’ve just re-invented is **Spearman’s rank order correlation**, "
"usually denoted *ρ* to distinguish it from the Pearson correlation *r*. We "
"can calculate Spearman’s ρ using jamovi simply by clicking the ``Spearman`` "
"check box in the ``Correlation Matrix`` options panel."
msgstr ""
"Was wir soeben neu erfunden haben, ist **Spearmans Rangfolgekorrelation**, "
"die üblicherweise als ρ bezeichnet wird, um sie von der Pearson-Korrelation "
"*r* zu unterscheiden. Wir können Spearmans ρ mit jamovi berechnen, indem wir "
"einfach auf das Kontrollkästchen ``Spearman`` innerhalb der Optionen von "
"``Correlation Matrix`` klicken."

#: ../../Ch12/Ch12_Regression_01.rst:401
msgid ""
"Actually, even that table is more than I’d bother with. In practice, most "
"people pick *one* measure of central tendency, and *one* measure of "
"variability only."
msgstr ""
"Eigentlich ist selbst diese Tabelle zu umfangreich, als dass ich mich damit "
"beschäftigen würde. In der Praxis wählen die meisten Leute *ein* Maß für die "
"zentrale Tendenz und *ein* Maß für die Variabilität."

#: ../../Ch12/Ch12_Regression_01.rst:406
msgid ""
"Just like we saw with the variance and the standard deviation, in practice "
"we divide by *N* - 1 rather than *N*."
msgstr ""
"Genau wie bei der Varianz und der Standardabweichung wird in der Praxis "
"durch *N* - 1 und nicht durch *N* geteilt."

#: ../../Ch12/Ch12_Regression_01.rst:410
msgid "This is an oversimplification, but it’ll do for our purposes."
msgstr ""
"Das ist eine grobe Vereinfachung, aber für unsere Zwecke ist es ausreichend."

#: ../../Ch12/Ch12_Regression_02.rst:4
msgid "Scatterplots"
msgstr "Streudiagramme"

#: ../../Ch12/Ch12_Regression_02.rst:6
msgid ""
"**Scatterplots** are a simple but effective tool for visualising the "
"relationship between *two* variables, like we saw with the figures in the "
"section on correlation (section :doc:`Correlations <../Ch12/"
"Ch12_Regression_01>`). It’s this latter application that we usually have in "
"mind when we use the term “scatterplot”. In this kind of plot each "
"observation corresponds to one dot. The horizontal location of the dot plots "
"the value of the observation on one variable, and the vertical location "
"displays its value on the other variable. In many situations you don’t "
"really have a clear opinions about what the *causal* relationship is (e.g., "
"does A cause B, or does B cause A, or does some other variable C control "
"both A and B). If that’s the case, it doesn’t really matter which variable "
"you plot on the x-axis and which one you plot on the y-axis. However, in "
"many situations you do have a pretty strong idea which variable you think is "
"most likely to be causal, or at least you have some suspicions in that "
"direction. If so, then it’s conventional to plot the cause variable on the x-"
"axis, and the effect variable on the y-axis. With that in mind, let’s look "
"at how to draw scatterplots in jamovi, using the same |parenthood|_ data set "
"that I used when introducing correlations."
msgstr ""
"**Streudiagramme** sind ein einfaches, aber effektives Instrument zur "
"Visualisierung der Beziehung zwischen *zwei* Variablen, wie wir es bei den "
"Abbildungen im Abschnitt über (:doc:`Korrelationen <../Ch12/"
"Ch12_Regression_01>`) gesehen haben. An diese letztere Anwendung denken wir "
"normalerweise, wenn wir den Begriff „Streudiagramm“ (oder „*scatterplot*“) "
"verwenden. Bei dieser Art von Diagramm entspricht jede Beobachtung einem "
"Punkt. Die horizontale Position des Punktes stellt den Wert der Beobachtung "
"für eine Variable dar, die vertikale Position zeigt den Wert für die andere "
"Variable. In vielen Situationen hat man keine klare Meinung über die "
"*kausale* Beziehung (z. B. ob A Ursache für B ist, oder ob B Ursache für A "
"ist, oder ob eine andere Variable C sowohl A als auch B kontrolliert). Wenn "
"das der Fall ist, spielt es keine Rolle, welche Variable Sie auf der x-Achse "
"und welche Sie auf der y-Achse auftragen. In manchen Situationen haben Sie "
"jedoch eine Vorstellung davon, welche Variable Ihrer Meinung nach am "
"wahrscheinlichsten kausal ist, oder Sie haben zumindest einen Verdacht in "
"dieser Richtung. In diesem Fall ist es üblich, der Ursachenvariablen die x-"
"Achse und der Wirkungsvariablen die y-Achse zuzuweisen. In diesem Sinne "
"schauen wir uns an, wie man in jamovi Streudiagramme zeichnet, und zwar mit "
"demselben |parenthood|_-Datensatz, den ich bei der Einführung von "
"Korrelationen verwendet habe."

#: ../../Ch12/Ch12_Regression_02.rst:24
msgid ""
"Suppose my goal is to draw a scatterplot displaying the relationship between "
"the amount of sleep that I get (``dani.sleep``) and how grumpy I am the next "
"day (``dani.grump``). There are two different ways in which we can use "
"jamovi to get the plot that we’re after. The first way is to use the "
"``Plot`` option under the ``Regression`` → ``Correlation Matrix`` button, "
"giving us the output shown in :numref:`fig-scatterplot1`. Note that jamovi "
"draws a line through the points, we’ll come onto this a bit later in "
"section :doc:`What is a linear regression model? <../Ch12/"
"Ch12_Regression_03>`. Plotting a scatterplot in this way also allow you to "
"specify ``Densities for variables`` and this option adds a density curve "
"showing how the data in each variable is distributed."
msgstr ""
"Angenommen, mein Ziel ist es, ein Streudiagramm zu zeichnen, das die "
"Beziehung zwischen der Menge an Schlaf, die ich bekomme (``dani.sleep``) und "
"wie miesepetrig ich am nächsten Tag bin (``dani.grump``) darstellt. Es gibt "
"zwei verschiedene Möglichkeiten, wie wir jamovi verwenden können, um das "
"gewünschte Diagramm zu erhalten. Die erste Möglichkeit ist die Verwendung "
"der Option ``Plot`` unter der Schaltfläche ``Regression`` → ``Correlation "
"Matrix``, die uns die in :numref:`fig-scatterplot1` gezeigte Ausgabe "
"liefert. Beachten Sie, dass jamovi eine Linie durch die Punkte zieht, wir "
"werden darauf später im Abschnitt :doc:`Was ist ein lineares "
"Regressionsmodell? <../Ch12/Ch12_Regression_03>` eingehen. Wenn Sie ein "
"Streudiagramm auf diese Weise erstellen, können Sie auch ``Densities for "
"variables`` angeben. Diese Option fügt eine Dichtekurve hinzu, die zeigt, "
"wie die Daten jeder einzelnen Variablen verteilt sind."

#: ../../Ch12/Ch12_Regression_02.rst:41
msgid "Scatterplot created with the ``Correlation Matrix`` analysis in jamovi"
msgstr ""
"Streudiagramm erstellt mit der Analyse ``Correlation Matrix`` in jamovi"

#: ../../Ch12/Ch12_Regression_02.rst:45
msgid ""
"The second way do to it is to use one of the jamovi add-on modules. This "
"module is called ``scatr`` and you can install it by clicking on the large "
"``+`` icon in the top right of the jamovi screen, opening the jamovi "
"library, scrolling down until you find ``scatr`` and clicking ``Install``. "
"When you have done this, you will find a new ``Scatterplot`` command "
"available under the ``Exploration`` button. This plot is a bit different "
"than the first way, see :numref:`fig-scatterplot2`, but the important "
"information is the same."
msgstr ""
"Die zweite Möglichkeit ist, eines der jamovi-Zusatzmodule zu verwenden. "
"Dieses Modul heißt ``scatr`` und Sie können es installieren, indem Sie auf "
"das große Symbol ``+`` oben rechts auf dem jamovi-Bildschirm klicken, die "
"jamovi-Bibliothek öffnen, nach unten scrollen, bis Sie ``scatr`` finden "
"(bzw. ``scatr`` in die Linie für die Suche eingeben) und dann auf "
"``Install`` klicken. Wenn Sie dies getan haben, finden Sie einen neuen "
"Befehl ``Scatterplot`` unter der Schaltfläche ``Exploration``. Diese "
"Darstellung ist etwas anders als die erste, siehe :numref:`fig-"
"scatterplot2`, aber die zentralen Informationen sind dieselben."

#: ../../Ch12/Ch12_Regression_02.rst:60
msgid "Scatterplot cretaed with the ``scatr`` add-on module in jamovi"
msgstr "Scatterplot erstellt mit dem ``scatr``-Zusatzmodul in jamovi"

#: ../../Ch12/Ch12_Regression_02.rst:65
msgid "More elaborate options"
msgstr "Ausgefeiltere Optionen"

#: ../../Ch12/Ch12_Regression_02.rst:67
msgid ""
"Often you will want to look at the relationships between several variables "
"at once, using a **scatterplot matrix** (in jamovi via the ``Correlation "
"Matrix`` - ``Plot`` command). Just add another variable, for example ``baby."
"sleep`` to the list of variables to be correlated, and jamovi will create a "
"scatterplot matrix for you, just like the one in :numref:`fig-scatterplot3`."
msgstr ""
"Oft werden Sie die Beziehungen zwischen mehreren Variablen auf einmal "
"betrachten wollen, indem Sie eine **Streudiagramm-Matrix** verwenden (in "
"jamovi über den Befehl ``Correlation Matrix`` - ``Plot``). Fügen Sie einfach "
"eine weitere Variable, z.B. ``baby.sleep`` zur Liste der zu korrelierenden "
"Variablen hinzu, und jamovi erstellt für Sie eine Streudiagramm-Matrix, "
"genau wie die in :numref:`fig-scatterplot3`."

#: ../../Ch12/Ch12_Regression_02.rst:80
msgid "Matrix of scatterplots cretaed with the ``Correlation Matrix`` analysis"
msgstr ""
"Matrix von Streudiagrammen, die mit der Analyse ``Correlation Matrix`` "
"erstellt wurden"

#: ../../Ch12/Ch12_Regression_02.rst:80
msgid ""
"Matrix of scatterplots cretaed with the ``Correlation Matrix`` analysis in "
"jamovi."
msgstr ""
"Matrix von Streudiagrammen, die mit der Analyse ``Correlation Matrix`` in "
"jamovi erstellt wurden."

#: ../../Ch12/Ch12_Regression_03.rst:4
msgid "What is a linear regression model?"
msgstr "Was ist ein lineares Regressionsmodell?"

#: ../../Ch12/Ch12_Regression_03.rst:6
msgid ""
"Stripped to its bare essentials, linear regression models are basically a "
"slightly fancier version of the Pearson correlation (section :doc:"
"`Correlations <../Ch12/Ch12_Regression_01>`), though as we’ll see regression "
"models are much more powerful tools."
msgstr ""
"Auf das Wesentliche reduziert, sind lineare Regressionsmodelle im Grunde "
"eine etwas ausgefeiltere Version der Pearson-Korrelation (Abschnitt :doc:"
"`Korrelationen <../Ch12/Ch12_Regression_01>`), obwohl Regressionsmodelle, "
"wie wir sehen werden, viel leistungsfähigere Werkzeuge sind."

#: ../../Ch12/Ch12_Regression_03.rst:25
msgid ""
"Since the basic ideas in regression are closely tied to correlation, we’ll "
"return to the |parenthood|_ data set that we were using to illustrate how "
"correlations work. Recall that, in this data set we were trying to find out "
"why Dani is so very grumpy all the time and our working hypothesis was that "
"I’m not getting enough sleep. We drew some scatterplots to help us examine "
"the relationship between the amount of sleep I get and my grumpiness the "
"following day, as in :numref:`fig-scatterplot2`, and as we saw previously "
"this corresponds to a correlation of *r* = -0.90, but what we find ourselves "
"secretly imagining is something that looks closer to :numref:`fig-"
"regression1` (left panel). That is, we mentally draw a straight line through "
"the middle of the data. In statistics, this line that we’re drawing is "
"called a **regression line**. Notice that, since we’re not idiots, the "
"regression line goes through the middle of the data. We don’t find ourselves "
"imagining anything like the rather silly plot shown in :numref:`fig-"
"regression1` (right panel)."
msgstr ""
"Da die grundlegenden Ideen der Regression eng mit der Korrelation verknüpft "
"sind, kehren wir zum Datensatz |parenthood|_ zurück, den wir zur "
"Veranschaulichung der Funktionsweise von Korrelationen verwendet haben. "
"Erinnern Sie sich daran, dass wir mit diesem Datensatz versucht haben, "
"herauszufinden, warum Dani ständig so schlecht gelaunt ist. Unsere "
"Arbeitshypothese war, dass ich nicht genug Schlaf bekommen hat. Wir haben "
"einige Streudiagramme gezeichnet, um die Beziehung zwischen der Menge an "
"Schlaf, die ich bekommen habe, und meiner schlechten Laune am nächsten Tag "
"zu untersuchen, wie in :numref:`fig-scatterplot2`. Wie wir zuvor gesehen "
"haben, entspricht dies einer Korrelation von *r* = -0,90, aber wir stellen "
"uns insgeheim etwas vor, das näher an :numref:`fig-regression1` (links) "
"liegt. Das heißt, wir ziehen gedanklich eine gerade Linie durch die Mitte "
"der Daten. In der Statistik wird diese Linie, die wir zeichnen, als "
"**Regressionsgerade** bezeichnet. Da wir keine Idioten sind, geht die "
"Regressionsgerade durch die Mitte der Daten. Wir stellen uns nicht so etwas "
"vor wie das ziemlich alberne Diagramm in :numref:`fig-regression1` (rechts)."

#: ../../Ch12/Ch12_Regression_03.rst:33
msgid "Best and poor choice of regression line"
msgstr "Beste und schlechte Wahl der Regressionsgeraden"

#: ../../Ch12/Ch12_Regression_03.rst:33
msgid ""
"The left panel shows the scatterplot of ``dani.sleep`` and ``dani.grump`` "
"from :numref:`fig-scatterplot2` with the best fitting regression line drawn "
"over the top. Not surprisingly, the line goes through the middle of the "
"data. In contrast, the right panel shows the same data, but with a very poor "
"choice of regression line drawn over the top."
msgstr ""
"Das linke Feld zeigt das Streudiagramm von ``dani.sleep`` und ``dani.grump`` "
"aus :numref:`fig-scatterplot2` inklusive der am besten passenden "
"Regressionsgerade. Es überrascht nicht, dass die Linie durch die Mitte der "
"Daten geht. Im Gegensatz dazu zeigt die rechte Abbildung die gleichen Daten, "
"diesmal aber mit einer sehr schlecht gewählten Regressionsgerade."

#: ../../Ch12/Ch12_Regression_03.rst:41
msgid ""
"This is not highly surprising. The line that I’ve drawn in :numref:`fig-"
"regression1` (right panel) doesn’t “fit” the data very well, so it doesn’t "
"make a lot of sense to propose it as a way of summarising the data, right? "
"This is a very simple observation to make, but it turns out to be very "
"powerful when we start trying to wrap just a little bit of maths around it. "
"To do so, let’s start with a refresher of some high school maths. The "
"formula for a straight line is usually written like this:"
msgstr ""
"Dies ist nicht sehr überraschend. Die Linie, die ich in :numref:`fig-"
"regression1` (rechts) gezeichnet habe, „passt“ nicht sehr gut zu den Daten, "
"so dass es nicht sehr sinnvoll ist, sie als Mittel zur Zusammenfassung der "
"Daten vorzuschlagen? Dies ist eine sehr einfache Beobachtung, die sich "
"jedoch als sehr aussagekräftig erweist, wenn wir versuchen, sie mit ein "
"wenig Mathematik zu ummanteln. Beginnen wir also mit einer Auffrischung der "
"Schulmathematik. Die Formel für eine gerade Linie wird normalerweise so "
"geschrieben:"

#: ../../Ch12/Ch12_Regression_03.rst:49
msgid "*y* = *a* + *bx*"
msgstr "*y* = *a* + *bx*"

#: ../../Ch12/Ch12_Regression_03.rst:51
msgid ""
"Or, at least, that’s what it was when I went to high school all those years "
"ago. The two *variables* are *x* and *y*, and we have two *coefficients*, "
"*a* and *b*\\.\\ [#]_ The coefficient *a* represents the **y-intercept** of "
"the line, and coefficient *b* represents the *slope* of the line. Digging "
"further back into our decaying memories of high school (sorry, for some of "
"us high school was a long time ago), we remember that the intercept is "
"interpreted as “the value of *y* that you get when *x* = 0”. Similarly, a "
"slope of *b* means that if you increase the *x*-value by 1 unit, then the "
"*y*-value goes up by *b* units, and a negative slope means that the *y*-"
"value would go down rather than up. Ah yes, it’s all coming back to me now. "
"Now that we’ve remembered that it should come as no surprise to discover "
"that we use the exact same formula for a regression line. If *Y* is the "
"outcome variable (the DV) and *X* is the predictor variable (the IV), then "
"the formula that describes our regression is written like this:"
msgstr ""
"Die beiden *Variablen* sind *x* und *y*, und wir haben zwei *Koeffizienten*, "
"*a* und *b*\\.\\ [#]_ Der Koeffizient *a* stellt den **y-Achsenabschnitt** "
"(das Interzept) der Geraden dar, und der Koeffizient *b* stellt die "
"*Steigung* der Geraden dar. Wenn wir weiter in unseren verblassten "
"Erinnerungen an Mathematik aus unserer Schulzeit kramen, erinnern wir uns, "
"dass der Achsenabschnitt (das Interzept) als „der Wert von *y* interpretiert "
"wird, den man erhält, wenn *x* = 0“. In ähnlicher Weise bedeutet eine "
"Steigung von *b*, dass der *y*-Wert um *b* Einheiten ansteigt, wenn man den "
"*x*-Wert um 1 Einheit erhöht, und eine negative Steigung bedeutet, dass der "
"*y*-Wert nicht ansteigt, sondern fällt. Da wir uns nun daran erinnert haben, "
"sollte es nicht mehr überraschen, dass wir genau dieselbe Formel für eine "
"Regressionsgerade verwenden können. Wenn *Y* die Ergebnisvariable (die DV) "
"und *X* die Prädiktorvariable (die IV) ist, dann lautet die Formel, die "
"unsere Regression beschreibt, wie folgt:"

#: ../../Ch12/Ch12_Regression_03.rst:66
msgid "*Ŷ*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i`"
msgstr "*Ŷ*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_03.rst:68
msgid ""
"Hmm. Looks like the same formula, but there’s some extra frilly bits in this "
"version. Let’s make sure we understand them. Firstly, notice that I’ve "
"written *X*\\ :sub:`i` and *Y*\\ :sub:`i` rather than just plain old *X* and "
"*Y*. This is because we want to remember that we’re dealing with actual "
"data. In this equation, *X*\\ :sub:`i` is the value of predictor variable "
"for the i\\ th observation (i.e., the number of hours of sleep that I got on "
"day i of my little study), and *Y*\\ :sub:`i` is the corresponding value of "
"the outcome variable (i.e., my grumpiness on that day). And although I "
"haven’t said so explicitly in the equation, what we’re assuming is that this "
"formula works for all observations in the data set (i.e., for all i). "
"Secondly, notice that I wrote *Ŷ*\\ :sub:`i` and not *Y*\\ :sub:`i`. This is "
"because we want to make the distinction between the *actual data* *Y*\\ :sub:"
"`i`, and the *estimate* *Ŷ*\\ :sub:`i` (i.e., the prediction that our "
"regression line is making). Thirdly, I changed the letters used to describe "
"the coefficients from *a* and *b* to *b*\\ :sub:`0` and *b*\\ :sub:`1`. "
"That’s just the way that statisticians like to refer to the coefficients in "
"a regression model. I’ve no idea why they chose *b*, but that’s what they "
"did. In any case *b*\\ :sub:`0` always refers to the intercept term, and "
"*b*\\ :sub:`1` refers to the slope."
msgstr ""
"Das sieht nach der gleichen Formel aus, aber diese Version enthält ein paar "
"zusätzliche Details. Vergewissern wir uns, dass wir sie verstehen. Zunächst "
"fällt auf, dass ich *X*\\ :sub:`i` und *Y*\\ :sub:`i` geschrieben habe und "
"nicht einfach nur *X* und *Y*. Das liegt daran, dass wir uns daran erinnern "
"wollen, dass wir es mit tatsächlichen Daten zu tun haben. In dieser "
"Gleichung ist *X*\\ :sub:`i` der Wert der Vorhersagevariable für die i\\-te "
"Beobachtung (d. h. die Anzahl der Schlafstunden, die ich am Tag i meiner "
"Aufzeichnungen bekommen habe), und *Y*\\ :sub:`i` ist der entsprechende Wert "
"der Ergebnisvariable (d. h. das Ausmaß meiner schlechten Laune an diesem "
"Tag). Und obwohl ich das in der Gleichung nicht ausdrücklich gesagt habe, "
"gehen wir davon aus, dass diese Formel für alle Beobachtungen im Datensatz "
"gilt (d. h. für alle i). Zweitens: Beachten Sie, dass ich *Ŷ*\\ :sub:`i` "
"geschrieben habe und nicht *Y*\\ :sub:`i`. Das liegt daran, dass wir "
"zwischen den *tatsächlichen Daten* *Y*\\ :sub:`i` und der *Schätzung* *Ŷ*\\ :"
"sub:`i` (d. h. der Vorhersage, die unsere Regressionsgerade macht) "
"unterscheiden wollen. Drittens habe ich die Buchstaben zur Beschreibung der "
"Koeffizienten von *a* und *b* in *b*\\ :sub:`0` und *b*\\ :sub:`1` geändert. "
"Das ist einfach die Art und Weise, wie Statistiker die Koeffizienten in "
"einem Regressionsmodell bezeichnen. Ich habe keine Ahnung, warum sie *b* "
"gewählt haben, aber sie haben es so gemacht. Auf jeden Fall bezieht sich "
"*b*\\ :sub:`0` immer auf den Interzept-Term und *b*\\ :sub:`1` auf die "
"Steigung."

#: ../../Ch12/Ch12_Regression_03.rst:89
msgid ""
"Excellent, excellent. Next, I can’t help but notice that, regardless of "
"whether we’re talking about the good regression line or the bad one, the "
"data don’t fall perfectly on the line. Or, to say it another way, the data "
"*Y*\\ :sub:`i` are not identical to the predictions of the regression model "
"*Ŷ*\\ :sub:`i`. Since statisticians love to attach letters, names and "
"numbers to everything, let’s refer to the difference between the model "
"prediction and that actual data point as a *residual*, and we’ll refer to it "
"as ε\\ :sub:`i`.\\ [#]_ Written using mathematics, the residuals are defined "
"as"
msgstr ""
"Ausgezeichnet. Als Nächstes kann ich nicht umhin festzustellen, dass die "
"Daten, unabhängig davon, ob es sich um eine gute oder eine schlechte "
"Regressionslinie handelt, nicht perfekt auf die Linie fallen. Oder anders "
"gesagt, die Daten *Y*\\ :sub:`i` sind nicht identisch mit den Vorhersagen "
"des Regressionsmodells *Ŷ*\\ :sub:`i`. Da Statistiker es lieben, alles mit "
"Buchstaben, Namen und Zahlen zu versehen, bezeichnen wir die Differenz "
"zwischen der Modellvorhersage und dem tatsächlichen Datenpunkt als "
"*Residuum* und bezeichnen sie als ε\\ :sub:`i`.\\ [#]_ Mathematisch "
"ausgedrückt sind die Residuen definiert als"

#: ../../Ch12/Ch12_Regression_03.rst:99 ../../Ch12/Ch12_Regression_11.rst:61
msgid "ε\\ :sub:`i` = *Y*\\ :sub:`i` - *Ŷ*\\ :sub:`i`"
msgstr "ε\\ :sub:`i` = *Y*\\ :sub:`i` - *Ŷ*\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_03.rst:101
msgid ""
"which in turn means that we can write down the complete linear regression "
"model as"
msgstr ""
"was wiederum bedeutet, dass wir das vollständige lineare Regressionsmodell "
"wie folgt aufschreiben können"

#: ../../Ch12/Ch12_Regression_03.rst:104
msgid ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i` + ε\\ :sub:"
"`i`"
msgstr ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i` + ε\\ :sub:"
"`i`"

#: ../../Ch12/Ch12_Regression_03.rst:109
msgid ""
"Also sometimes written as *y* = mx + c where m is the slope coefficient and "
"c is the intercept (constant) coefficient."
msgstr ""
"Manchmal auch geschrieben als *y* = mx + c, wobei m der Steigungskoeffizient "
"und c der Achsenabschnitts (Interzept, Konstante) ist."

#: ../../Ch12/Ch12_Regression_03.rst:113
msgid ""
"The ε symbol is the Greek letter epsilon. It’s traditional to use ε\\ :sub:"
"`i` or e\\ :sub:`i` to denote a residual."
msgstr ""
"Das Symbol ε ist der griechische Buchstabe Epsilon. Es ist üblich, ε\\ :sub:"
"`i` oder e\\ :sub:`i` zu verwenden, um die Residuen zu bezeichnen."

#: ../../Ch12/Ch12_Regression_04.rst:4
msgid "Estimating a linear regression model"
msgstr "Schätzen eines linearen Regressionsmodells"

#: ../../Ch12/Ch12_Regression_04.rst:12
msgid "Residuals associated with the best and with a poor regression line"
msgstr ""
"Residuen in Verbindung mit der besten und der schlechtesten Regressionslinie"

#: ../../Ch12/Ch12_Regression_04.rst:12
msgid ""
"Depiction of the residuals associated with the best fitting regression line "
"(left panel), and the residuals associated with a poor regression line "
"(right panel). The residuals are much smaller for the good regression line. "
"Again, this is no surprise given that the good line is the one that goes "
"right through the middle of the data."
msgstr ""
"Darstellung der Residuen mit der am besten passenden Regressionsgerade "
"(links) und einer schlechten Regressionsgerade (rechts). Die Residuen sind "
"bei der guten Regressionsgerade viel kleiner. Auch hier ist dies keine "
"Überraschung, da die gute Linie genau durch die Mitte der Daten verläuft."

#: ../../Ch12/Ch12_Regression_04.rst:20
msgid ""
"Okay, now let’s redraw our pictures but this time I’ll add some lines to "
"show the size of the residual for all observations. When the regression line "
"is good, our residuals (the lengths of the solid black lines) all look "
"pretty small, as shown in :numref:`fig-regression3` (left panel), but when "
"the regression line is a bad one the residuals are a lot larger, as you can "
"see from looking at :numref:`fig-regression3` (right panel). Hmm. Maybe what "
"we “want” in a regression model is *small* residuals. Yes, that does seem to "
"make sense. In fact, I think I’ll go so far as to say that the “best "
"fitting” regression line is the one that has the smallest residuals. Or, "
"better yet, since statisticians seem to like to take squares of everything "
"why not say that: The estimated regression coefficients, :math:`\\hat{b}_0` "
"and :math:`\\hat{b}_1`, are those that minimise the sum of the squared "
"residuals, which we could either write as"
msgstr ""
"Jetzt erstellen wir unsere Diagramme noch einmal, aber dieses Mal füge ich "
"einige Linien hinzu, um die Größe der Residuen für alle Beobachtungen zu "
"zeigen. Wenn die Regressionsgerade gut gewählt ist, sehen unsere Residuen "
"(die Längen der durchgezogenen schwarzen Linien) alle ziemlich klein aus, "
"wie in :numref:`fig-regression3` (links) zu sehen ist, aber wenn die "
"Regressionsgerade schlecht gewählt ist, sind die Residuen viel größer, wie "
"man in :numref:`fig-regression3` (rechts) sehen kann. Vielleicht ist das, "
"was wir in einem Regressionsmodell „wollen“, *kleine* Residuen. Ja, das "
"scheint wirklich Sinn zu machen. Ich würde sogar so weit gehen zu sagen, "
"dass die am besten passende Regressionsgerade diejenige ist, welche die "
"kleinsten Residuen aufweist. Oder, noch besser, da Statistiker anscheinend "
"gerne Quadrate von allem nehmen, warum sollte man das nicht sagen: Die "
"geschätzten Regressionskoeffizienten, :math:`\\hat{b}_0` und :math:`\\hat{b}"
"_1`, sind diejenigen, die die Summe der quadrierten Residuen minimieren. Wir "
"könnten das entweder schreiben als"

#: ../../Ch12/Ch12_Regression_04.rst:34
msgid ""
"\\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"
msgstr ""
"\\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_04.rst:36
msgid "or as"
msgstr "oder als"

#: ../../Ch12/Ch12_Regression_04.rst:38
msgid ""
"\\sum_i \\epsilon_{i}^2\n"
"\n"
msgstr ""
"\\sum_i \\epsilon_{i}^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_04.rst:40
msgid ""
"Yes, yes that sounds even better. And since I’ve indented it like that, it "
"probably means that this is the right answer. And since this is the right "
"answer, it’s probably worth making a note of the fact that our regression "
"coefficients are *estimates* (we’re trying to guess the parameters that "
"describe a population!), which is why I’ve added the little hats, so that we "
"get :math:`\\hat{b}_0` and :math:`\\hat{b}_1` rather than *b*\\ :sub:`0` and "
"*b*\\ :sub:`1`. Finally, I should also note that, since there’s actually "
"more than one way to estimate a regression model, the more technical name "
"for this estimation process is **ordinary least squares (OLS) regression**."
msgstr ""
"Das klingt sogar noch besser. Und da ich es so eingerückt habe, bedeutet das "
"wahrscheinlich, dass dies die richtige Antwort ist. Und da dies die richtige "
"Antwort ist, lohnt es sich wahrscheinlich, die Tatsache zu erwähnen, dass "
"unsere Regressionskoeffizienten *Schätzungen* sind (wir versuchen, die "
"Parameter zu schätzen, die eine Population beschreiben!). Deshalb habe ich "
"die kleinen Hüte hinzugefügt, so dass wir :math:`\\hat{b}_0` und :math:"
"`\\hat{b}_1` erhalten, anstatt *b*\\ :sub:`0` und *b*\\ :sub:`1`. "
"Abschließend sollte ich noch anmerken, dass es mehr als eine Möglichkeit "
"gibt, ein Regressionsmodell zu schätzen. Der technischere Name für dieses "
"Schätzverfahren ist **ordinary least squares (OLS) regression** "
"(Kleinstquadrate-Kriterium oder Minimieren der quadrierten Abweichungen)."

#: ../../Ch12/Ch12_Regression_04.rst:51
msgid ""
"At this point, we now have a concrete definition for what counts as our "
"“best” choice of regression coefficients, :math:`\\hat{b}_0` and :math:"
"`\\hat{b}_1`. The natural question to ask next is, if our optimal regression "
"coefficients are those that minimise the sum squared residuals, how do we "
"*find* these wonderful numbers? The actual answer to this question is "
"complicated and doesn’t help you understand the logic of regression.\\ [#]_ "
"This time I’m going to let you off the hook. Instead of showing you the long "
"and tedious way first and then “revealing” the wonderful shortcut that "
"jamovi provides, let’s cut straight to the chase and just use jamovi to do "
"all the heavy lifting."
msgstr ""
"An diesem Punkt haben wir nun eine konkrete Definition dafür, was als unsere "
"„beste“ Wahl von Regressionskoeffizienten gilt, :math:`\\hat{b}_0` und :math:"
"`\\hat{b}_1`. Wenn unsere optimalen Regressionskoeffizienten diejenigen "
"sind, die die Summe der quadrierten Residuen minimieren, dann stellt sich "
"natürlich die Frage, wie wir diese wunderbaren Zahlen *finden*. Die "
"eigentliche Antwort auf diese Frage ist kompliziert und hilft Ihnen nicht, "
"um die Logik der Regression zu verstehen.\\ [#]_ Daher verwenden wir gleich "
"jamovi, um die ganze schwere Arbeit zu erledigen, anstatt Ihnen zuerst den "
"langen, mühsamen und schrittweisen Weg zu zeigen und dann die wunderbare "
"Abkürzung zu „enthüllen“, die jamovi bietet."

#: ../../Ch12/Ch12_Regression_04.rst:63
msgid "Linear regression in jamovi"
msgstr "Lineare Regression in jamovi"

#: ../../Ch12/Ch12_Regression_04.rst:71
msgid "jamovi screenshot showing a simple linear regression analysis"
msgstr "jamovi-Screenshot mit einer einfachen linearen Regressionsanalyse"

#: ../../Ch12/Ch12_Regression_04.rst:75
msgid ""
"To run my linear regression, open up the ``Regression`` - ``Linear "
"Regression`` analysis in jamovi, using the |parenthood|_ data set. Then "
"specify ``dani.grump`` as the ``Dependent Variable`` and ``dani.sleep`` as "
"the variable entered in the ``Covariates`` box. This gives the results shown "
"in :numref:`fig-reg1`, showing an intercept :math:`\\hat{b}_0` = 125.96 and "
"the slope :math:`\\hat{b}_1` = -8.94. In other words, the best-fitting "
"regression line that I plotted in :numref:`fig-regression1` has this formula:"
msgstr ""
"Um eine lineare Regression unter Verwendung des Datensatzes |parenthood|_ "
"durchzuführen, öffnen Sie die Analyse ``Regression`` - ``Linear Regression`` "
"in jamovi. Verschieben Sie dann ``dani.grump`` in die ``Dependent Variable`` "
"und ``dani.sleep`` in das Variablenfeld ``Covariates``. Dadurch ergeben sich "
"die in :numref:`fig-reg1` gezeigten Ergebnisse, mit einem Interzept :math:`"
"\\hat{b}_0` = 125,96 und einer Steigung :math:`\\hat{b}_1` = -8,94. Mit "
"anderen Worten, die am besten passende Regressionslinie, die ich in :numref"
":`fig-regression1` eingezeichnet habe, hat die folgende Formel:"

#: ../../Ch12/Ch12_Regression_04.rst:83
msgid "*Ŷ*\\ :sub:`i` = 125.96 + (-8.94 \\ *X*\\ :sub:`i`)"
msgstr "*Ŷ*\\ :sub:`i` = 125.96 + (-8.94 \\ *X*\\ :sub:`i`)"

#: ../../Ch12/Ch12_Regression_04.rst:86
msgid "Interpreting the estimated model"
msgstr "Interpretation des geschätzten Modells"

#: ../../Ch12/Ch12_Regression_04.rst:88
msgid ""
"The most important thing to be able to understand is how to interpret these "
"coefficients. Let’s start with :math:`\\hat{b}_1`, the slope. If we remember "
"the definition of the slope, a regression coefficient of :math:`\\hat{b}_1` "
"= -8.94 means that if I increase *X*\\ :sub:`i` by 1, then I’m decreasing "
"*Y*\\ :sub:`i` by 8.94. That is, each additional hour of sleep that I gain "
"will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What "
"about the intercept? Well, since :math:`\\hat{b}_0` corresponds to “the "
"expected value of *Y*\\ :sub:`i` when *X*\\ :sub:`i` equals 0”, it’s pretty "
"straightforward. It implies that if I get zero hours of sleep (*X*\\ :sub:"
"`i` = 0) then my grumpiness will go off the scale, to an insane value of "
"(*Y*\\ :sub:`i` = \\125.96). Best to be avoided, I think."
msgstr ""
"Das Wichtigste, was man verstehen muss, ist, wie man diese Koeffizienten "
"interpretieren kann. Beginnen wir mit :math:`\\hat{b}_1`, der Steigung. Wenn "
"wir uns an die Definition der Steigung erinnern, bedeutet ein "
"Regressionskoeffizient von :math:`\\hat{b}_1` = -8,94, dass, wenn ich *X*\\ "
":sub:`i` um 1 erhöhe, ich *Y*\\ :sub:`i` um 8,94 vermindern werde. Das "
"heißt, dass jede zusätzliche Stunde Schlaf, die ich gewinne, meine Stimmung "
"verbessert und meine schlechte Laune um 8,94 Punkte verringert. Was ist mit "
"dem Schnittpunkt? Nun, da :math:`\\hat{b}_0` dem „erwarteten Wert von *Y*\\ "
":sub:`i` entspricht, wenn *X*\\ :sub:`i` gleich 0 ist“, ist das ziemlich "
"einfach. Das bedeutet, dass, wenn ich null Stunden Schlaf bekomme (*X*\\ "
":sub:`i` = 0), meine schlechte Laune einen unglaublich hohen Wert erreichen "
"wird (*Y*\\ :sub:`i` = \\125,96). Das sollte man besser vermeiden, denke ich."

#: ../../Ch12/Ch12_Regression_04.rst:103
msgid ""
"Or at least, I’m assuming that it doesn’t help most people. But on the off-"
"chance that someone reading this is a proper kung fu master of linear "
"algebra (and to be fair, I always have a few of these people in my "
"intorductory statistics class), it *will* help *you* to know that the "
"solution to the estimation problem turns out to be :math:`\\hat{b} = "
"(\\mathbf{X}^\\prime\\mathbf{X})^{-1} \\mathbf{X}^\\prime y`, where :math:"
"`\\hat{b}` is a vector containing the estimated regression coefficients, "
"**X** is the “design matrix” that contains the predictor variables (plus an "
"additional column containing all ones; strictly **X** is a matrix of the "
"regressors, but I haven’t discussed the distinction yet), and *y* is a "
"vector containing the outcome variable. For everyone else, this isn’t "
"exactly helpful and can be downright scary. However, since quite a few "
"things in linear regression can be written in linear algebra terms, you’ll "
"see a bunch of footnotes like this one in this chapter. If you can follow "
"the maths in them, great. If not, ignore it."
msgstr ""
"Zumindest gehe ich davon aus, dass es den meisten Menschen nicht hilft. Aber "
"für den unwahrscheinlichen Fall, dass jemand, der dies liest, ein echter "
"Kung-Fu-Meister der linearen Algebra ist (und um fair zu sein, ich habe "
"immer ein paar von diesen Leuten in meinem Einführungskurs in Statistik), "
"*wird* es Ihnen helfen zu wissen, dass die Lösung des Schätzungsproblems "
"sich als :math:`\\hat{b} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1} \\mathbf{X}^"
"\\prime y`, wobei :math:`\\hat{b}` ein Vektor ist, der die geschätzten "
"Regressionskoeffizienten enthält, **X** die „Designmatrix“ ist, welche die "
"Prädiktorvariablen enthält (plus eine zusätzliche Spalte, die Einsen in "
"allen Zeilen enthält; streng genommen ist **X** eine Matrix der Regressoren, "
"aber ich habe die Unterscheidung hier noch nicht diskutiert), und *y* ein "
"Vektor ist, der die Ergebnisvariable enthält. Für alle anderen ist das nicht "
"gerade hilfreich und wirkt eher beängstigend. Da sich jedoch viele Dinge in "
"der linearen Regression in Begriffen der linearen Algebra ausdrücken lassen, "
"werden Sie in diesem Kapitel eine Reihe von Fußnoten wie diese finden. Wenn "
"Sie der Mathematik darin folgen können, großartig. Wenn nicht, ignorieren "
"Sie sie."

#: ../../Ch12/Ch12_Regression_05.rst:4
msgid "Multiple linear regression"
msgstr "Multiple lineare Regression"

#: ../../Ch12/Ch12_Regression_05.rst:6
msgid ""
"The simple linear regression model that we’ve discussed up to this point "
"assumes that there’s a single predictor variable that you’re interested in, "
"in this case ``dani.sleep``. In fact, up to this point *every* statistical "
"tool that we’ve talked about has assumed that your analysis uses one "
"predictor variable and one outcome variable. However, in many (perhaps most) "
"research projects you actually have multiple predictors that you want to "
"examine. If so, it would be nice to be able to extend the linear regression "
"framework to be able to include multiple predictors. Perhaps some kind of "
"**multiple regression** model would be in order?"
msgstr ""
"Das einfache lineare Regressionsmodell, das wir bisher besprochen haben, "
"geht davon aus, dass es eine einzige Prädiktorvariable gibt, an der Sie "
"interessiert sind, in diesem Fall ``dani.sleep``. Tatsächlich ist bis zu "
"diesem Punkt *jedes* statistische Werkzeug, über das wir gesprochen haben, "
"davon ausgegangen, dass Ihre Analyse eine Prädiktorvariable und eine "
"Ergebnisvariable verwendet. In vielen (vielleicht den meisten) "
"Forschungsprojekten haben Sie jedoch mehrere Prädiktoren, die Sie "
"untersuchen möchten. In diesem Fall wäre es schön, wenn man den Rahmen der "
"linearen Regression erweitern könnte, um mehrere Prädiktoren einbeziehen zu "
"können. Vielleicht wäre eine Art **multiple Regression** angebracht?"

#: ../../Ch12/Ch12_Regression_05.rst:17
msgid ""
"Multiple regression is conceptually very simple. All we do is add more terms "
"to our regression equation. Let’s suppose that we’ve got two variables that "
"we’re interested in; perhaps we want to use both ``dani.sleep`` and ``baby."
"sleep`` to predict the ``dani.grump`` variable. As before, we let *Y*\\ :sub:"
"`i` refer to my grumpiness on the i-th day. But now we have two *X* "
"variables: the first corresponding to the amount of sleep I got and the "
"second corresponding to the amount of sleep my son got. So we’ll let *X*\\ :"
"sub:`i1` refer to the hours I slept on the i-th day and *X*\\ :sub:`i2` "
"refers to the hours that the baby slept on that day. If so, then we can "
"write our regression model like this:"
msgstr ""
"Die multiple Regression ist konzeptionell sehr einfach. Wir fügen lediglich "
"weitere Terme zu unserer Regressionsgleichung hinzu. Nehmen wir an, wir "
"haben zwei Variablen, an denen wir interessiert sind; vielleicht wollen wir "
"sowohl ``dani.sleep`` als auch ``baby.sleep`` verwenden, um die Variable "
"``dani.grump`` vorherzusagen. Wie zuvor lassen wir *Y*\\ :sub:`i` sich auf "
"meine schlechte Laune am i-ten Tag beziehen. Aber jetzt haben wir zwei *X* "
"Variablen: die erste entspricht der Menge an Schlaf, die ich bekommen habe, "
"und die zweite entspricht der Menge an Schlaf, die mein Sohn bekommen hat. "
"Wir lassen also *X*\\ :sub:`i1` sich auf die Stunden beziehen, die ich am i-"
"ten Tag geschlafen habe, und *X*\\ :sub:`i2` bezieht sich auf die Stunden, "
"die das Baby an diesem Tag geschlafen hat. Wenn dies der Fall ist, können "
"wir unser Regressionsmodell wie folgt schreiben:"

#: ../../Ch12/Ch12_Regression_05.rst:29
msgid ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i1` + *b*\\ :"
"sub:`2` *X*\\ :sub:`i2` + ε\\ :sub:`i`"
msgstr ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i1` + *b*\\ :"
"sub:`2` *X*\\ :sub:`i2` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_05.rst:31
msgid ""
"As before, ε\\ :sub:`i` is the residual associated with the i-th "
"observation, ε\\ :sub:`i` = {Y}_i - \\hat{Y}_i`. In this model, we now have "
"three coefficients that need to be estimated: *b*\\ :sub:`0` is the "
"intercept, *b*\\ :sub:`1` is the coefficient associated with my sleep, and "
"*b*\\ :sub:`2` is the coefficient associated with my son’s sleep. However, "
"although the number of coefficients that need to be estimated has changed, "
"the basic idea of how the estimation works is unchanged: our estimated "
"coefficients :math:`\\hat{b}_0`, :math:`\\hat{b}_1` and :math:`\\hat{b}_2` "
"are those that minimise the sum squared residuals."
msgstr ""
"Wie zuvor ist ε\\ :sub:`i` das mit der i-ten Beobachtung verbundene "
"Residuum, :math:`{\\epsilon}_i = {Y}_i - \\hat{Y}_i`. In diesem Modell haben "
"wir nun drei Koeffizienten, die geschätzt werden müssen: *b*\\ :sub:`0` ist "
"das Interzept, *b*\\ :sub:`1` ist der Koeffizient, der mit meinem Schlaf "
"verbunden ist, und *b*\\ :sub:`2` ist der Koeffizient, der mit dem Schlaf "
"meines Sohnes verbunden ist. Obwohl sich die Anzahl der zu schätzenden "
"Koeffizienten geändert hat, bleibt die Grundidee der Schätzung unverändert: "
"Unsere geschätzten Koeffizienten :math:`\\hat{b}_0`, :math:`\\hat{b}_1` und "
":math:`\\hat{b}_2` sind diejenigen, welche die Summe der quadrierten "
"Residuen minimieren."

#: ../../Ch12/Ch12_Regression_05.rst:43
msgid "Doing it in jamovi"
msgstr "Durchführung in jamovi"

#: ../../Ch12/Ch12_Regression_05.rst:45
msgid ""
"Multiple regression in jamovi is no different to simple regression. All we "
"have to do is add additional variables to the ``Covariates`` box in jamovi. "
"For example, if we want to use both ``dani.sleep`` and ``baby.sleep`` as "
"predictors in our attempt to explain why I’m so grumpy, then move ``baby."
"sleep`` across into the ``Covariates`` box alongside ``dani.sleep``. By "
"default, jamovi assumes that the model should include an intercept. The "
"coefficients we get this time are:"
msgstr ""
"Die multiple Regression in jamovi unterscheidet sich kaum von der einfachen "
"Regression. Alles, was wir tun müssen, ist, die zusätzliche Variablen in das "
"Feld ``Covariates`` in jamovi einzufügen. Wenn wir zum Beispiel sowohl ``dani"
".sleep`` als auch ``baby.sleep`` als Prädiktoren verwenden wollen, um zu "
"erklären, warum ich so schlechte Laune habe, dann verschieben Sie ``baby."
"sleep`` in das Feld ``Covariates`` zusätzlich zur schon enthaltenen Variable "
"``dani.sleep``. jamovi nimmt standardmäßig an, dass jedes Modell einen "
"Interzept enthalten sollte. Die Koeffizienten, die wir dieses Mal erhalten, "
"sind:"

#: ../../Ch12/Ch12_Regression_05.rst:53
msgid ""
"Model coefficients for the linear model predicting ``dani.grump`` using "
"``baby.sleep`` and ``dani.sleep`` (from the |parenthood|_ data set)."
msgstr ""
"Modellkoeffizienten für das lineare Modell zur Vorhersage von ``dani.grump`` "
"unter Verwendung von ``baby.sleep`` und ``dani.sleep`` (aus dem |parenthood|"
"_ Datensatz)."

#: ../../Ch12/Ch12_Regression_05.rst:58
msgid "Predictor"
msgstr "Prädiktor"

#: ../../Ch12/Ch12_Regression_05.rst:58
msgid "Estimate"
msgstr "Schätzung"

#: ../../Ch12/Ch12_Regression_05.rst:60
msgid "Intercept"
msgstr "Interzept"

#: ../../Ch12/Ch12_Regression_05.rst:60
msgid "125.966"
msgstr "125.966"

#: ../../Ch12/Ch12_Regression_05.rst:62
msgid "``dani.sleep``"
msgstr "``dani.sleep``"

#: ../../Ch12/Ch12_Regression_05.rst:62
msgid "-8.950"
msgstr "-8.950"

#: ../../Ch12/Ch12_Regression_05.rst:64
msgid "``baby.sleep``"
msgstr "``baby.sleep``"

#: ../../Ch12/Ch12_Regression_05.rst:64
msgid "0.011"
msgstr "0.011"

#: ../../Ch12/Ch12_Regression_05.rst:67
msgid ""
"The coefficient associated with ``dani.sleep`` is quite large, suggesting "
"that every hour of sleep I lose makes me a lot grumpier. However, the "
"coefficient for ``baby.sleep`` is very small, suggesting that it doesn’t "
"really matter how much sleep my son gets. What matters as far as my "
"grumpiness goes is how much sleep *I* get. To get a sense of what this "
"multiple regression model looks like, :numref:`fig-scatter3d_1` shows a 3D "
"plot that plots all three variables, along with the regression model itself."
msgstr ""
"Der Koeffizient für ``dani.sleep`` ist recht groß, was darauf hindeutet, "
"dass jede Stunde Schlaf, die ich verliere, mich schlechter gelaunt macht. "
"Der Koeffizient für ``baby.sleep`` ist jedoch sehr klein, was darauf "
"hindeutet, dass es nicht wirklich wichtig ist, wie viel Schlaf mein Sohn "
"bekommt. Entscheidend für meine schelchte Laune ist, wie viel Schlaf *ich* "
"bekomme. Um ein Gefühl dafür zu bekommen, wie dieses multiple "
"Regressionsmodell aussieht, zeigt :numref:`fig-scatter3d_1` ein 3D-Diagramm, "
"in dem alle drei Variablen sowie das Regressionsmodell selbst dargestellt "
"sind."

#: ../../Ch12/Ch12_Regression_05.rst:82
msgid "3D visualisation of a multiple regression model"
msgstr "3D-Visualisierung eines multiplen Regressionsmodells"

#: ../../Ch12/Ch12_Regression_05.rst:82
msgid ""
"3D visualisation of a multiple regression model: There are two predictors in "
"the model, ``dani.sleep`` and ``baby.sleep`` and the outcome variable is "
"``dani.grump``. Together, these three variables form a 3D space. Each "
"observation (dot) is a point in this space. In much the same way that a "
"simple linear regression model forms a line in 2D space, this multiple "
"regression model forms a plane in 3D space. When we estimate the regression "
"coefficients what we’re trying to do is find a plane that is as close to all "
"the blue dots as possible."
msgstr ""
"3D-Visualisierung eines multiplen Regressionsmodells: Es gibt zwei "
"Prädiktoren in dem Modell, ``dani.sleep`` und ``baby.sleep`` und die "
"Ergebnisvariable ist ``dani.grump``. Zusammen bilden diese drei Variablen "
"einen 3D-Raum. Jede Beobachtung (Punkt) ist ein Punkt in diesem Raum. "
"Genauso wie ein einfaches lineares Regressionsmodell eine Linie im 2D-Raum "
"bildet, bildet dieses multiple Regressionsmodell eine Ebene im 3D-Raum. Bei "
"der Schätzung der Regressionskoeffizienten geht es darum, eine Ebene zu "
"finden, die möglichst nahe an allen blauen Punkten liegt."

#: ../../Ch12/Ch12_Regression_05.rst:94
msgid "Formula for the general case"
msgstr "Die Formel für den allgemeinen Fall"

#: ../../Ch12/Ch12_Regression_05.rst:96
msgid ""
"The equation that I gave above shows you what a multiple regression model "
"looks like when you include two predictors. Not surprisingly, then, if you "
"want more than two predictors all you have to do is add more *X* terms and "
"more *b* coefficients. In other words, if you have K predictor variables in "
"the model then the regression equation looks like this"
msgstr ""
"Die oben dargestellte Gleichung zeigt Ihnen, wie ein multiples "
"Regressionsmodell aussieht, wenn Sie zwei Prädiktoren einbeziehen. Wenn Sie "
"also mehr als zwei Prädiktoren benötigen, müssen Sie lediglich weitere *X* "
"Terme und weitere *b* Koeffizienten hinzufügen. Mit anderen Worten: Wenn Sie "
"K Prädiktorvariablen im Modell haben, sieht die Regressionsgleichung wie "
"folgt aus"

#: ../../Ch12/Ch12_Regression_05.rst:103
msgid ""
"Y_i = b_0 + \\left( \\sum_{k=1}^K b_{k} X_{ik} \\right) + \\epsilon_i\n"
"\n"
msgstr ""
"Y_i = b_0 + \\left( \\sum_{k=1}^K b_{k} X_{ik} \\right) + \\epsilon_i\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:4
msgid "Quantifying the fit of the regression model"
msgstr "Quantifizieren der Anpassung des Regressionsmodells"

#: ../../Ch12/Ch12_Regression_06.rst:6
msgid ""
"So we now know how to estimate the coefficients of a linear regression "
"model. The problem is, we don’t yet know if this regression model is any "
"good. For example, the ``regression.1`` model *claims* that every hour of "
"sleep will improve my mood by quite a lot, but it might just be rubbish. "
"Remember, the regression model only produces a prediction *Ŷ*\\ :sub:`i` "
"about what my mood is like, but my actual mood is *Y*\\ :sub:`i`. If these "
"two are very close, then the regression model has done a good job. If they "
"are very different, then it has done a bad job."
msgstr ""
"Wir wissen jetzt also, wie man die Koeffizienten eines linearen "
"Regressionsmodells schätzt. Das Problem ist, dass wir noch nicht wissen, ob "
"dieses Regressionsmodell überhaupt etwas taugt. Zum Beispiel *behauptet* das "
"Modell ``regression.1``, dass jede Stunde Schlaf meine Stimmung um einiges "
"verbessert, aber das könnte auch einfach nur Unsinn sein. Denken Sie daran, "
"dass das Regressionsmodell nur eine Vorhersage *Ŷ*\\ :sub:`i` darüber macht, "
"wie meine Stimmung ist, während *Y*\\ :sub:`i` meine tatsächliche Stimmung "
"beschreibt. Wenn diese beiden Werte sehr nahe beieinander liegen, dann hat "
"das Regressionsmodell gute Arbeit geleistet. Wenn sie sehr unterschiedlich "
"sind, hat es schlechte Arbeit geleistet."

#: ../../Ch12/Ch12_Regression_06.rst:16
msgid "The *R²* (R-squared) value"
msgstr "Der *R²* (R-Quadrat) Wert"

#: ../../Ch12/Ch12_Regression_06.rst:18
msgid ""
"Once again, let’s wrap a little bit of mathematics around this. Firstly, "
"we’ve got the sum of the squared residuals"
msgstr ""
"Lassen Sie uns auch hier ein wenig Mathematik anwenden. Erstens haben wir "
"die Summe der quadrierten Residuen"

#: ../../Ch12/Ch12_Regression_06.rst:21
msgid ""
"\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"
msgstr ""
"\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:23
msgid ""
"which we would hope to be pretty small. Specifically, what we’d like is for "
"it to be very small in comparison to the total variability in the outcome "
"variable"
msgstr ""
"von der wir hoffen, dass sie ziemlich klein ist. Konkret wünschen wir uns, "
"dass sie im Vergleich zur Gesamtvariabilität der Ergebnisvariablen sehr "
"klein ist"

#: ../../Ch12/Ch12_Regression_06.rst:27
msgid ""
"\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2\n"
"\n"
msgstr ""
"\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:29
msgid ""
"While we’re here, let’s calculate these values ourselves, not by hand "
"though. Let’s use jamovi instead. Open up the |parenthood|_ data set in so "
"that we can work in it. The first thing to do is calculate the *Ŷ* values, "
"and for the simple model that uses only a single predictor we would do the "
"following:"
msgstr ""
"Wenn wir schon dabei sind, sollten wir diese Werte selbst berechnen, "
"allerdings nicht von Hand. Verwenden wir stattdessen jamovi. Öffnen Sie den "
"Datensatz |parenthood|_, damit wir darin arbeiten können. Als Erstes müssen "
"wir die *Ŷ* Werte berechnen, und für das einfache Modell, das nur einen "
"einzigen Prädiktor verwendet, würden wir folgendes tun:"

#: ../../Ch12/Ch12_Regression_06.rst:34
msgid ""
"Go to an empty column (at the end of the data set) and double click on the "
"column header, choose “New computed variable” and enter ``Y_pred`` in the "
"first line and the formula ``125.97 + (-8.94 * dani.sleep)`` in the line "
"starting with ``=`` (next to the *f*\\ :sub:`x`)."
msgstr ""
"Gehen Sie zu einer leeren Spalte (am Ende des Datensatzes) und doppelklicken "
"Sie auf die Spaltenüberschrift, wählen Sie ``New computed variable`` und "
"geben Sie ``Y_pred`` in die erste Zeile und die Formel ``125.97 + (-8.94 * "
"dani.sleep)`` in die Zeile beginnend mit ``=`` (neben *f*\\ :sub:`x`) ein."

#: ../../Ch12/Ch12_Regression_06.rst:39
msgid ""
"Okay, now that we’ve got a variable which stores the regression model "
"predictions for how grumpy I will be on any given day, let’s calculate our "
"sum of squared residuals. We would do that using the following formula:"
msgstr ""
"Damit haben wir nun eine Variable, welche die Vorhersagen des "
"Regressionsmodells dafür, wie mürrisch ich an einem bestimmten Tag sein "
"werde, enthält. Lassen Sie uns jetzt die Summe der quadrierten Residuen "
"berechnen. Dazu verwenden wir die folgende Formel:"

#: ../../Ch12/Ch12_Regression_06.rst:44
msgid ""
"Calculate the squared residuals by creating a new column called ``sq_resid`` "
"using the formula ``(dani.grump - Y_pred) ^ 2``. The values in this column "
"are later summed up to obtain SS\\ :sub:`res`."
msgstr ""
"Zum Berechnen der quadrierten Residuen in jamovi, fügen Sie eine neue Spalte "
"mit der Bezeichnung ``sq_resid`` hinzu und verwenden Sie die Formel ``("
"dani.grump - Y_pred) ^ 2`` für diese Variable. Die Werte in dieser Spalte "
"werden später aufsummiert, um SS\\ :sub:`res` zu erhalten."

#: ../../Ch12/Ch12_Regression_06.rst:48
msgid ""
"Calculate the squared deviation from the mean by creating yet another "
"column  called ``sq_total`` using the formula ``(dani.grump - VMEAN(dani."
"grump)) ^ 2``. The values in this column are later summed up to obtain SS\\ :"
"sub:`tot`."
msgstr ""
"Berechnen Sie die quadratische Abweichung vom Mittelwert, indem Sie eine "
"weitere Spalte mit der Bezeichnung ``sq_total`` unter Verwendung der Formel "
"``(dani.grump - VMEAN(dani.grump)) ^ 2`` erstellen. Die Werte in dieser "
"Spalte werden später aufsummiert, um SS\\ :sub:`tot` zu erhalten."

#: ../../Ch12/Ch12_Regression_06.rst:53
msgid ""
"To calculate the sum of these values, click ``Descriptives`` → ``Descriptive "
"Statistics`` and move ``sq_resid`` and ``sq_total`` to the ``Variables`` "
"box. You’ll then need to select ``Sum`` from the ``Statistics`` drop-down "
"menu below. The sum of ``sq_resid`` has a value of **1838.722**. This is a "
"big number, however, that doesn’t mean very much. The sum of ``sq_total`` "
"has a value of **9998.590**. Well, it’s a much (about five times) bigger "
"number than the last one, so this does suggest that our regression model was "
"making good predictions (that is, it has greatly reduced the residual error "
"compared to the model that uses the mean as a single predictor). But it’s "
"not very interpretable."
msgstr ""
"Um die Summe dieser Werte zu berechnen, klicken Sie auf ``Descriptives`` → ``"
"Descriptive Statistics`` und verschieben Sie ``sq_resid`` und ``sq_total`` "
"in das Feld ``Variables``. Anschließend müssen Sie ``Sum`` aus dem Dropdown-"
"Menü ``Statistics`` auswählen. Die Summe von ``sq_resid`` hat einen Wert von "
"**1838.722**. Dies ist eine große Zahl, die jedoch nicht viel aussagt. Die "
"Summe von ``sq_total`` hat einen Wert von **9998.590**. Das ist eine viel ("
"etwa fünfmal) größere Zahl als die letzte, was darauf hindeutet, dass unser "
"Regressionsmodell gute Vorhersagen macht (d. h. es hat den Restfehler im "
"Vergleich zu dem Modell, das den Mittelwert als einzigen Prädiktor "
"verwendet, stark reduziert). Aber diese Zahlen sind nicht so einfach "
"interpretierbar."

#: ../../Ch12/Ch12_Regression_06.rst:64
msgid ""
"To can fix this, we’d like to convert these two fairly meaningless numbers "
"into one number. A nice, interpretable number, which for no particular "
"reason we’ll call *R²*. What we would like is for the value of *R²* to be "
"equal to 1 if the regression model makes no errors in predicting the data. "
"In other words, if it turns out that the residual errors are zero. That is, "
"if SS\\ :sub:`res` = 0 then we expect *R²* = 1. Similarly, if the model is "
"completely useless, we would like *R²* to be equal to 0. What do I mean by "
"“useless”? Tempting as it is to demand that the regression model move out of "
"the house, cut its hair and get a real job, I’m probably going to have to "
"pick a more practical definition. In this case, all I mean is that the "
"residual sum of squares is no smaller than the total sum of squares, SS\\ :"
"sub:`res` = SS\\ :sub:`tot`. Wait, why don’t we do exactly that? The formula "
"that provides us with our *R²* value is pretty simple to write down, and "
"equally simple to calculate by hand:\\ [#]_"
msgstr ""
"Um dies zu beheben, möchten wir diese beiden ziemlich bedeutungslosen Zahlen "
"in eine einzige Zahl umwandeln. Eine schöne, interpretierbare Zahl, die wir "
"aus keinem besonderen Grund *R²* nennen. Wir möchten, dass der Wert von *R²* "
"gleich 1 ist, wenn das Regressionsmodell bei der Vorhersage der Daten keine "
"Fehler macht. Mit anderen Worten, wenn sich herausstellt, dass die "
"Restfehler gleich Null sind. Das heißt, wenn SS\\ :sub:`res` = 0 ist, dann "
"erwarten wir *R²* = 1. Ähnlich verhält es sich, wenn das Modell völlig "
"nutzlos ist: dann sollte *R²* gleich 0 sein. Was meine ich mit „nutzlos“? "
"Hierfür muss ich wahrscheinlich eine praktischere Definition wählen. In "
"diesem Fall meine ich nur, dass die Quadratsumme der Residuen nicht kleiner "
"ist als die Gesamtquadratsumme, SS\\ :sub:`res` = SS\\ :sub:`tot`. Die "
"Formel, die uns unseren *R²* Wert liefert, ist daher ziemlich einfach "
"aufzuschreiben und ebenso einfach von Hand zu berechnen:\\ [#]_"

#: ../../Ch12/Ch12_Regression_06.rst:78
msgid "R² = 1 - (SS\\ :sub:`res` / SS\\ :sub:`tot`)"
msgstr "R² = 1 - (SS\\ :sub:`res` / SS\\ :sub:`tot`)"

#: ../../Ch12/Ch12_Regression_06.rst:79
msgid "R² = 1 - (1838.722 / 9998.590)"
msgstr "R² = 1 - (1838.722 / 9998.590)"

#: ../../Ch12/Ch12_Regression_06.rst:80
msgid "R² = 1 - 0.184"
msgstr "R² = 1 - 0.184"

#: ../../Ch12/Ch12_Regression_06.rst:82
#, python-format
msgid ""
"This gives a value for R² of **0.816**. The R² value, sometimes called the "
"**coefficient of determination**\\ [#]_ has a simple interpretation: it is "
"the *proportion* of the variance in the outcome variable that can be "
"accounted for by the predictor. So, in this case the fact that we have "
"obtained *R²* = 0.816 means that the predictor (``dani.sleep``) explains "
"81.6\\% of the variance in the outcome (``dani.grump``).\\ [#]_"
msgstr ""
"Dies ergibt einen Wert für R² von **0,816**. Der R²-Wert, der manchmal auch "
"als **Bestimmtheitsmaß**\\ [#]_ bezeichnet wird, hat eine einfache "
"Interpretation: Er ist der *Anteil* der Varianz in der Ergebnisvariablen, "
"der durch einen Prädiktor erklärt werden kann. In diesem Fall bedeutet die "
"Tatsache, dass wir *R²* = 0,816 erhalten haben, dass der Prädiktor (``dani."
"sleep``) 81,6 % der Varianz des Ergebnisses (``dani.grump``) erklärt.\\ [#]_"

#: ../../Ch12/Ch12_Regression_06.rst:89
msgid ""
"Naturally, you don’t actually need to do all these calculations yourself if "
"you want to obtain the *R²* value for your regression model. As we’ll see "
"later on in :doc:`Running the hypothesis tests in jamovi <../Ch12/"
"Ch12_Regression_07>`, all you need to do is specify this as an option in "
"jamovi. However, let’s put that to one side for the moment. There’s another "
"property of *R²* that I want to point out."
msgstr ""
"Natürlich müssen Sie all diese Berechnungen nicht selbst durchführen, wenn "
"Sie den *R²* Wert für Ihr Regressionsmodell erhalten möchten. Wie wir später "
"in :doc:`Durchführen von Hypothesentests für Regressionsmodelle <../Ch12/"
"Ch12_Regression_07>` sehen werden, müssen Sie dies lediglich als Option in "
"jamovi angeben. Aber lassen wir das für den Moment beiseite. Es gibt eine "
"weitere Eigenschaft von *R²*, auf die ich hinweisen möchte."

#: ../../Ch12/Ch12_Regression_06.rst:97
msgid "The relationship between regression and correlation"
msgstr "Die Beziehung zwischen Regression und Korrelation"

#: ../../Ch12/Ch12_Regression_06.rst:99
msgid ""
"At this point we can revisit my earlier claim that regression, in this very "
"simple form that I’ve discussed so far, is basically the same thing as a "
"correlation. Previously, we used the symbol *r* to denote a Pearson "
"correlation. Might there be some relationship between the value of the "
"correlation coefficient *r* and the *R²* value from linear regression? Of "
"course there is: the squared correlation *r²* is identical to the *R²* value "
"for a linear regression with only a single predictor. In other words, "
"running a Pearson correlation is more or less equivalent to running a linear "
"regression model that uses only one predictor variable."
msgstr ""
"An dieser Stelle können wir noch einmal auf meine frühere Behauptung "
"zurückkommen, dass die Regression in dieser sehr einfachen Form, die ich "
"bisher besprochen habe, im Grunde dasselbe ist wie eine Korrelation. Zuvor "
"haben wir das Symbol *r* verwendet, um eine Pearson-Korrelation zu "
"bezeichnen. Könnte es eine Beziehung zwischen dem Wert des "
"Korrelationskoeffizienten *r* und dem Wert *R²* bei der linearen Regression "
"geben? Natürlich: Die quadrierte Korrelation *r²* ist identisch mit dem Wert "
"*R²* für eine lineare Regression mit nur einem einzigen Prädiktor. Mit "
"anderen Worten: Die Durchführung einer Pearson-Korrelation ist mehr oder "
"weniger gleichbedeutend mit der Durchführung eines linearen "
"Regressionsmodells, das nur eine Prädiktorvariable verwendet."

#: ../../Ch12/Ch12_Regression_06.rst:110
msgid "The adjusted *R²* (R-squared) value"
msgstr "Der adjustierte *R²* (R-Quadrat) Wert"

#: ../../Ch12/Ch12_Regression_06.rst:112
msgid ""
"One final thing to point out before moving on. It’s quite common for people "
"to report a slightly different measure of model performance, known as "
"“adjusted *R²*”. The motivation behind calculating the adjusted *R²* value "
"is the observation that adding more predictors into the model will *always* "
"cause the *R²* value to increase (or at least not decrease)."
msgstr ""
"Bevor wir fortfahren, möchte ich noch auf eine letzte Sache hinweisen. Es "
"ist durchaus üblich, ein weiteres Maß für die Modellgüte anzugeben, das als „"
"adjustiertes *R²*“ bekannt ist. Der Grund für die Berechnung des "
"adjustierten *R²*-Wertes ist die Beobachtung, dass das Hinzufügen weiterer "
"Prädiktoren zum Modell *immer* dazu führt, dass der *R²* Wert steigt (oder "
"zumindest nicht sinkt)."

#: ../../Ch12/Ch12_Regression_06.rst:119
msgid ""
"The adjusted *R²* value introduces a slight change to the calculation, as "
"follows. For a regression model with K predictors, fit to a data set "
"containing *N* observations, the adjusted *R²* is:"
msgstr ""
"Der adjustierte *R²*-Wert führt zu einer geringfügigen Änderung der "
"Berechnung. Für ein Regressionsmodell mit K Prädiktoren, das an einen "
"Datensatz mit *N* Beobachtungen angepasst wird, ist das adjustierte *R²*:"

#: ../../Ch12/Ch12_Regression_06.rst:124
msgid ""
"\\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} "
"\\times \\frac{N-1}{N-K-1} \\right)\n"
"\n"
msgstr ""
"\\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} "
"\\times \\frac{N-1}{N-K-1} \\right)\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:126
msgid ""
"This adjustment is an attempt to take the degrees of freedom into account. "
"The big advantage of the adjusted *R²* value is that when you add more "
"predictors to the model, the adjusted *R²* value will only increase if the "
"new variables improve the model performance more than you’d expect by "
"chance. The big disadvantage is that the adjusted *R²* value *can’t* be "
"interpreted in the elegant way that *R²* can. *R²* has a simple "
"interpretation as the proportion of variance in the outcome variable that is "
"explained by the regression model. To my knowledge, no equivalent "
"interpretation exists for adjusted *R²*."
msgstr ""
"Diese Anpassung ist ein Versuch, die Freiheitsgrade zu berücksichtigen. Der "
"große Vorteil des adjustierten *R²*-Wertes besteht darin, dass der "
"adjustierte *R²*-Wert nur dann ansteigt, wenn die neuen Variablen die "
"Leistung des Modells stärker verbessern, als man zufällig erwarten würde, "
"wenn man dem Modell weitere Prädiktoren hinzufügt. Der große Nachteil ist, "
"dass der adjustierte *R²* Wert *nicht* mehr auf die elegante Art und Weise "
"interpretiert werden kann, wie das für *R²* der Fall ist. *R²* lässt sich "
"einfach als der Anteil der Varianz in der Ergebnisvariablen interpretieren, "
"der durch das Regressionsmodell erklärt wird. Meines Wissens gibt es keine "
"gleichwertige Interpretation für das adjustierte *R²*."

#: ../../Ch12/Ch12_Regression_06.rst:137
msgid ""
"An obvious question then is whether you should report *R²* or adjusted *R²*. "
"This is probably a matter of personal preference. If you care more about "
"interpretability, then *R²* is better. If you care more about correcting for "
"bias, then adjusted *R²* is probably better. Speaking just for myself, I "
"prefer *R²*. My feeling is that it’s more important to be able to interpret "
"your measure of model performance. Besides, as we’ll see in section :doc:"
"`Hypothesis tests for regression models <../Ch12/Ch12_Regression_07>`, if "
"you’re worried that the improvement in *R²* that you get by adding a "
"predictor is just due to chance and not because it’s a better model, well "
"we’ve got hypothesis tests for that."
msgstr ""
"Eine offensichtliche Frage ist, ob Sie *R²* oder das adjustierte *R²* "
"angeben sollten. Dies ist wahrscheinlich eine Frage der persönlichen "
"Vorliebe. Wenn Sie mehr Wert auf die Interpretierbarkeit legen, dann ist *R²*"
" besser. Wenn Sie mehr Wert auf die Korrektur von Verzerrungen legen, dann "
"ist das adjustierte *R²* wahrscheinlich besser. Ich persönlich bevorzuge *R²*"
". Meines Erachtens ist es wichtiger, dass Sie Ihr Maß für die Modellgüte "
"interpretieren können. Außerdem werden wir in Abschnitt :doc:`Durchführen "
"von Hypothesentests für Regressionsmodelle <../Ch12/Ch12_Regression_07>` "
"sehen, dass sich mit Hypothesentests überprüfen lässt, ob das Hinzufügen "
"eines Prädiktors zu einem besseren Modell führt, oder ob sich die "
"Verbesserung von *R²*, die Sie durch das Hinzufügen erhalten, ausschließlich "
"auf zufällige Variation zurückzuführen lässt."

#: ../../Ch12/Ch12_Regression_06.rst:151
msgid ""
"If you don't want to do these calculations by hand, just create another "
"computed variable called, e.g., ``R2``, and containing the formula ``1 - "
"VSUM(sq_resid) / VSUM(sq_total)``. But then you have a whole column "
"containing R²."
msgstr ""
"Wenn Sie diese Berechnungen nicht von Hand durchführen möchten, erstellen "
"Sie einfach eine weitere berechnete Variable, die z. B. ``R2`` heißt und die "
"Formel ``1 - VSUM(sq_resid) / VSUM(sq_total)`` enthält. Aber dann haben Sie "
"eine ganze Spalte mit R²."

#: ../../Ch12/Ch12_Regression_06.rst:157
msgid ""
"And by “sometimes” I mean “almost never”. In practice everyone just calls it "
"“*R*-squared”."
msgstr ""
"Und mit „manchmal“ meine ich „fast nie“. In der Praxis nennt man es einfach "
"„*R*-Quadrat“ (oder *R squared*, wenn man den englischen Begriff benutzt)."

#: ../../Ch12/Ch12_Regression_06.rst:161
msgid ""
"If you made a mistake or could not follow the explanations, you can simply "
"download and open the |parenthood_r2|_ data set."
msgstr ""
"Wenn Sie einen Fehler gemacht haben oder den Erklärungen nicht folgen "
"konnten, können Sie auch einfach den Datensatz |parenthood_r2|_ "
"herunterladen und öffnen."

#: ../../Ch12/Ch12_Regression_07.rst:4
msgid "Hypothesis tests for regression models"
msgstr "Durchführen von Hypothesentests für Regressionsmodelle"

#: ../../Ch12/Ch12_Regression_07.rst:6
msgid ""
"So far we’ve talked about what a regression model is, how the coefficients "
"of a regression model are estimated, and how we quantify the performance of "
"the model (the last of these, incidentally, is basically our measure of "
"effect size). The next thing we need to talk about is hypothesis tests. "
"There are two different (but related) kinds of hypothesis tests that we need "
"to talk about: those in which we test whether the regression model as a "
"whole is performing significantly better than a null model, and those in "
"which we test whether a particular regression coefficient is significantly "
"different from zero."
msgstr ""
"Bisher haben wir darüber gesprochen, was ein Regressionsmodell ist, wie die "
"Koeffizienten eines Regressionsmodells geschätzt werden und wie wir die "
"Leistung des Modells quantifizieren (der letzte Punkt ist übrigens unser Maß "
"für die Effektstärke). Der nächste Punkt, über den wir sprechen müssen, sind "
"Hypothesentests. Es gibt zwei verschiedene (aber verwandte) Arten von "
"Hypothesentests, über die wir sprechen müssen: solche, bei denen wir testen, "
"ob das Regressionsmodell insgesamt signifikant besser abschneidet als ein "
"Nullmodell, und solche, bei denen wir testen, ob ein bestimmter "
"Regressionskoeffizient sich signifikant von Null unterscheidet."

#: ../../Ch12/Ch12_Regression_07.rst:17
msgid "Testing the model as a whole"
msgstr "Testen des gesamten Modells"

#: ../../Ch12/Ch12_Regression_07.rst:19
msgid ""
"Okay, suppose you’ve estimated your regression model. The first hypothesis "
"test you might try is the null hypothesis that there is *no relationship* "
"between the predictors and the outcome, and the alternative hypothesis that "
"*the data are distributed in exactly the way that the regression model "
"predicts.*"
msgstr ""
"Nehmen wir an, Sie haben Ihr Regressionsmodell geschätzt. Der erste "
"Hypothesentest, den Sie versuchen könnten, ist die Nullhypothese, dass es *"
"keine Beziehung* zwischen den Prädiktoren und dem Ergebnis gibt, und die "
"Alternativhypothese, dass *die Daten genau so verteilt sind, wie es das "
"Regressionsmodell vorhersagt.*"

#: ../../Ch12/Ch12_Regression_07.rst:25
msgid ""
"Formally, our “null model” corresponds to the fairly trivial “regression” "
"model in which we include 0 predictors and only include the intercept term "
"*b*\\ :sub:`0`:"
msgstr ""
"Formal entspricht unser „Nullmodell“ dem recht trivialen „Regressionsmodell“"
", bei dem wir keine Prädiktoren und nur den Interzept-Term *b*\\ :sub:`0` "
"einbeziehen:"

#: ../../Ch12/Ch12_Regression_07.rst:29
msgid "H\\ :sub:`0`: *Y*\\ :sub:`i` = b\\ :sub:`0` + ε\\ :sub:`i`"
msgstr "H\\ :sub:`0`: *Y*\\ :sub:`i` = b\\ :sub:`0` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_07.rst:31
msgid ""
"If our regression model has K predictors, the “alternative model” is "
"described using the usual formula for a multiple regression model:"
msgstr ""
"Unser „Alternativmodell“ wird mit der üblichen Formel für ein multiples "
"Regressionsmodell mit K Prädiktoren beschrieben:"

#: ../../Ch12/Ch12_Regression_07.rst:35
msgid ""
"H\\ :sub:`1`: *Y*\\ :sub:`i` = b\\ :sub:`0` + math:`\\left( \\sum_{k=1}^K "
"b_{k} X_{ik} \\right)` + ε\\ :sub:`i`"
msgstr ""
"H\\ :sub:`1`: *Y*\\ :sub:`i` = b\\ :sub:`0` + math:`\\left( \\sum_{k=1}^K "
"b_{k} X_{ik} \\right)` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_07.rst:37
msgid ""
"How can we test these two hypotheses against each other? The trick is to "
"understand that it’s possible to divide up the total variance SS\\ :sub:"
"`tot` into the sum of the residual variance SS\\ :sub:`res` and the "
"regression model variance SS\\ :sub:`mod`. I’ll skip over the "
"technicalities, since we’ll get to that later when we look at ANOVA in "
"chapter :doc:`Comparing several means (one-way ANOVA) <../Ch13/Ch13_ANOVA>`. "
"But just note that"
msgstr ""
"Wie können wir diese beiden Hypothesen gegeneinander testen? Der Trick "
"besteht darin, zu verstehen, dass es möglich ist, die totale Quadratsumme "
"SS\\ :sub:`tot` in die Quadratsumme der Residuen SS\\ :sub:`res` und die "
"durch das Regressionsmodell erklärte Quadratsumme SS\\ :sub:`mod` "
"aufzuteilen (und da die Quadratsumme nichts anderes ist als die Varianz "
"multipliziert mit *N* bzw. *N* - 1 hätte ich genauso gut Varianz an Stelle "
"von Quadratsumme schreiben können). Ich überspringe die technischen Details, "
"da wir dazu später kommen, wenn wir uns die ANOVA in Kapitel :doc:`Vergleich "
"mehrerer Mittelwerte (einfaktorielle ANOVA) <../Ch13/Ch13_ANOVA>` ansehen. "
"Aber beachten Sie einfach, dass"

#: ../../Ch12/Ch12_Regression_07.rst:44
msgid "SS\\ :sub:`mod` = SS\\ :sub:`tot` - SS\\ :sub:`res`"
msgstr "SS\\ :sub:`mod` = SS\\ :sub:`tot` - SS\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:46
msgid ""
"And we can convert the sums of squares into mean squares by dividing by the "
"degrees of freedom."
msgstr ""
"Und wir können die Quadratsummen in mittlere quadrierte Abweichungen "
"umwandeln, indem wir sie durch die Freiheitsgrade teilen."

#: ../../Ch12/Ch12_Regression_07.rst:49
msgid "MS\\ :sub:`mod` = SS\\ :sub:`mod` / *df*\\ :sub:`mod`"
msgstr "MS\\ :sub:`mod` = SS\\ :sub:`mod` / *df*\\ :sub:`mod`"

#: ../../Ch12/Ch12_Regression_07.rst:50
msgid "SS\\ :sub:`res` = SS\\ :sub:`res` / *df*\\ :sub:`res`"
msgstr "SS\\ :sub:`res` = SS\\ :sub:`res` / *df*\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:52
msgid ""
"So, how many degrees of freedom do we have? As you might expect the *df* "
"associated with the model is closely tied to the number of predictors that "
"we’ve included. In fact, it turns out that *df*\\ :sub:`mod` = K. For the "
"residuals the total degrees of freedom is *df*\\ :sub:`res` = N - K - 1."
msgstr ""
"Wie viele Freiheitsgrade haben wir also? Wie zu erwarten, ist der *df* des "
"Modells eng mit der Anzahl der Prädiktoren verbunden, die wir einbezogen "
"haben. Tatsächlich stellt sich heraus, dass *df*\\ :sub:`mod` = *K*. Für die "
"Residuen sind die gesamten Freiheitsgrade *df*\\ :sub:`res` = *N* - *K* - 1."

#: ../../Ch12/Ch12_Regression_07.rst:58
msgid ""
"Now that we’ve got our mean square values we can calculate an *F*-statistic "
"like this"
msgstr ""
"Da wir nun unsere mittleren Quadratwerte haben, können wir eine *F*-"
"Statistik wie folgt berechnen"

#: ../../Ch12/Ch12_Regression_07.rst:61
msgid "F = MS\\ :sub:`mod` / SS\\ :sub:`res`"
msgstr "F = MS\\ :sub:`mod` / SS\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:63
msgid "and the degrees of freedom associated with this are K and N - K - 1."
msgstr "und die damit verbundenen Freiheitsgrade sind *K* und *N* - *K* - 1."

#: ../../Ch12/Ch12_Regression_07.rst:66
msgid ""
"We’ll see much more of the *F* statistic in chapter :doc:`Comparing several "
"means (one-way ANOVA) <../Ch13/Ch13_ANOVA>`, but for now just know that we "
"can interpret large *F* values as indicating that the null hypothesis is "
"performing poorly in comparison to the alternative hypothesis. In a moment "
"I’ll show you how to do the test in jamovi the easy way, but first let’s "
"have a look at the tests for the individual regression coefficients."
msgstr ""
"Wir werden mehr über die *F*-Statistik in Kapitel :doc:`Vergleich mehrerer "
"Mittelwerte (einfaktorielle ANOVA) <../Ch13/Ch13_ANOVA>` erfahren, aber für "
"den Moment reicht es, wenn wir wissen, dass wir große *F*-Werte als Hinweis "
"darauf interpretieren können, dass die Nullhypothese im Vergleich zur "
"Alternativhypothese schlecht abschneidet. In Kürze werde ich Ihnen zeigen, "
"wie Sie den Test in jamovi auf einfache Weise durchführen können, aber sehen "
"wir uns zunächst die Tests für die einzelnen Regressionskoeffizienten an."

#: ../../Ch12/Ch12_Regression_07.rst:74
msgid "Tests for individual coefficients"
msgstr "Tests für einzelne Koeffizienten"

#: ../../Ch12/Ch12_Regression_07.rst:76
msgid ""
"The *F*-test that we’ve just introduced is useful for checking that the "
"model as a whole is performing better than chance. If your regression model "
"doesn’t produce a significant result for the *F*-test then you probably "
"don’t have a very good regression model (or, quite possibly, you don’t have "
"very good data). However, while failing this test is a pretty strong "
"indicator that the model has problems, *passing* the test (i.e., rejecting "
"the null) doesn’t imply that the model is good! Why is that, you might be "
"wondering? The answer to that can be found by looking at the coefficients "
"for the multiple regression model we have already looked at :numref:`tab-"
"parent_coeff` in section :doc:`Multiple linear regression <../Ch12/"
"Ch12_Regression_05>` above, where the coefficients we got were 125.966 (for "
"the intercept), -8.950 (for ``dani.sleep``) and 0.011 (for ``baby.sleep``). "
"I can’t help but notice that the estimated regression coefficient for the "
"``baby.sleep`` variable is tiny, relative to the value that we get for "
"``dani.sleep``. Given that these two variables are absolutely on the same "
"scale (they’re both measured in “hours slept”), I find this illuminating. In "
"fact, I’m beginning to suspect that it’s really only the amount of sleep "
"that *I* get that matters in order to predict my grumpiness."
msgstr ""
"Der *F*-Test, den wir gerade eingeführt haben, ist nützlich, um zu "
"überprüfen, ob das Modell als Ganzes besser als der Zufall funktioniert. "
"Wenn Ihr Regressionsmodell kein signifikantes Ergebnis für den *F*-Test "
"liefert, dann haben Sie wahrscheinlich kein sehr gutes Regressionsmodell ("
"oder Sie haben wahrscheinlich eher keine sehr guten Daten). Das "
"Nichtbestehen dieses Tests ist zwar ein ziemlich starker Indikator dafür, "
"dass das Modell Probleme hat, aber das *Bestehen* des Tests (d. h. die "
"Zurückweisung der Nullhypothese bzw. des Nullmodells) bedeutet nicht "
"zwanghsläufig, dass das Modell gut ist! Sie fragen Sie sich vielleicht, "
"warum das so ist? Die Antwort darauf finden Sie, wenn Sie sich die "
"Koeffizienten des multiplen Regressionsmodells ansehen, das wir bereits "
"unter :numref:`tab-parent_coeff` im Abschnitt :doc:`Multiple lineare "
"Regression <../Ch12/Ch12_Regression_05>` betrachtet haben, wo die "
"Koeffizienten 125,966 (für das Interzept), -8,950 (für ``dani.sleep``) und "
"0,011 (für ``baby.sleep``) betragen. Es fällt auf, dass der geschätzte "
"Regressionskoeffizient für die Variable ``baby.sleep`` winzig ist, im "
"Vergleich zu dem Wert, den wir für ``dani.sleep`` erhalten. Ich finde dies "
"besonders aufschlussreich, in Anbetracht der Tatsache, dass diese beiden "
"Variablen absolut auf der gleichen Skala liegen (sie werden beide als „"
"geschlafene Stunden“ gemessen). In der Tat beginne ich zu vermuten, dass es "
"wirklich nur auf die Menge an Schlaf ankommt, die *ich* bekomme, wenn es "
"darum geht, meine schlechte Laune vorherzusagen."

#: ../../Ch12/Ch12_Regression_07.rst:94
msgid ""
"We can re-use a hypothesis test that we discussed earlier, the *t*-test. The "
"test that we’re interested in has a null hypothesis that the true regression "
"coefficient is zero (*b* = 0), which is to be tested against the alternative "
"hypothesis that it isn’t (*b* ≠ 0). That is:"
msgstr ""
"Wir können einen Hypothesentest wiederverwenden, den wir bereits besprochen "
"haben, den *t*-Test. Der Test, an dem wir interessiert sind, hat die "
"Nullhypothese, dass der wahre Regressionskoeffizient Null ist (*b* = 0), die "
"gegen die Alternativhypothese, dass er es nicht ist (*b* ≠ 0), getestet "
"werden soll. Das heißt:"

#: ../../Ch12/Ch12_Regression_07.rst:100
msgid "H\\ :sub:`0`: *b* = 0"
msgstr "H\\ :sub:`0`: *b* = 0"

#: ../../Ch12/Ch12_Regression_07.rst:101
msgid "H\\ :sub:`1`: *b* ≠ 0"
msgstr "H\\ :sub:`1`: *b* ≠ 0"

#: ../../Ch12/Ch12_Regression_07.rst:103
msgid ""
"How can we test this? Well, if the central limit theorem is kind to us we "
"might be able to guess that the sampling distribution of :math:`\\hat{b}`, "
"the estimated regression coefficient, is a normal distribution with mean "
"centred on *b*. What that would mean is that if the null hypothesis were "
"true, then the sampling distribution of :math:`\\hat{b}` has mean zero and "
"unknown standard deviation. Assuming that we can come up with a good "
"estimate for the standard error of the regression coefficient, :math:"
"`SE(\\hat{b})`, then we’re in luck. That’s *exactly* the situation for which "
"we introduced the one-sample *t*-test way back in chapter :doc:`Comparing "
"two means <../Ch11/Ch11_tTest>`. So let’s define a *t*-statistic like this:"
msgstr ""
"Wie können wir das testen? Wenn der zentrale Grenzwertsatz uns wohlgesonnen "
"ist, können wir vermuten, dass die Stichprobenverteilung von :math:`\\hat{b}`"
", dem geschätzten Regressionskoeffizienten, eine Normalverteilung mit einem "
"Mittelwert ist, der auf *b* zentriert ist. Das würde bedeuten, wenn die "
"Nullhypothese wahr wäre, hätte die Stichprobenverteilung von :math:`\\hat{b}`"
" einen Mittelwert von Null und eine unbekannte Standardabweichung. Wenn wir "
"einen guten Schätzwert für den Standardfehler des Regressionskoeffizienten "
":math:`SE(\\hat{b})` finden können, dann können wir die Hypothese testen. "
"Das ist *genau* die Situation, für die wir in Kapitel :doc:`Vergleich zweier "
"Mittelwerte <../Ch11/Ch11_tTest>` den *t*-Test bei einer Stichprobe "
"eingeführt haben. Definieren wir also eine *t*-Statistik wie folgt:"

#: ../../Ch12/Ch12_Regression_07.rst:114
msgid ""
"t = \\frac{\\hat{b}}{SE(\\hat{b})}\n"
"\n"
msgstr ""
"t = \\frac{\\hat{b}}{SE(\\hat{b})}\n"
"\n"

#: ../../Ch12/Ch12_Regression_07.rst:116
msgid ""
"I’ll skip over the reasons why, but our degrees of freedom in this case are "
"*df* = N - K - 1. Irritatingly, the estimate of the standard error of the "
"regression coefficient, :math:`SE(\\hat{b})`, is not as easy to calculate as "
"the standard error of the mean that we used for the simpler *t*-tests in "
"chapter :doc:`Comparing two means <../Ch11/Ch11_tTest>`. In fact, the "
"formula is somewhat ugly, and not terribly helpful to look at.\\ [#]_ For "
"our purposes it’s sufficient to point out that the standard error of the "
"estimated regression coefficient depends on both the predictor and outcome "
"variables, and it is somewhat sensitive to violations of the homogeneity of "
"variance assumption (discussed shortly)."
msgstr ""
"Ich überspringe die Gründe dafür, aber unsere Freiheitsgrade sind in diesem "
"Fall *df* = *N* - *K* - 1. Irritierenderweise ist die Schätzung des "
"Standardfehlers des Regressionskoeffizienten, :math:`SE(\\hat{b})`, nicht so "
"einfach zu berechnen wie der Standardfehler des Mittelwertes, den wir für "
"die einfacheren *t*-Tests in Kapitel :doc:`Vergleich zweier Mittelwerte <../"
"Ch11/Ch11_tTest>` verwendet haben. Tatsächlich ist die Formel etwas hässlich "
"und nicht sehr hilfreich.\\ [#]_ Für unsere Zwecke reicht es aus, darauf "
"hinzuweisen, dass der Standardfehler des geschätzten "
"Regressionskoeffizienten sowohl von den Prädiktor- als auch von der "
"Ergebnisvariablen abhängt und empfindlich auf Verletzungen der Annahme der "
"Varianzhomogenität reagiert (die in Kürze besprochen wird)."

#: ../../Ch12/Ch12_Regression_07.rst:127
msgid ""
"In any case, this *t*-statistic can be interpreted in the same way as the "
"*t*-statistics that we discussed in chapter :doc:`Comparing two means <../"
"Ch11/Ch11_tTest>`. Assuming that you have a two-sided alternative (i.e., you "
"don’t really care if *b* > 0 or *b* < 0), then it’s the extreme values of "
"*t* (i.e., a lot less than zero or a lot greater than zero) that suggest "
"that you should reject the null hypothesis."
msgstr ""
"In jedem Fall kann diese *t*-Statistik auf die gleiche Weise interpretiert "
"werden wie die *t*-Statistik, die wir in Kapitel :doc:`Vergleich zweier "
"Mittelwerte <../Ch11/Ch11_tTest>` besprochen haben. Angenommen, Sie haben "
"eine zweiseitige Alternative (d.h. es ist Ihnen egal, ob *b* > 0 oder *b* < "
"0), dann sind es hohe (Absolut-)Werte von *t* (d.h. hohe Werte mit entweder "
"negativem oder ohne, .d.h. positivem, Vorzeichen), die darauf hindeuten, "
"dass Sie die Nullhypothese ablehnen können."

#: ../../Ch12/Ch12_Regression_07.rst:135
msgid "Running the hypothesis tests in jamovi"
msgstr "Durchführen der Hypothesentests in jamovi"

#: ../../Ch12/Ch12_Regression_07.rst:137
msgid ""
"To compute all of the statistics that we have talked about so far, all you "
"need to do is make sure the relevant options are checked in jamovi and then "
"run the regression. If we do that, as in :numref:`fig-reg2`, we get a whole "
"bunch of useful output."
msgstr ""
"Um alle Statistiken zu berechnen, über die wir bisher gesprochen haben, "
"müssen Sie lediglich sicherstellen, dass alle relevanten Optionen in jamovi "
"gesetzt sind, und dann die Regression ausführen. Wenn wir das tun, wie in "
":numref:`fig-reg2` gezeigt, erhalten wir eine ganze Reihe nützlicher "
"Ausgaben."

#: ../../Ch12/Ch12_Regression_07.rst:148
msgid "jamovi screenshot showing a multiple linear regression"
msgstr "jamovi-Screenshot mit einer multiplen linearen Regressionsanalyse"

#: ../../Ch12/Ch12_Regression_07.rst:148
msgid ""
"jamovi screenshot showing a multiple linear regression analysis, with some "
"useful options checked."
msgstr ""
"jamovi-Screenshot, der eine multiple lineare Regressionsanalyse zeigt, bei "
"der einige wichtige Ausgabeoptionen gesetzt wurden."

#: ../../Ch12/Ch12_Regression_07.rst:153
#, python-format
msgid ""
"The ``Model Coefficients`` at the bottom of the jamovi analysis results "
"shown in `fig-reg2` provides the coefficients of the regression model. Each "
"row in this table refers to one of the coefficients in the regression model. "
"The first row is the intercept term, and the later ones look at each of the "
"predictors. The columns give you all of the relevant information. The first "
"column is the actual estimate of *b* (e.g., 125.97 for the intercept, and "
"-8.95 for the ``dani.sleep`` predictor). The second column is the standard "
"error estimate :math:`\\hat\\sigma_b`. The third and fourth columns provide "
"the lower and upper values for the 95\\% confidence interval around the *b* "
"estimate (more on this later). The fifth column gives you the *t*-statistic, "
"and it’s worth noticing that in this table :math:`t= \\hat{b}/ SE(\\hat{b})` "
"every time. Finally, the last column gives you the actual *p*-value for each "
"of these tests.\\ [#]_"
msgstr ""
"Die ``Model Coefficients`` am Ende der jamovi-Analyseergebnisse, die in `fig-"
"reg2` dargestellt sind, enthält die Koeffizienten des Regressionsmodells. "
"Jede Zeile in dieser Tabelle bezieht sich auf einen der Koeffizienten des "
"Regressionsmodells. Die erste Zeile ist der Interzept-Term, und die weiteren "
"Zeilen befassen sich mit den einzelnen Prädiktoren. Die Spalten geben Ihnen "
"alle relevanten Informationen. Die erste Spalte ist die aktuelle Schätzung "
"von *b* (z. B. 125,97 für das Interzept und -8,95 für den Prädiktor ``dani."
"sleep``). Die zweite Spalte ist die Schätzung des Standardfehlers :math:"
"\\hat\\sigma_b`. Die dritte und vierte Spalte enthalten die unteren und "
"oberen Werte für das 95\\%-Konfidenzintervall um die Schätzung von *b* (mehr "
"dazu später). Die fünfte Spalte enthält die *t*-Statistik, und es ist "
"erwähnenswert, dass dieser Wert in dieser Tabelle immer :math:`t= \\hat{b}/ "
"SE(\\hat{b})` ist. In der letzten Spalte schließlich finden Sie den *p*-Wert "
"für jeden dieser Tests.\\ [#]_"

#: ../../Ch12/Ch12_Regression_07.rst:169
#, python-format
msgid ""
"The only thing that the coefficients table itself doesn’t list is the "
"degrees of freedom used in the *t*-test, which is always *N* - K - 1 and is "
"listed in the table at the top of the output, labelled ``Model Fit "
"Measures``. We can see from this table that the model performs significantly "
"better than you’d expect by chance (*F*\\(2,97) = 215.24, *p* < 0.001), "
"which isn’t all that surprising: the *R²* = 0.81 value indicate that the "
"regression model accounts for 81\\% of the variability in the outcome "
"measure (and 82\\% for the adjusted *R²*). However, when we look back up at "
"the *t*-tests for each of the individual coefficients, we have pretty strong "
"evidence that the ``baby.sleep`` variable has no significant effect. All the "
"work in this model is being done by the ``dani.sleep`` variable. Taken "
"together, these results suggest that this regression model is actually the "
"wrong model for the data. You’d probably be better off dropping the ``baby."
"sleep`` predictor entirely. In other words, the simple regression model that "
"we started with is the better model."
msgstr ""
"Das Einzige, was in der Koeffiziententabelle nicht aufgeführt ist, sind die "
"Freiheitsgrade, die im *t*-Test verwendet werden. Diese sind immer *N* - K - "
"1 ist und in der Tabelle mit ``Model Fit Measures`` weiter oben in der "
"Ausgabe aufgeführt. Aus dieser Tabelle geht hervor, dass das Modell "
"signifikant besser abschneidet, als man zufällig erwarten würde (*F*\\(2,97) "
"= 215.24, *p* < 0.001), was nicht allzu überraschend ist: der *R²* = 0.81 "
"Wert zeigt an, dass das Regressionsmodell 81 % der Variabilität der "
"Ergebnisgröße erklärt (und 82 % für das bereinigte *R²*). Wenn wir uns "
"jedoch die *t*-Tests für jeden der einzelnen Koeffizienten ansehen, haben "
"wir deutliche Hinweise darauf, dass die Variable ``baby.sleep`` keinen "
"signifikanten Effekt hat. Die gesamte Arbeit in diesem Modell wird von der "
"Variable ``dani.sleep`` erledigt. Zusammengenommen deuten diese Ergebnisse "
"darauf hin, dass dieses Regressionsmodell eigentlich das falsche Modell für "
"die Daten ist. Es wäre wahrscheinlich besser, den Prädiktor ``baby.sleep`` "
"ganz wegzulassen. Mit anderen Worten: Das einfache Regressionsmodell, mit "
"dem wir begonnen haben, ist das bessere Modell."

#: ../../Ch12/Ch12_Regression_07.rst:189
msgid ""
"For advanced readers only. The vector of residuals is :math:`\\epsilon = y - "
"X \\hat{b}`. For K predictors plus the intercept, the estimated residual "
"variance is :math:`\\hat\\sigma^2 = \\epsilon^\\prime\\epsilon / (N-K-1)`. "
"The estimated covariance matrix of the coefficients is :math:"
"`\\hat\\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}`, the main diagonal of "
"which is :math:`SE(\\hat{b})`, our estimated standard errors."
msgstr ""
"Für fortgeschrittene Leser: Der Vektor der Residuen ist :math:`\\epsilon = y "
"- X \\hat{b}`. Für K-Prädiktoren plus dem Interzept ist die geschätzte "
"Varianz der Residuen :math:`\\hat\\sigma^2 = \\epsilon^\\prime\\epsilon / "
"(N-K-1)`. Die geschätzte Kovarianzmatrix der Koeffizienten ist :math:`\\hat"
"\\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}`, dessen Hauptdiagonale "
":math:`SE(\\hat{b})` ist, und unsere geschätzten Standardfehler enthält."

#: ../../Ch12/Ch12_Regression_07.rst:199
msgid ""
"Note that, although jamovi has done multiple tests here, it hasn’t done a "
"Bonferroni correction or anything. These are standard one-sample *t*-tests "
"with a two-sided alternative. If you want to make corrections for multiple "
"tests, you need to do that yourself."
msgstr ""
"Beachten Sie, dass keine Bonferroni- oder eine andre Korrektur verwendet "
"wurde, obwohl jamovi hier mehrere Tests durchgeführt hat. Es handelt sich um "
"Standardversionen von *t*-Tests bei einer Stichprobe mit einer zweiseitigen "
"Alternative. Wenn Sie Korrekturen für das Durchführen mehrere Tests "
"vornehmen möchten, müssen Sie dies selbst tun."

#: ../../Ch12/Ch12_Regression_09.rst:4
msgid "Regarding regression coefficients"
msgstr "Eine Anmerkung zu den Regressionskoeffizienten"

#: ../../Ch12/Ch12_Regression_09.rst:6
msgid ""
"Before moving on to discuss the assumptions underlying linear regression and "
"what you can do to check if they’re being met, there’s two more topics I "
"want to briefly discuss, both of which relate to the regression "
"coefficients. The first thing to talk about is calculating confidence "
"intervals for the coefficients. After that, I’ll discuss the somewhat murky "
"question of how to determine which predictor is most important."
msgstr ""
"Wir werden gleich mit den Annahmen fortfahren, die der linearen Regression "
"zugrunde liegen, und was Sie tun können, um zu überprüfen, ob diese erfüllt "
"sind. Zuerst möchte ich aber kurz zwei weitere Themen besprechen, die sich "
"beide auf die Regressionskoeffizienten beziehen. Das Erste, worüber man "
"sprechen muss, ist die Berechnung von Konfidenzintervallen für die "
"Koeffizienten. Danach werde ich die etwas verworrene Frage diskutieren, wie "
"man bestimmt, welcher Prädiktor am wichtigsten ist."

#: ../../Ch12/Ch12_Regression_09.rst:14
msgid "Confidence intervals for the coefficients"
msgstr "Konfidenzintervalle für die Koeffizienten"

#: ../../Ch12/Ch12_Regression_09.rst:16
msgid ""
"Like any population parameter, the regression coefficients *b* cannot be "
"estimated with complete precision from a sample of data; that’s part of why "
"we need hypothesis tests. Given this, it’s quite useful to be able to report "
"confidence intervals that capture our uncertainty about the true value of "
"*b*. This is especially useful when the research question focuses heavily on "
"an attempt to find out *how* strongly variable *X* is related to variable "
"*Y*, since in those situations the interest is primarily in the regression "
"weight *b*."
msgstr ""
"Wie jeder Parameter der Grundgesamtheit können die Regressionskoeffizienten "
"*b* nicht mit absoluter Genauigkeit aus einer Stichprobe von Daten geschätzt "
"werden. Das ist einer der Gründe, warum wir Hypothesentests brauchen. Vor "
"diesem Hintergrund ist es sehr nützlich, Konfidenzintervalle berichten zu "
"können, die unsere Unsicherheit über den wahren Wert von *b* erfassen. Dies "
"ist besonders nützlich, wenn sich die Forschungsfrage stark auf den Versuch "
"konzentriert, herauszufinden, *wie* stark die Variable *X* mit der Variablen "
"*Y* zusammenhängt, da in diesen Situationen das Interesse hauptsächlich auf "
"dem Regressionsgewicht *b* liegt."

#: ../../Ch12/Ch12_Regression_09.rst:26
msgid ""
"Fortunately, confidence intervals for the regression weights can be "
"constructed in the usual fashion"
msgstr ""
"Erfreulicherweise können Konfidenzintervalle für die "
"Regressionskoeffizienten auf die übliche Weise konstruiert werden."

#: ../../Ch12/Ch12_Regression_09.rst:29
msgid ""
"\\mbox{CI}(b) = \\hat{b} \\pm \\left( t_{crit} \\times SE(\\hat{b})  "
"\\right)\n"
"\n"
msgstr ""
"\\mbox{CI}(b) = \\hat{b} \\pm \\left( t_{crit} \\times SE(\\hat{b})  "
"\\right)\n"
"\n"

#: ../../Ch12/Ch12_Regression_09.rst:31
#, python-format
msgid ""
"where :math:`SE(\\hat{b})` is the standard error of the regression "
"coefficient, and *t*\\ :sub:`crit` is the relevant critical value of the "
"appropriate *t*-distribution. For instance, if it’s a 95\\% confidence "
"interval that we want, then the critical value is the 97.5th quantile of a "
"*t* distribution with *N* - *K* - 1 degrees of freedom. In other words, this "
"is basically the same approach to calculating confidence intervals that "
"we’ve used throughout."
msgstr ""
"wobei :math:`SE(\\hat{b})` der Standardfehler des Regressionskoeffizienten "
"ist, und *t*\\ :sub:`crit` der entsprechende kritische Wert der "
"entsprechenden *t*-Verteilung ist. Wenn wir beispielsweise ein 95\\%-"
"Konfidenzintervall wünschen, dann ist der kritische Wert das 97,5. Quantil "
"einer *t* -Verteilung mit *N* - *K* - 1 Freiheitsgraden. Mit anderen Worten, "
"dies ist im Grunde derselbe Ansatz zur Berechnung von Konfidenzintervallen, "
"den wir bisher verwendet haben."

#: ../../Ch12/Ch12_Regression_09.rst:38
msgid ""
"In jamovi we had already specified the ``95\\% Confidence interval`` as "
"shown in :numref:`fig-reg2`, although we could easily have chosen another "
"value, say a ``99\\% Confidence interval`` if that is what we decided on."
msgstr ""
"In jamovi hatten wir bereits ``95\\% Confidence interval`` angegeben, wie "
"in :numref:`fig-reg2` gezeigt, obwohl auch wir leicht einen anderen Wert "
"hätten wählen können, z.B. ein ``99\\% Confidence interval``, wenn wir uns "
"dafür entschieden hätten."

#: ../../Ch12/Ch12_Regression_09.rst:43
msgid "Calculating standardised regression coefficients"
msgstr "Berechnung standardisierter Regressionskoeffizienten"

#: ../../Ch12/Ch12_Regression_09.rst:45
msgid ""
"One more thing that you might want to do is to calculate “standardised” "
"regression coefficients, often denoted β. The rationale behind standardised "
"coefficients goes like this. In a lot of situations, your variables are on "
"fundamentally different scales. Suppose, for example, my regression model "
"aims to predict people’s IQ scores using their educational attainment "
"(number of years of education) and their income as predictors. Obviously, "
"educational attainment and income are not on the same scales. The number of "
"years of schooling might only vary by 10s of years, whereas income can vary "
"by 10,000s of dollars (or more). The units of measurement have a big "
"influence on the regression coefficients. The *b* coefficients only make "
"sense when interpreted in light of the units, both of the predictor "
"variables and the outcome variable. This makes it very difficult to compare "
"the coefficients of different predictors. Yet there are situations where you "
"really do want to make comparisons between different coefficients. "
"Specifically, you might want some kind of standard measure of which "
"predictors have the strongest relationship to the outcome. This is what "
"**standardised coefficients** aim to do."
msgstr ""
"Eine weitere Sache, die Sie vielleicht tun möchten, ist die Berechnung von "
"„standardisierten“ Regressionskoeffizienten, die oft als β bezeichnet "
"werden. Die Begründung für standardisierte Koeffizienten lautet wie folgt. "
"In vielen Situationen haben Ihre Variablen grundlegend unterschiedliche "
"Skalen. Nehmen wir zum Beispiel an, mein Regressionsmodell zielt darauf ab, "
"die IQ-Werte von Menschen anhand ihres Bildungsgrades (Anzahl der "
"Bildungsjahre) und ihres Einkommens als Prädiktoren vorherzusagen. "
"Offensichtlich sind Bildungsstand und Einkommen nicht auf den gleichen "
"Skalen. Die Anzahl der Schuljahre kann nur um 10 Jahre variieren, während "
"das Einkommen um 10.000 Dollar (oder mehr) variieren kann. Die Maßeinheiten "
"haben einen großen Einfluss auf die Regressionskoeffizienten. Die *b*-"
"Koeffizienten machen nur Sinn, wenn sie im Lichte der Einheiten "
"interpretiert werden, sowohl der Prädiktorvariablen als auch der "
"Ergebnisvariablen. Dies macht es sehr schwierig, die Koeffizienten "
"verschiedener Prädiktoren miteinander zu vergleichen. Es gibt jedoch oft "
"Situationen, in denen Sie verschiedene Koeffizienten miteinander vergleichen "
"möchten. Insbesondere möchten Sie möglicherweise eine Art Standardmaß dafür "
"haben, welche Prädiktoren die stärkste Beziehung zum Ergebnis haben. Darauf "
"zielen **standardisierte Koeffizienten** ab."

#: ../../Ch12/Ch12_Regression_09.rst:64
msgid ""
"The basic idea is quite simple; the standardised coefficients are the "
"coefficients that you would have obtained if you’d converted all the "
"variables to *z*-scores before running the regression.\\ [#]_ The idea here "
"is that, by converting all the predictors to *z*-scores, they all go into "
"the regression on the same scale, thereby removing the problem of having "
"variables on different scales. Regardless of what the original variables "
"were, a β value of 1 means that an increase in the predictor of 1 standard "
"deviation will produce a corresponding 1 standard deviation increase in the "
"outcome variable. Therefore, if variable A has a larger absolute value of β "
"than variable B, it is deemed to have a stronger relationship with the "
"outcome. Or at least that’s the idea. It’s worth being a little cautious "
"here, since this does rely very heavily on the assumption that “a 1 standard "
"deviation change” is fundamentally the same kind of thing for all variables. "
"It’s not always obvious that this is true."
msgstr ""
"Die Grundidee ist ganz einfach. Die standardisierten Koeffizienten sind die "
"Koeffizienten, die Sie erhalten hätten, wenn Sie alle Variablen vor dem "
"Ausführen der Regression in *z*-Werte konvertiert hätten.\\ [#]_ Die Idee "
"dabei ist, dass alle Prädiktoren durch die Konvertierung in *z*-Werte auf "
"derselben Skala in die Regression eingehen. Dadurch wird das Problem "
"beseitigt, Variablen auf verschiedenen Skalen zu haben. Unabhängig von den "
"ursprünglichen Variablen bedeutet ein β-Wert von 1, dass eine Erhöhung des "
"Prädiktors um 1 Standardabweichung zu einer entsprechenden Erhöhung der "
"Ergebnisvariablen um 1 Standardabweichung führt. Wenn also Variable A einen "
"größeren absoluten Wert von β als Variable B hat, kann davon ausgegangen "
"werden, dass sie eine stärkere Beziehung zum Ergebnis hat. Es lohnt sich "
"aber, hier etwas vorsichtig zu sein, da alles sehr stark auf der Annahme "
"beruht, dass „eine Änderung der Standardabweichung von 1“ grundsätzlich für "
"alle Variablen gleich ist. Es ist nicht immer klar, ob eine solche Annahme "
"korrekt ist."

#: ../../Ch12/Ch12_Regression_09.rst:80
msgid ""
"Leaving aside the interpretation issues, let’s look at how it’s calculated. "
"What you could do is standardise all the variables yourself and then run a "
"regression, but there’s a much simpler way to do it. As it turns out, the β "
"coefficient for a predictor *X* and outcome *Y* has a very simple formula, "
"namely"
msgstr ""
"Lassen wir die Interpretationsprobleme beiseite, und schauen wir uns die "
"Berechnung an. Was Sie tun könnten, ist, alle Variablen selbst zu "
"standardisieren und dann eine Regression durchzuführen, aber es gibt einen "
"viel einfacheren Weg, dies zu tun. Wie sich herausstellt, hat der β-"
"Koeffizient für einen Prädiktor *X* und das Ergebnis *Y* eine sehr einfache "
"Formel, nämlich"

#: ../../Ch12/Ch12_Regression_09.rst:86
msgid "β\\ :sub:`X` = *b*\\ :sub:`X` × (σ\\ :sub:`X` / σ\\ :sub:`Y`)"
msgstr "β\\ :sub:`X` = *b*\\ :sub:`X` × (σ\\ :sub:`X` / σ\\ :sub:`Y`)"

#: ../../Ch12/Ch12_Regression_09.rst:88
msgid ""
"where σ\\ :sub:`X` is the standard deviation of the predictor, and σ\\ :sub:"
"`Y` is the standard deviation of the outcome variable *Y*. This makes "
"matters a lot simpler."
msgstr ""
"wobei σ\\ :sub:`X` die Standardabweichung des Prädiktors und σ\\:sub:`Y` die "
"Standardabweichung der Ergebnisvariablen *Y* ist. Das macht die Sache viel "
"einfacher."

#: ../../Ch12/Ch12_Regression_09.rst:92
msgid ""
"To make things even simpler, jamovi has an option that computes the β "
"coefficients for you using the ``Standardized estimate`` checkbox in the "
"``Model Coefficients`` options, see results in :numref:`fig-reg3`."
msgstr ""
"Um die Dinge noch einfacher zu machen, hat jamovi eine Option, welche die β-"
"Koeffizienten für Sie berechnet. Setzen Sie hierfür das Kontrollkästchen ``"
"Standardized estimate`` in den Optionen ``Model Coefficients``, und sie "
"erhalten die Ergebnisausgabe in :numref:`fig-reg3`."

#: ../../Ch12/Ch12_Regression_09.rst:102
#, python-format
msgid "Standardised coefficients with 95\\% confidence intervals"
msgstr "Standardisierte Koeffizienten mit 95\\%-Konfidenzintervallen"

#: ../../Ch12/Ch12_Regression_09.rst:102
#, python-format
msgid ""
"Standardised coefficients, with 95\\% confidence intervals, for multiple "
"linear regression"
msgstr ""
"Standardisierte Koeffizienten mit 95\\%-Konfidenzintervallen für eine "
"multiple lineare Regression"

#: ../../Ch12/Ch12_Regression_09.rst:107
msgid ""
"These results clearly show that the ``dani.sleep`` variable has a much "
"stronger effect than the ``baby.sleep`` variable. However, this is a perfect "
"example of a situation where it would probably make sense to use the "
"original coefficients *b* rather than the standardised coefficients β. After "
"all, my sleep and the baby’s sleep are *already* on the same scale: number "
"of hours slept. Why complicate matters by converting these to *z*-scores?"
msgstr ""
"Diese Ergebnisse zeigen deutlich, dass die Variable ``dani.sleep`` einen "
"viel stärkeren Einfluss bzw. Effekt hat als die Variable ``baby.sleep``. "
"Dies ist jedoch ein perfektes Beispiel für eine Situation, in der es "
"wahrscheinlich sinnvoll wäre, die ursprünglichen Koeffizienten *b* anstelle "
"der standardisierten Koeffizienten β zu verwenden. Immerhin liegen mein "
"Schlaf und der Schlaf des Babys *bereits* auf der gleichen Skala, nämlich „"
"Anzahl der geschlafenen Stunden“. Warum die Sache verkomplizieren, indem man "
"diese in *z*-Scores umwandelt?"

#: ../../Ch12/Ch12_Regression_09.rst:117
msgid ""
"Strictly, you standardise all the *regressors*. That is, every “thing” that "
"has a regression coefficient associated with it in the model. For the "
"regression models that I’ve talked about so far, each predictor variable "
"maps onto exactly one regressor, and vice versa. However, that’s not "
"actually true in general and we’ll see some examples of this in chapter :doc:"
"`Factorial ANOVA <../Ch14/Ch14_ANOVA2>`. But, for now we don’t need to care "
"too much about this distinction."
msgstr ""
"Streng genommen standardisieren Sie alle *Regressoren*. Das heißt, jedes "
"„Ding“, dem im Modell ein Regressionskoeffizient zugeordnet ist. Bei den "
"Regressionsmodellen, über die ich bisher gesprochen habe, bildet jede "
"Prädiktorvariable genau einen Regressor ab und umgekehrt. Allerdings stimmt "
"das im Allgemeinen nicht wirklich und wir werden einige Beispiele dafür im "
"Kapitel :doc:`Faktorielle ANOVA <../Ch14/Ch14_ANOVA2>` sehen. Aber im Moment "
"brauchen wir uns nicht allzu sehr um diese Unterscheidung zu kümmern."

#: ../../Ch12/Ch12_Regression_10.rst:4
msgid "Assumptions of regression"
msgstr "Voraussetzungen für das Durchführen einer Regression"

#: ../../Ch12/Ch12_Regression_10.rst:6
msgid ""
"The linear regression model that I’ve been discussing relies on several "
"assumptions. In section :doc:`Model checking <../Ch12/Ch12_Regression_11>` "
"we’ll talk a lot more about how to check that these assumptions are being "
"met, but first let’s have a look at each of them."
msgstr ""
"Das lineare Regressionsmodell, das ich besprochen habe, beruht auf mehreren "
"Voraussetzungen. Im Abschnitt :doc:`Modellüberprüfung <../Ch12/"
"Ch12_Regression_11>` werden wir mehr darüber sprechen, wie überprüft werden "
"kann, ob diese Voraussetzungen erfüllt sind, aber lassen Sie uns zuerst "
"einen Blick auf die einzelnen Voraussetzungen werfen."

#: ../../Ch12/Ch12_Regression_10.rst:11
msgid ""
"*Normality*. Like many of the models in statistics, basic simple or multiple "
"linear regression relies on an assumption of normality. Specifically, it "
"assumes that the *residuals* are normally distributed. It’s actually okay if "
"the predictors *X* and the outcome *Y* are non-normal, so long as the "
"residuals ε are normal. See section :doc:`Checking the normality of the "
"residuals <../Ch12/Ch12_Regression_11>`."
msgstr ""
"*Normalverteilung*. Wie viele Modelle in der Statistik beruht auch die "
"einfache oder multiple lineare Regression auf der Annahme einer "
"Normalverteilung. Insbesondere wird angenommen, dass die *Residuen* "
"normalverteilt sind. Es ist eigentlich in Ordnung, wenn die Prädiktoren *X* "
"und das Ergebnis *Y* nicht normalverteilt sind, solange die Residuen ε "
"normalverteilt sind. Siehe Abschnitt :doc:`Überprüfen der Normalverteilung "
"der Residuen <../Ch12/Ch12_Regression_11>`."

#: ../../Ch12/Ch12_Regression_10.rst:18
msgid ""
"*Linearity*. A pretty fundamental assumption of the linear regression model "
"is that the relationship between *X* and *Y* actually is linear! Regardless "
"of whether it’s a simple regression or a multiple regression, we assume that "
"the relationships involved are linear."
msgstr ""
"*Linearität*. Eine grundlegende Annahme des linearen Regressionsmodells ist, "
"dass die Beziehung zwischen *X* und *Y* tatsächlich linear ist! Unabhängig "
"davon, ob es sich um eine einfache Regression oder eine multiple Regression "
"handelt, gehen wir davon aus, dass die beteiligten Beziehungen linear sind."

#: ../../Ch12/Ch12_Regression_10.rst:23
msgid ""
"*Homogeneity of variance*. Strictly speaking, the regression model assumes "
"that each residual ε\\ :sub:`i` is generated from a normal distribution with "
"mean 0, and (more importantly for the current purposes) with a standard "
"deviation σ that is the same for every single residual. In practice, it’s "
"impossible to test the assumption that every residual is identically "
"distributed. Instead, what we care about is that the standard deviation of "
"the residual is the same for all values of *Ŷ*, and (if we’re being "
"especially paranoid) all values of every predictor *X* in the model."
msgstr ""
"*Varianzhomogenität*. Streng genommen geht das Regressionsmodell davon aus, "
"dass jede verbleibende ε\\:sub:'i' aus einer Normalverteilung mit Mittelwert "
"0 und (für die aktuellen Zwecke wichtiger) mit einer Standardabweichung σ "
"erzeugt wird, die für jedes einzelne Residuum gleich ist. In der Praxis ist "
"es unmöglich, die Annahme zu testen, dass jedes Residuum gleich verteilt "
"ist. Stattdessen kümmern wir uns darum, dass die Standardabweichungen der "
"Residuen für alle Werte von *Ŷ* und (wenn wir besonders paranoid sind) alle "
"Werte jedes Prädiktors *X* im Modell gleich ist."

#: ../../Ch12/Ch12_Regression_10.rst:32
msgid ""
"*Uncorrelated predictors*. The idea here is that, in a multiple regression "
"model, you don’t want your predictors to be too strongly correlated with "
"each other. This isn’t “technically” an assumption of the regression model, "
"but in practice it’s required. Predictors that are too strongly correlated "
"with each other (referred to as “collinearity”) can cause problems when "
"evaluating the model. See section :doc:`Checking for collinearity <../Ch12/"
"Ch12_Regression_11>`."
msgstr ""
"*Unkorrelierte Prädiktoren*. Die Idee dabei ist, dass Sie in einem multiplen "
"Regressionsmodell nicht möchten, dass Ihre Prädiktoren zu stark miteinander "
"korrelieren. Dies ist „technisch“ keine Annahme des Regressionsmodells, aber "
"in der Praxis ist es erforderlich. Prädiktoren, die stark miteinander "
"korreliert sind (die wird als „Kollinearität“ bezeichnet), können Probleme "
"beim Überprüfen des Modells verursachen (vgl. Abschnitt :doc:`Prüfung auf "
"Kollinearität <.. /Ch12/Ch12_Regression_11>`\\)."

#: ../../Ch12/Ch12_Regression_10.rst:40
msgid ""
"*Residuals are independent of each other*. This is really just a “catch all” "
"assumption, to the effect that “there’s nothing else funny going on in the "
"residuals”. If there is something weird (e.g., the residuals all depend "
"heavily on some other unmeasured variable) going on, it might screw things "
"up."
msgstr ""
"*Die Residuen sind unabhängig voneinander*. Dies ist eine unspezifische (*"
"catch all*) Annahme, die besagt, dass „in den Residuen keine seltsamen Dinge "
"vorgehen“. Wenn etwas Seltsames vor sich geht (z. B. wenn die Residuen alle "
"stark von einer anderen nicht gemessenen Variablen abhängen), könnte es "
"unser Modell ungültig machen."

#: ../../Ch12/Ch12_Regression_10.rst:46
msgid ""
"*No “bad” outliers*. Again, not actually a technical assumption of the model "
"(or rather, it’s sort of implied by all the others), but there is an "
"implicit assumption that your regression model isn’t being too strongly "
"influenced by one or two anomalous data points because this raises questions "
"about the adequacy of the model and the trustworthiness of the data in some "
"cases. See section :doc:`Three kinds of anomalous data <../Ch12/"
"Ch12_Regression_11>`."
msgstr ""
"*Keine „schlechten“ Ausreißer*. Auch hier handelt es sich nicht wirklich um "
"eine technische Annahme des Modells (oder genau genommen wird es von allen "
"Modellen erwartet). Aber es gibt eine implizite Annahme, dass Ihr "
"Regressionsmodell aus diesem Grund nicht zu stark von ein oder zwei anomalen "
"Datenpunkten beeinflusst wird. Wäre dies der Fall, würde es Fragen zur "
"Angemessenheit des Modells und der Vertrauenswürdigkeit der Daten aufwerfen ("
"vgl. Abschnitt :doc:`Drei Arten von anomalen Daten <../Ch12/"
"Ch12_Regression_11>`\\)."

#: ../../Ch12/Ch12_Regression_11.rst:4
msgid "Model checking"
msgstr "Modellprüfung"

#: ../../Ch12/Ch12_Regression_11.rst:6
msgid ""
"The main focus of this section is **regression diagnostics**, a term that "
"refers to the art of checking that the assumptions of your regression model "
"have been met, figuring out how to fix the model if the assumptions are "
"violated, and generally to check that nothing “funny” is going on. I refer "
"to this as the “art” of model checking with good reason. It’s not easy, and "
"while there are a lot of fairly standardised tools that you can use to "
"diagnose and maybe even cure the problems that ail your model (if there are "
"any, that is!), you really do need to exercise a certain amount of judgement "
"when doing this. It’s easy to get lost in all the details of checking this "
"thing or that thing, and it’s quite exhausting to try to remember what all "
"the different things are. This has the very nasty side effect that a lot of "
"people get frustrated when trying to learn *all* the tools, so instead they "
"decide not to do *any* model checking. This is a bit of a worry!"
msgstr ""
"Der Schwerpunkt dieses Abschnitts liegt auf der **Regressionsdiagnostik**, "
"einem Begriff, der sich auf die Kunst bezieht, zu überprüfen, ob die "
"Annahmen Ihres Regressionsmodells erfüllt wurden und herauszufinden, wie das "
"Modell geändert werden kann, wenn die Annahmen verletzt wurden, und "
"allgemein zu überprüfen, ob nichts „Seltsames“ vor sich geht. Ich bezeichne "
"es aus diesem Grund als die „Kunst“ der Modellprüfung. Es ist nicht einfach. "
"Obwohl es viele standardisierte Werkzeuge gibt, mit denen Sie die Probleme "
"Ihres Modells diagnostizieren und vielleicht sogar beheben können (falls es "
"eine solche Lösung gibt), müssen Sie dabei ein gewisses Maß an "
"Urteilsvermögen walten lassen. Es ist leicht, sich in all den Details der "
"Überprüfung dieses oder jenes Dings zu verlieren, und es ist ziemlich "
"anstrengend, sich daran zu erinnern, was all die verschiedenen Teilaspekte "
"sind. Dies hat den sehr unangenehmen Nebeneffekt, dass viele Leute "
"frustriert sind. Und anstatt zu versuchen, *alle* Werkzeuge zu lernen, "
"beschließen sie, *keine* Modellprüfung durchzuführen. Das ist "
"besorgniserregend!"

#: ../../Ch12/Ch12_Regression_11.rst:21
msgid ""
"In this section I describe several different things you can do to check that "
"your regression model is doing what it’s supposed to. It doesn’t cover the "
"full space of things you could do, but it’s still much more detailed than "
"what I see a lot of people doing in practice, and even I don’t usually cover "
"all of this in my intro stats class either. However, I do think it’s "
"important that you get a sense of what tools are at your disposal, so I’ll "
"try to introduce a bunch of them here. Finally, I should note that this "
"section draws quite heavily from `Fox and Weisberg (2011) <../Other/"
"References.html#fox-2011>`__, the book associated with the ``car`` package "
"that is used to conduct regression analysis in R. The ``car`` package is "
"notable for providing some excellent tools for regression diagnostics, and "
"the book itself talks about them in an admirably clear fashion. I don’t want "
"to sound too gushy about it, but I do think that `Fox and Weisberg (2011) "
"<../Other/References.html#fox-2011>`__ is well worth reading, even if some "
"of the advanced diagnostic techniques are only available in R and not jamovi."
msgstr ""
"In diesem Abschnitt beschreibe ich verschiedene Dinge, die Sie tun können, "
"um zu überprüfen, ob Ihr Regressionsmodell das tut, was es tun soll. Es "
"deckt nicht das gesamte Spektrum der Möglichkeiten ab, die Sie untersuchen "
"könnten. Aber es ist immer noch viel detaillierter als das, was ich viele "
"Leute in der Praxis tun sehe, und selbst ich decke normalerweise nicht alle "
"Möglichkeiten in meinen Statistik-Einführungskursen ab. Ich denke jedoch, "
"dass es wichtig ist, dass Sie ein Gefühl dafür bekommen, welche Werkzeuge "
"Ihnen zur Verfügung stehen. Also werde ich versuchen, hier ein paar davon "
"vorzustellen. Schließlich sollte ich anmerken, dass dieser Abschnitt "
"ziemlich stark von `Fox and Weisberg (2011) <.. /Other/References."
"html#fox-2011>`__ geprägt ist, das Buch, das mit dem R-Paket ``car`` "
"verbunden ist, und zur Durchführung von Regressionsanalysen in R verwendet "
"werden kann. Das ``car``-Paket zeichnet sich dadurch aus, dass es einige "
"hervorragende Werkzeuge für die Regressionsdiagnose bietet, und das Buch "
"beschreibt diese Werkzeuge in bewundernswerter Klarheit. Ich möchte nicht zu "
"überschwänglich klingen, aber ich denke, dass `Fox und Weisberg (2011) <.. /"
"Other/References.html#fox-2011>`__ lesenswert ist, auch wenn einige der "
"fortgeschrittenen Diagnosetechniken nur in R und nicht in jamovi verfügbar "
"sind."

#: ../../Ch12/Ch12_Regression_11.rst:39
msgid "Three kinds of residuals"
msgstr "Drei Arten von Residuen"

#: ../../Ch12/Ch12_Regression_11.rst:41
msgid ""
"The majority of regression diagnostics revolve around looking at the "
"residuals, and by now you’ve probably formed a sufficiently pessimistic "
"theory of statistics to be able to guess that, precisely *because* of the "
"fact that we care a lot about the residuals, there are several different "
"kinds of residual that we might consider. In particular, the following three "
"kinds of residuals are referred to in this section: “ordinary residuals”, "
"“standardised residuals”, and “Studentised residuals”. There is a fourth "
"kind that you’ll see referred to in some of the Figures, and that’s the "
"“Pearson residual”. However, for the models that we’re talking about in this "
"chapter, the Pearson residual is identical to the ordinary residual."
msgstr ""
"Ein wichtiger Aspekt der Regressionsdiagnostik ist die Betrachtung der "
"Residuen. Inzwischen haben Sie wahrscheinlich eine ausreichend "
"pessimistische Sicht auf die Statistik gebildet, um erraten zu können, dass "
"es verschiedene Arten von Residuen gibt, die wir betrachten können, und dass "
"dies der Grund dafür ist, *weshalb* wir uns so sehr um die Residuen kümmern. "
"In diesem Abschnitt wird insbesondere auf die folgenden drei Arten von "
"Residuen Bezug genommen: „gewöhnliche Residuen“, „standardisierte Residuen“ "
"und „studentisierte Residuen“. Es gibt eine vierte Art, auf die in einigen "
"der Abbildungen Bezug genommen wird, und das sind „Pearson-Residuen“. Für "
"die Modelle, über die wir in diesem Kapitel sprechen, sind Pearson-Residuen "
"jedoch identisch mit den gewöhnlichen Residuen."

#: ../../Ch12/Ch12_Regression_11.rst:53
msgid ""
"The first and simplest kind of residuals that we care about are **ordinary "
"residuals**. These are the actual raw residuals that I’ve been talking about "
"throughout this chapter so far. The ordinary residual is just the difference "
"between the fitted value *Ŷ*\\ :sub:`i` and the observed value *Y*\\ :sub:"
"`i`. I’ve been using the notation ε\\ :sub:`i` to refer to the i-th ordinary "
"residual, and by gum I’m going to stick to it. With this in mind, we have "
"the very simple equation:"
msgstr ""
"Die erste und einfachste Art von Residuen, die uns wichtig sind, sind **"
"gewöhnliche Residuen**. Dies sind die eigentlichen Roh-Residuen, über die "
"ich in diesem Kapitel bis jetzt gesprochen habe. Das gewöhnliche Residuum "
"ist nur die Differenz zwischen dem vorhergesagten Wert *Ŷ*\\:sub:`i` und dem "
"beobachteten Wert *Y*\\:sub:`i`. Ich habe die Notation ε\\:sub:`i` benutzt, "
"um auf das i-te gewöhnliche Residuum zu verweisen, und ich werde mich and "
"diese Nomenklatur halten. In diesem Sinne haben wir die sehr einfache "
"Gleichung:"

#: ../../Ch12/Ch12_Regression_11.rst:63
msgid ""
"This is of course what we saw earlier, and unless I specifically refer to "
"some other kind of residual, this is the one I’m talking about. So there’s "
"nothing new here. I just wanted to repeat myself. One drawback to using "
"ordinary residuals is that they’re always on a different scale, depending on "
"what the outcome variable is and how good the regression model is. That is, "
"unless you’ve decided to run a regression model without an intercept term, "
"the ordinary residuals will have mean 0 but the variance is different for "
"every regression. In a lot of contexts, especially where you’re only "
"interested in the *pattern* of the residuals and not their actual values, "
"it’s convenient to estimate the **standardised residuals**, which are "
"normalised in such a way as to have standard deviation 1."
msgstr ""
"Das ist natürlich das, was wir früher gesehen haben. Wenn ich mich nicht "
"speziell auf eine andere Art von Residuen beziehe, sind es gewöhnliche "
"Residuen, von denen ich spreche. Hier gibt es also nichts Neues. Ein "
"Nachteil bei der Verwendung gewöhnlicher Residuen besteht darin, dass sie "
"jeweils verschiedene Skalierungen haben, je nachdem, was die "
"Ergebnisvariable ist und wie gut das Regressionsmodell angepasst ist. Das "
"heißt, nur dann wenn Sie sich entschieden haben, ein Regressionsmodell ohne "
"einen Interzept-Term zu erstellen, haben die gewöhnlichen Residuen einen "
"Mittelwert von 0, aber in jedem Fall ist die Varianz ist für jede Regression "
"unterschiedlich. In vielen Zusammenhängen, insbesondere wenn Sie nur am "
"*Verhältnis* der Residuen zueinander und nicht an ihren tatsächlichen Werten "
"interessiert sind, ist es praktisch, die **standardisierten Residuen** zu "
"schätzen, die so normiert werden, dass sie eine Standardabweichung 1 haben."

#: ../../Ch12/Ch12_Regression_11.rst:76
msgid ""
"The way we calculate these is to divide the ordinary residual by an estimate "
"of the (population) standard deviation of these residuals. For technical "
"reasons, mumble mumble, the formula for this is"
msgstr ""
"Die Art und Weise, wie wir diese berechnen, besteht darin, das gewöhnlichen "
"Residuum durch eine Schätzung der Standardabweichung dieser Residuen in der "
"Grundgesamtheit zu dividieren. Aus technischen Gründen… unverständlich… "
"lautet die Formel dafür"

#: ../../Ch12/Ch12_Regression_11.rst:80
msgid ""
"ε\\ :sub:`i`\\' = :math:`\\frac{\\epsilon_i}{\\hat{\\sigma} \\sqrt{1-h_i}}`"
msgstr ""
"ε\\ :sub:`i`\\' = :math:`\\frac{\\epsilon_i}{\\hat{\\sigma} \\sqrt{1-h_i}}`"

#: ../../Ch12/Ch12_Regression_11.rst:82
msgid ""
"where :math:`\\hat\\sigma` in this context is the estimated population "
"standard deviation of the ordinary residuals, and h\\ :sub:`i` is the “hat "
"value” of the *i*-th observation. I haven’t explained hat values to you yet "
"(but have no fear,\\ [#]_ it’s coming shortly), so this won’t make a lot of "
"sense. For now, it’s enough to interpret the standardised residuals as if "
"we’d converted the ordinary residuals to *z*-scores. In fact, that is more "
"or less the truth, it’s just that we’re being a bit fancier."
msgstr ""
"wobei :math:`\\hat{\\sigma}` in diesem Zusammenhang die geschätzte "
"Standardabweichung der gewöhnlichen Residuen und h\\ :sub:'i' der „Dach-Wert“"
" der *i*-ten Beobachtung ist. Ich habe die Dach-Werte noch nicht erklärt ("
"aber keine Angst,\\ [#]_ das kommt in Kürze), also wird das nicht viel Sinn "
"machen. Im Moment reicht es aus, die standardisierten Residuen so zu "
"interpretieren, als hätten wir die gewöhnlichen Residuen in *z*-Werte "
"umgewandelt. Tatsächlich entspricht das mehr oder weniger die Wahrheit, es "
"ist nur so, dass wir das auf eine ein bisschen ausgefallenere Art machen."

#: ../../Ch12/Ch12_Regression_11.rst:90
msgid ""
"The third kind of residuals are **Studentised residuals** (also called "
"“jackknifed residuals”) and they’re even fancier than standardised "
"residuals. Again, the idea is to take the ordinary residual and divide it by "
"some quantity in order to estimate some standardised notion of the residual."
msgstr ""
"Die dritte Art von Residuen sind **Studentisierte Residuen** (auch „*"
"jackknifed residuals*“ genannt). Sie sind noch schicker als standardisierte "
"Residuen. Auch hier besteht die Idee darin, das gewöhnlichen Residuum zu "
"nehmen und durch eine bestimmte Größe zu dividieren, um eine standardisierte "
"Schätzung des Residuums zu erhalten."

#: ../../Ch12/Ch12_Regression_11.rst:95
msgid "The formula for doing the calculations this time is subtly different"
msgstr ""
"Die Formel für die Durchführung der Berechnungen ist diesmal etwas anders"

#: ../../Ch12/Ch12_Regression_11.rst:97
msgid ""
"\\epsilon_{i}^* = \\frac{\\epsilon_i}{\\hat{\\sigma}_{(-i)} \\sqrt{1-h_i}}\n"
"\n"
msgstr ""
"\\epsilon_{i}^* = \\frac{\\epsilon_i}{\\hat{\\sigma}_{(-i)} \\sqrt{1-h_i}}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:99
msgid ""
"Notice that our estimate of the standard deviation here is written :math:"
"`\\hat{\\sigma}_{(-i)}`. What this corresponds to is the estimate of the "
"residual standard deviation that you *would have obtained* if you just "
"deleted the i\\ th observation from the data set. This sounds like the sort "
"of thing that would be a nightmare to calculate, since it seems to be saying "
"that you have to run *N* new regression models (even a modern computer might "
"grumble a bit at that, especially if you’ve got a large data set). "
"Fortunately, some terribly clever person has shown that this standard "
"deviation estimate is actually given by the following equation:"
msgstr ""
"Beachten Sie, dass unsere Schätzung der Standardabweichung hier als :math:`"
"\\hat{\\sigma}_{(-i)}`. Dies entspricht der Schätzung der Standardabweichung "
"der Residuen, die Sie erhalten hätten, wenn Sie die i-te Beobachtung einfach "
"aus dem Datensatz gelöscht hätten. Das klingt nach etwas, was zu berechnen "
"ein Albtraum ist, da es zu sagen scheint, dass Sie *N* neue "
"Regressionsmodelle ausführen müssten (wo selbst ein moderner Computer ein "
"wenig murren würde, besonders wenn Sie einen großen Datensatz haben). "
"Glücklicherweise hat eine schrecklich kluge Person gezeigt, dass diese "
"Schätzung der Standardabweichung tatsächlich durch die folgende Gleichung "
"möglich ist:"

#: ../../Ch12/Ch12_Regression_11.rst:110
msgid ""
"\\hat\\sigma_{(-i)} = \\hat{\\sigma} \\ \\sqrt{\\frac{N-K-1 - {\\epsilon_{i}"
"^\\prime}^2}{N-K-2}}\n"
"\n"
msgstr ""
"\\hat\\sigma_{(-i)} = \\hat{\\sigma} \\ \\sqrt{\\frac{N-K-1 - {\\epsilon_{i}"
"^\\prime}^2}{N-K-2}}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:112
msgid "Isn’t that a pip?"
msgstr "Ist das nicht ein krönender Abschluss?"

#: ../../Ch12/Ch12_Regression_11.rst:114
msgid ""
"Before moving on, I should point out that you don’t often need to obtain "
"these residuals yourself, even though they are at the heart of almost all "
"regression diagnostics. Most of the time the various options that provide "
"the diagnostics, or assumption checks, will take care of these calculations "
"for you. Even so, it’s always nice to know how to actually get hold of these "
"things yourself in case you ever need to do something non-standard."
msgstr ""
"Bevor ich fortfahre, sollte ich darauf hinweisen, dass Sie diese Residuen "
"nicht oft selbst berechnen müssen, obwohl sie im Mittelpunkt fast jeder "
"Regressionsdiagnostik stehen. Meistens übernehmen die verschiedenen "
"Optionen, welche die Diagnose oder das Prüfen von Voraussetzungen "
"durchführen, diese Berechnungen für Sie. Trotzdem ist es gut zu wissen, wie "
"man diese Berechnungen selbst durchführen könnte, falls man jemals eine "
"Diagnostik durchführen möchte, die durch keine der Standardmethoden "
"abgedeckt wird."

#: ../../Ch12/Ch12_Regression_11.rst:123
msgid "Three kinds of anomalous data"
msgstr "Drei Arten von anomalen Daten"

#: ../../Ch12/Ch12_Regression_11.rst:125
msgid ""
"One danger that you can run into with linear regression models is that your "
"analysis might be disproportionately sensitive to a smallish number of "
"“unusual” or “anomalous” observations. I discussed this idea previously in "
"section :doc:`Using box plots to detect outliers <../Ch05/Ch05_Graphics_2>` "
"in the context of discussing the outliers that get automatically identified "
"by the ``Box plot`` option under ``Exploration`` → ``Descriptives``, but "
"this time we need to be much more precise. In the context of linear "
"regression, there are three conceptually distinct ways in which an "
"observation might be called “anomalous”. All three are interesting, but they "
"have rather different implications for your analysis."
msgstr ""
"Eine Gefahr, auf die Sie bei linearen Regressionsmodellen stoßen können, "
"ist, dass Ihre Analyse möglicherweise unverhältnismäßig empfindlich auf eine "
"kleine Anzahl von „ungewöhnlichen“ oder „anomalen“ Beobachtungen reagiert. "
"Ich habe diese Idee bereits in Abschnitt :doc:`Verwenden von Boxplots zum "
"Identifizieren von Ausreißern <.. /Ch05/Ch05_Graphics_2>` vorgestellt, wo "
"Ausreißer automatisch durch die Option ``Box plot`` unter ``Exploration`` → "
"``Descriptives`` identifiziert wurden. Diesmal müssen wir aber viel präziser "
"sein. Im Zusammenhang mit der linearen Regression gibt es drei konzeptuell "
"verschiedene Arten, um eine Beobachtung als „anomal“ zu klassifizieren. Alle "
"drei sind interessant, aber sie haben ziemlich unterschiedliche Auswirkungen "
"auf Ihre Analyse."

#: ../../Ch12/Ch12_Regression_11.rst:136
msgid ""
"The first kind of unusual observation is an **outlier**. The definition of "
"an outlier (in this context) is an observation that is very different from "
"what the regression model predicts. An example is shown in :numref:`fig-"
"outlier`. In practice, we operationalise this concept by saying that an "
"outlier is an observation that has a very large Studentised residual, ε\\ :"
"sub:`i`\\ :sup:`*`. Outliers are interesting: a big outlier *might* "
"correspond to junk data, e.g., the variables might have been recorded "
"incorrectly in the data set, or some other defect may be detectable. Note "
"that you shouldn’t throw an observation away just because it’s an outlier. "
"But the fact that it’s an outlier is often a cue to look more closely at "
"that case and try to find out why it’s so different."
msgstr ""
"Die erste Art von ungewöhnlichen Beobachtungen sind **Ausreißer**. Ein "
"Ausreißer ist (in diesem Zusammenhang) eine Beobachtung, die sich stark von "
"dem unterscheidet, was das Regressionsmodell vorhersagt. Ein Beispiel ist in "
":numref:`fig-outlier` dargestellt. In der Praxis operationalisieren wir "
"dieses Konzept, indem wir sagen, dass ein Ausreißer eine Beobachtung ist, "
"die einen sehr großes studentisiertes Residuum hat, ε\\ :sub:`i`\\ :sup:`*`. "
"Ausreißer sind interessant: Ein Ausreißer *könnte* fehlerhaften Daten "
"entsprechen, z.B. weil Variablen falsch im Datensatz aufgezeichnet wurden, "
"oder auf Grund von Fehlern bei der Dateneingabe. Beachten Sie, dass Sie eine "
"Beobachtung nicht wegwerfen sollten, nur weil es sich um einen Ausreißer "
"handelt. Aber die Tatsache, dass es sich um einen Ausreißer handelt, ist oft "
"ein Hinweis, sich diesen Fall genauer anzusehen und herauszufinden, warum er "
"sich so stark von den übrigen Fällen unterscheidet."

#: ../../Ch12/Ch12_Regression_11.rst:155
msgid "Outliers and their effect"
msgstr "Ausreißer und ihre Wirkung"

#: ../../Ch12/Ch12_Regression_11.rst:155
msgid ""
"Illustration of outliers: The dotted lines plot the regression line that "
"would have been estimated without the anomalous observation included, and "
"the corresponding residual (i.e., the Studentised residual). The solid line "
"shows the regression line with the anomalous observation included. The "
"outlier has an unusual value on the outcome (y axis location) but not the "
"predictor (x axis location), and lies a long way from the regression line."
msgstr ""
"Darstellung von Ausreißern: Die gepunkteten Linien stellen die "
"Regressionsgeraden dar, welche ohne die anomale Beobachtung geschätzt worden "
"wäre, und das entsprechende Residuum (d.h. das studentisierte Residuum). Die "
"durchgezogene Linie zeigt die Regressionsgerade mit der eingeschlossenen "
"anomalen Beobachtung. Der Ausreißer hat einen ungewöhnlichen Wert für das "
"Ergebnis (Position auf der Y-Achse), aber nicht für den Prädiktor (Position "
"auf der X-Achse) und liegt weit von der Regressionsgerade entfernt."

#: ../../Ch12/Ch12_Regression_11.rst:171
msgid "High leverage points and their effect"
msgstr "Hebelpunkte und ihre Wirkung"

#: ../../Ch12/Ch12_Regression_11.rst:171
msgid ""
"Illustration of high leverage points: The anomalous observation in this case "
"is unusual both in terms of the predictor (x axis) and the outcome (y axis), "
"but this unusualness is highly consistent with the pattern of correlations "
"that exists among the other observations. The observation falls very close "
"to the regression line and does not distort it."
msgstr ""
"Illustration von Hebelpunkten: Die anomale Beobachtung ist in diesem Fall "
"sowohl in Bezug auf den Prädiktor (x-Achse) als auch auf das Ergebnis "
"(y-Achse) ungewöhnlich, aber diese Ungewöhnlichkeit stimmt stark mit dem "
"Korrelationsmuster überein, das zwischen den anderen Beobachtungen besteht. "
"Die Beobachtung liegt sehr nahe an der Regressionsgerade und verzerrt sie "
"nicht."

#: ../../Ch12/Ch12_Regression_11.rst:179
msgid ""
"The second way in which an observation can be unusual is if it has high "
"**leverage**, which happens when the observation is very different from all "
"the other observations. This doesn’t necessarily have to correspond to a "
"large residual. If the observation happens to be unusual on all variables in "
"precisely the same way, it can actually lie very close to the regression "
"line. An example of this is shown in :numref:`fig-leverage`. The leverage of "
"an observation is operationalised in terms of its *hat value*, usually "
"written h\\ :sub:`i`. The formula for the hat value is rather complicated,\\ "
"[#]_ but it interpretation is not: h\\ :sub:`i` is a measure of the extent "
"to which the *i*-th observation is “in control” of where the regression line "
"ends up going."
msgstr ""
"Die zweite Möglichkeit, wie eine Beobachtung ungewöhnlich sein kann, ist, "
"wenn sie eine hohe ** Hebelwirkung ** hat. Das passiert, wenn sich die "
"Beobachtung stark von allen anderen Beobachtungen unterscheidet. Dies muss "
"nicht unbedingt einem großen Residuum entsprechen. Wenn die Beobachtung bei "
"allen Variablen auf genau die gleiche Weise ungewöhnlich ist, kann sie "
"tatsächlich sehr nahe an der Regressionsgeraden liegen. Ein Beispiel dafür "
"ist in :numref:`fig-leverage` dargestellt. Die Hebelwirkung einer "
"Beobachtung wird in Bezug auf ihren *Dach-Wert* operationalisiert, der "
"normalerweise h\\ :sub:`i` geschrieben wird. Die Formel für den Dach-Wert "
"ist ziemlich kompliziert,\\ [#]_ aber die Interpretation ist es nicht: h\\ "
":sub:`i` ist ein Maß dafür, inwieweit die *i*-te Beobachtung „kontrolliert“ "
"hat, wie die Regressionsgerade verläuft."

#: ../../Ch12/Ch12_Regression_11.rst:190
msgid ""
"In general, if an observation lies far away from the other ones in terms of "
"the predictor variables, it will have a large hat value (as a rough guide, "
"high leverage is when the hat value is more than 2 - 3 times the average; "
"and note that the sum of the hat values is constrained to be equal to K + "
"1). High leverage points are also worth looking at in more detail, but "
"they’re much less likely to be a cause for concern unless they are also "
"outliers."
msgstr ""
"Wenn eine Beobachtung in Bezug auf die Prädiktorvariablen weit von den "
"anderen entfernt liegt, hat sie im Allgemeinen einen großen Dach-Wert (als "
"grobe Richtlinie gilt, dass eine hohe Hebelwirkung vorliegt, wenn der Dach-"
"Wert mehr als das 2- bis 3-fache des Durchschnitts beträgt; aber beachten "
"Sie, dass die Summe der Dach-Werte auf K + 1 beschränkt ist). Punkte mit "
"großer Hebelwirkung sind ebenfalls einen genaueren Blick wert, aber sie "
"geben viel weniger Anlass zur Sorge, es sei denn, sie sind zusätzlich "
"Ausreißer."

#: ../../Ch12/Ch12_Regression_11.rst:203
msgid "High influence points and their effect"
msgstr "Punkte mit hohem Einfluss und deren Wirkung"

#: ../../Ch12/Ch12_Regression_11.rst:203
msgid ""
"Illustration of high influence points: In this case, the anomalous "
"observation is highly unusual on the predictor variable (x axis), and falls "
"a long way from the regression line. As a consequence, the regression line "
"is highly distorted, even though (in this case) the anomalous observation is "
"entirely typical in terms of the outcome variable (y axis)."
msgstr ""
"Veranschaulichung von Punkten mit hohem Einfluss: In diesem Fall ist die "
"anomale Beobachtung in Bezug auf die Prädiktorvariable (x-Achse) höchst "
"ungewöhnlich und liegt weit von der Regressionsgeraden entfernt. Als Folge "
"ist die Regressionsgerade stark verzerrt, obwohl (in diesem Fall) die "
"anomale Beobachtung in Bezug auf die Ergebnisvariable (y-Achse) durchaus "
"typisch ist."

#: ../../Ch12/Ch12_Regression_11.rst:211
msgid ""
"This brings us to our third measure of unusualness, the **influence** of an "
"observation. A high influence observation is an outlier that has high "
"leverage. That is, it is an observation that is very different to all the "
"other ones in some respect, and also lies a long way from the regression "
"line. This is illustrated in :numref:`fig-influence`. Notice the contrast to "
"the previous two figures. Outliers don’t move the regression line much and "
"neither do high leverage points. But something that is both an outlier and "
"has high leverage, well that has a big effect on the regression line. That’s "
"why we call these points high influence, and it’s why they’re the biggest "
"worry. We operationalise influence in terms of a measure known as **Cook’s "
"distance**."
msgstr ""
"Dies bringt uns zu unserem dritten Maß der Ungewöhnlichkeit, dem **Einfluss**"
" einer Beobachtung. Eine Beobachtung mit hohem Einfluss ist ein Ausreißer "
"mit hoher Hebelwirkung. Das heißt, es ist eine Beobachtung, die sich in "
"gewisser Hinsicht von allen anderen stark unterscheidet und auch weit von "
"der Regressionsgeraden entfernt liegt. Dies wird in :numref:`fig-influence` "
"veranschaulicht. Beachten Sie den Kontrast zu den beiden vorherigen "
"Abbildungen. Ausreißer verändern die Regressionsgerade nicht stark und "
"Punkte mit hoher Hebelwirkung ebenfalls nicht. Aber etwas, das sowohl ein "
"Ausreißer ist, als auch eine hohe Hebelwirkung hat, hat einen großen "
"Einfluss auf die Regressionsgerade. Deshalb nennen wir diese Punkte solche "
"mit hohem Einfluss, und deshalb sind sie unsere größte Sorge. Wir "
"operationalisieren den Einfluss durch ein Maß, welches als **Cook-Distanz** "
"bekannt ist."

#: ../../Ch12/Ch12_Regression_11.rst:222
msgid ""
"D_i = \\frac{{\\epsilon_i^*}^2 }{K+1} \\times \\frac{h_i}{1-h_i}\n"
"\n"
msgstr ""
"D_i = \\frac{{\\epsilon_i^*}^2 }{K+1} \\times \\frac{h_i}{1-h_i}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:224
msgid ""
"Notice that this is a multiplication of something that measures the outlier-"
"ness of the observation (the bit on the left), and something that measures "
"the leverage of the observation (the bit on the right)."
msgstr ""
"Beachten Sie, dass dies eine Multiplikation von etwas ist, das beschreibt, "
"in welchem Maß eine Beobachtung ein Ausreißer ist (links), und etwas, das "
"die Hebelwirkung dieser Beobachtung misst (rechts)."

#: ../../Ch12/Ch12_Regression_11.rst:228
msgid ""
"In order to have a large Cook’s distance an observation must be a fairly "
"substantial outlier *and* have high leverage. As a rough guide, Cook’s "
"distance greater than 1 is often considered large (that’s what I typically "
"use as a quick and dirty rule)."
msgstr ""
"Um eine große Cook-Distanz zu haben, muss eine Beobachtung ein ziemlich "
"wesentlicher Ausreißer sein *und* eine hohe Hebelwirkung haben. Als grobe "
"Richtlinie wird eine Cooks Distanz von mehr als 1 oft als groß angesehen ("
"das ist, was ich normalerweise als Faustregel verwende)."

#: ../../Ch12/Ch12_Regression_11.rst:233
msgid ""
"In jamovi, information about Cook’s distance can be calculated by clicking "
"on the ``Cook’s Distance`` checkbox in the ``Assumption Checks`` → ``Data "
"Summary`` options. When you do this, for the multiple regression model we "
"have been using as an example in this chapter, you get the results as shown "
"in :numref:`fig-reg4`\\."
msgstr ""
"In jamovi können Informationen zur Cook-Distanz berechnet werden, indem Sie "
"auf das Kontrollkästchen ``Cook’s Distance`` in den Optionen ``Assumption "
"Checks`` → ``Data Summary`` setzen. Wenn Sie dies tun, erhalten Sie für das "
"multiple Regressionsmodell, das wir in diesem Kapitel als Beispiel verwendet "
"haben, die Ergebnisse, die in :numref:`fig-reg4`\\ zu sehen sind."

#: ../../Ch12/Ch12_Regression_11.rst:245
msgid "jamovi output showing the table for the Cook’s distance statistics"
msgstr "jamovi-Ausgabe mit einer Tabelle mit der Cook-Distanz-Statistik"

#: ../../Ch12/Ch12_Regression_11.rst:249
msgid ""
"You can see that, in this example, the mean Cook’s distance value is 0.01, "
"and the range is from 0.00000262 to 0.11, so this is some way off the rule "
"of thumb figure mentioned above that a Cook’s distance greater than 1 is "
"considered large."
msgstr ""
"Sie können sehen, dass in diesem Beispiel der mittlere Wert für die Cook-"
"Distanz 0,01 beträgt und der Wertebereich zwischen 0,00000262 und 0,11 "
"liegt. Dies ist also ein Stück unterhalb der oben erwähnten Faustregel, nach "
"der eine Cook-Distanz von über 1 als groß gilt."

#: ../../Ch12/Ch12_Regression_11.rst:254
msgid ""
"An obvious question to ask next is, if you do have large values of Cook’s "
"distance what should you do? As always, there’s no hard and fast rule. "
"Probably the first thing to do is to try running the regression with the "
"outlier with the greatest Cook’s distance\\ [#]_ excluded and see what "
"happens to the model performance and to the regression coefficients. If they "
"really are substantially different, it’s time to start digging into your "
"data set and your notes that you no doubt were scribbling as your ran your "
"study. Try to figure out *why* the point is so different. If you start to "
"become convinced that this one data point is badly distorting your results "
"then you might consider excluding it, but that’s less than ideal unless you "
"have a solid explanation for why this particular case is qualitatively "
"different from the others and therefore deserves to be handled separately."
msgstr ""
"Die naheliegende Frage, die Sie als Nächstes stellen sollten, lautet: Was "
"sollten Sie tun, wenn Sie große Werte für die Cook-Distanz erhalten? Wie "
"immer gibt es keine feste Regel. Wahrscheinlich ist das Erste, was Sie tun "
"sollten, zu versuchen, die Regression ohne den Ausreißer mit der größten "
"Cook-Distanz durchzuführen\\ [#]_ und zu sehen, wie sich das auf die "
"Modellleistung und die Regressionskoeffizienten auswirkt. Wenn sie sich "
"wesentlich unterscheiden, sollten Sie Ihren Datensatz und Ihre Notizen "
"während der Durchführung Ihrer Studie genauer ansehen. Versuchen Sie "
"herauszufinden, *warum* dieser Datenpunkt sich so sehr von den übrigen "
"unterscheidet. Wenn Sie davon überzeugt sind, dass nur dieser eine "
"Datenpunkt Ihre Ergebnisse stark verzerrt, sollten Sie erwägen, ihn "
"auszuschließen. Das ist aber alles andere als ideal, solange Sie nicht eine "
"solide Erklärung dafür haben, warum sich dieser spezielle Fall qualitativ "
"von den anderen unterscheidet und es daher verdient hat, separat behandelt "
"zu werden."

#: ../../Ch12/Ch12_Regression_11.rst:268
msgid "Checking the normality of the residuals"
msgstr "Überprüfen der Normalverteilung der Residuen"

#: ../../Ch12/Ch12_Regression_11.rst:270
msgid ""
"Like many of the statistical tools we’ve discussed in this book, regression "
"models rely on a normality assumption. In this case, we assume that the "
"residuals are normally distributed. The first thing we can do is draw a QQ-"
"plot via the ``Assumption Checks`` → ``Q-Q plot of residuals`` option."
msgstr ""
"Wie viele der statistischen Werkzeuge, die wir in diesem Buch besprochen "
"haben, beruhen auch Regressionsmodelle auf der Annahme einer "
"Normalverteilung. In diesem Fall gehen wir davon aus, dass die Residuen "
"normalverteilt sind. Als Erstes können wir mit der Option ``Assumption "
"Checks`` → ``Q-Q plot of residuals`` ein QQ-Diagramm zeichnen."

#: ../../Ch12/Ch12_Regression_11.rst:275
msgid ""
"The output is shown in :numref:`fig-reg5`, showing the standardised "
"residuals plotted as a function of their theoretical quantiles according to "
"the regression model."
msgstr ""
"Die Ausgabe findet sich in :numref:`fig-reg5` und zeigt die standardisierten "
"Residuen als Funktion ihrer theoretischen Quantile gemäß dem "
"Regressionsmodell."

#: ../../Ch12/Ch12_Regression_11.rst:285
msgid "Quantiles according to the model against standardised residuals"
msgstr "Quantile entsprechend dem Modell vs. die standardisierten Residuen"

#: ../../Ch12/Ch12_Regression_11.rst:285
msgid ""
"Plot of the theoretical quantiles according to the model, against the "
"quantiles of the standardised residuals, produced in jamovi"
msgstr ""
"Diagramm der theoretischen Quantile gemäß dem Modell vs. der Quantile der "
"standardisierten Residuen, erstellt mit jamovi"

#: ../../Ch12/Ch12_Regression_11.rst:290
msgid ""
"Another thing we should check is the relationship between the fitted values "
"and the residuals themselves. We can get jamovi to do this using the "
"``Residuals Plots`` option, which provides a scatterplot for each predictor "
"variable, the outcome variable, and the fitted values against residuals, "
"see :numref:`fig-reg6`. In these plots we are looking for a fairly uniform "
"distribution of “dots”, with no clear bunching or patterning of the “dots”. "
"Looking at these plots, there is nothing particularly worrying as the dots "
"are fairly evenly spread across the whole plot. There may be a little bit of "
"non-uniformity in the right panel, but it is not a strong deviation and "
"probably not worth worrying about."
msgstr ""
"Eine andere Frage, die wir überprüfen sollten, ist die Beziehung zwischen "
"den durch das Modell vorhergesagten Werten und den Residuen selbst. Wir "
"können jamovi mit dem Setzen der Option ``Residuals Plots`` auffordern, ein "
"Streudiagramm für jede Prädiktorvariable, die Ergebnisvariable und die durch "
"das Modell vorhergesagten Werten im Vergleich zu den Residuen zu erstellen, "
"wie in :numref:`fig-reg6` zu sehen. In diesen Diagrammen suchen wir nach "
"einer ziemlich gleichmäßigen Verteilung der „Punkte“, ohne dass eine "
"Konzentration oder ein klares Muster der „Punkte“ zu erkennen wäre. Wenn man "
"sich die Diagramme in :numref:`fig-reg6` ansieht, gibt es nichts besonders "
"Besorgniserregendes, da die Punkte ziemlich gleichmäßig über das gesamte "
"Diagramm verteilt sind. Es mag eine gewisse Konzentration in den beiden "
"Diagrammen auf der rechten Seite geben, aber dies ist nicht besonders "
"ausgeprägt und wahrscheinlich nicht wert, sich Sorgen zu machen."

#: ../../Ch12/Ch12_Regression_11.rst:307
msgid "Residuals plots produced in jamovi"
msgstr "Streudiagramme der Residuen, in Jamovi erstellt"

#: ../../Ch12/Ch12_Regression_11.rst:311
msgid ""
"If we were worried, then in a lot of cases the solution to this problem (and "
"many others) is to transform one or more of the variables. We discussed the "
"basics of variable transformation in the sections :doc:`Transforming or "
"recoding a variable <../Ch06/Ch06_DataHandling_3>` and :doc:`Mathematical "
"functions and operations <../Ch06/Ch06_DataHandling_4>`, but I do want to "
"make special note of one additional possibility that I didn’t explain fully "
"earlier: the Box-Cox transform."
msgstr ""
"Wenn die Diagramme Grund zur Bekümmerung geben, dann besteht eine mögliche "
"Lösung für dieses Problem (und viele andere) darin, eine oder mehrere der "
"Variablen zu transformieren. Wir haben die Grundlagen der "
"Variablentransformation in den Abschnitten :doc:`Variablen transformieren "
"oder umkodieren <../Ch06/Ch06_DataHandling_3>` und :doc:`Mathematische "
"Funktionen und Operationen <../Ch06/Ch06_DataHandling_4>` besprochen. Ich "
"möchte aber hier besonders auf eine zusätzliche Möglichkeit hinweisen, die "
"ich zuvor nicht vollständig erklärt habe: die Box-Cox-Transformation."

#: ../../Ch12/Ch12_Regression_11.rst:321
msgid "The Box-Cox function is a fairly simple one and it’s very widely used."
msgstr ""
"Die Box-Cox-Funktion ist ziemlich einfach und wird sehr häufig verwendet."

#: ../../Ch12/Ch12_Regression_11.rst:323
msgid ""
"f(x,\\lambda) = \\frac{x^\\lambda - 1}{\\lambda}\n"
"\n"
msgstr ""
"f(x,\\lambda) = \\frac{x^\\lambda - 1}{\\lambda}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:325
msgid ""
"for all values of λ except λ = 0. When λ = 0 we just take the natural "
"logarithm (i.e., *ln*\\(x))."
msgstr ""
"für alle Werte von λ außer λ = 0. Wenn λ = 0 ist, nehmen wir einfach den "
"natürlichen Logarithmus (d. h. *ln*\\(x))."

#: ../../Ch12/Ch12_Regression_11.rst:328
msgid ""
"You can calculate it using the ``BOXCOX`` function in the ``Compute`` "
"variables screen in jamovi."
msgstr ""
"Sie können sie mit der ``BOXCOX``-Funktion im ``Compute``-"
"Variablenbildschirm in jamovi berechnen."

#: ../../Ch12/Ch12_Regression_11.rst:332
msgid "Checking for collinearity"
msgstr "Prüfen auf Kollinearität"

#: ../../Ch12/Ch12_Regression_11.rst:334
msgid ""
"The last kind of regression diagnostic that I’m going to discuss in this "
"chapter is the use of **variance inflation factors** (VIFs), which are "
"useful for determining whether or not the predictors in your regression "
"model are too highly correlated with each other. There is a variance "
"inflation factor associated with each predictor *X*\\ :sub:`k` in the model."
msgstr ""
"Die letzte Art von Regressionsdiagnose, die ich in diesem Kapitel erörtern "
"werde, ist die Verwendung von **Varianzinflationsfaktoren** (VIFs). Diese "
"sind nützlich, um festzustellen, ob die Prädiktoren in Ihrem "
"Regressionsmodell zu stark miteinander korreliert sind. Jedem Prädiktor *X*\\"
" :sub:`k` im Modell ist ein Varianz-Inflationsfaktor zugeordnet."

#: ../../Ch12/Ch12_Regression_11.rst:341
msgid "The formula for the k-th VIF is:"
msgstr "Die Formel für den k-ten VIF lautet:"

#: ../../Ch12/Ch12_Regression_11.rst:343
msgid "VIF\\ :sub:`k` = 1 / (1 - *R²*\\ :sub:`(-k)`\\)"
msgstr "VIF\\ :sub:`k` = 1 / (1 - *R²*\\ :sub:`(-k)`\\)"

#: ../../Ch12/Ch12_Regression_11.rst:345
msgid ""
"where *R²*\\ :sub:`(-k)` refers to *R*-squared value you would get if you "
"ran a regression using *X*\\ :sub:`k` as the outcome variable, and all the "
"other *X* variables as the predictors. The idea here is that *R²*\\ :sub:`(-"
"k)` is a very good measure of the extent to which *X*\\ :sub:`k` is "
"correlated with all the other variables in the model."
msgstr ""
"wobei sich *R²*\\ :sub:`(-k)` auf den *R*-Quadratwert bezieht, den Sie "
"erhalten würden, wenn Sie eine Regression mit *X*\\ :sub:`k` als "
"Ergebnisvariable und allen anderen Variablen in *X* als Prädiktoren "
"durchführen würden. Die Idee dabei ist, dass *R²*\\ :sub:`(-k)` ein sehr "
"gutes Maß dafür ist, inwieweit *X*\\ :sub:`k` mit allen anderen Variablen im "
"Modell korreliert ist (d.h. durch die übrigen Variablen erklärt wird)."

#: ../../Ch12/Ch12_Regression_11.rst:351
msgid ""
"The square root of the VIF is pretty interpretable. It tells you how much "
"wider the confidence interval for the corresponding coefficient *b*\\ :sub:"
"`k` is, relative to what you would have expected if the predictors are all "
"nice and uncorrelated with one another. If you’ve only got two predictors, "
"the VIF values are always going to be the same, as we can see if we click on "
"the ``Collinearity`` checkbox in the ``Regression`` → ``Assumption Checks`` "
"options in jamovi. For both ``dani.sleep`` and ``baby.sleep`` the VIF is "
"1.65. And since the square root of 1.65 is 1.28, we see that the correlation "
"between our two predictors isn’t causing much of a problem."
msgstr ""
"Die Quadratwurzel des VIF ist ziemlich einfach interpretierbar. Es sagt "
"Ihnen, wie viel breiter das Konfidenzintervall für den entsprechenden "
"Koeffizienten *b*\\ :sub:`k` ist, relativ zu dem, was Sie erwartet hätten, "
"wenn die Prädiktoren alle unkorreliert miteinander gewesen wären. Wenn Sie "
"nur zwei Prädiktoren haben, werden die VIF-Werte immer gleich sein, wie wir "
"sehen können, wenn wir in jamovi das Kontrollkästchen ``Collinearity`` in "
"den Optionen unter ``Regression`` → ``Assumption Checks`` setzen. Sowohl für "
"``dani.sleep`` als auch für ``baby.sleep`` beträgt der VIF 1,65. Und da 1,28 "
"die Quadratwurzel von 1,65 ist, sehen wir, dass die Korrelation zwischen "
"unseren beiden Prädiktoren kein großes Problem verursacht."

#: ../../Ch12/Ch12_Regression_11.rst:362
msgid ""
"To give a sense of how we could end up with a model that has bigger "
"collinearity problems, suppose I were to run a much less interesting "
"regression model, in which I tried to predict the ``day`` on which the data "
"were collected, as a function of all the other variables in the data set. To "
"see why this would be a bit of a problem, let’s have a look at the "
"correlation matrix for all four variables:"
msgstr ""
"Um ein Gefühl dafür zu vermitteln, wie wir zu einem Modell mit größeren "
"Kollinearitätsproblemen kommen könnten, nehmen wir an, ich würde ein viel "
"weniger interessantes Regressionsmodell ausführen. Dabei würde ich "
"versuchen, den ``day`` vorherzusagen, an dem die Daten gesammelt wurden, und "
"alle übrigen Variablen im Datensatz als Prädiktoren verwenden. Um zu sehen, "
"warum dies problematisch wäre, werfen wir einen Blick auf die "
"Korrelationsmatrix für die vier Variablen:"

#: ../../Ch12/Ch12_Regression_11.rst:377
msgid ""
"We have some fairly large correlations between some of our predictor "
"variables! When we run the regression model and look at the VIF values, we "
"see that the collinearity is causing a lot of uncertainty about the "
"coefficients. First, run the regression, as in :numref:`fig-reg7` and you "
"can see from the VIF values that, yep, that’s some mighty fine collinearity "
"there."
msgstr ""
"Wir haben einige ziemlich hohe Korrelationen zwischen einigen unserer "
"Prädiktorvariablen. Wenn wir das Regressionsmodell ausführen und uns die VIF-"
"Werte ansehen, sehen wir, dass die Kollinearität eine Menge Unsicherheit in "
"Bezug auf die Koeffizienten verursacht. Führen Sie zuerst die Regression "
"aus, wie in :numref:`fig-reg7` gezeigt, und Sie können anhand der VIF-Werte "
"sehen, dass eine deutliche Kollinearität vorliegt."

#: ../../Ch12/Ch12_Regression_11.rst:389
msgid "Collinearity statistics for multiple regression, produced in jamovi"
msgstr "Kollinearitätsstatistik für die multiple Regression, erstellt in jamovi"

#: ../../Ch12/Ch12_Regression_11.rst:396
msgid "Or have no hope, as the case may be."
msgstr "Oder keine Hoffnung haben, je nachdem."

#: ../../Ch12/Ch12_Regression_11.rst:399
msgid ""
"Again, for the linear algebra fanatics: the “hat matrix” is defined to be "
"that matrix **H** that converts the vector of observed values *y* into a "
"vector of fitted values ŷ, such that ŷ = **H**\\ *y*. The name comes from "
"the fact that this is the matrix that “puts a hat on *y*”. The hat *value* "
"of the i-th observation is the i-th diagonal element of this matrix (so "
"technically I should be writing it as h\\ :sub:`ii` rather than h\\ :sub:"
"`i`). Oh, and in case you care, here’s how it’s calculated: **H** = "
"**X**\\(**X**'**X**\\)\\ :sup:`-1` **X**'\\. Pretty, isn’t it?"
msgstr ""
"Wiederum für die Liebhaber der linearen Algebra: Die „Dach-Matrix“ ist "
"definiert als die Matrix **H**, die den Vektor der beobachteten Werte *y* in "
"einen Vektor der vom Modell geschätzten Werten ŷ umwandelt, so dass ŷ = **H**"
"\\ *y*. Der Name kommt von der Tatsache, dass dies die Matrix ist, die „ein "
"Dach auf *y* setzt“. Der *Wert* der i-ten Beobachtung ist das i-te diagonale "
"Element in dieser Matrix (also sollte ich es technisch als h\\ :sub:`ii` und "
"nicht als h\\ :sub:`i` schreiben). Oh, und falls es Sie interessiert, hier "
"ist, wie es berechnet wird:**H** = **X**\\(**X**'**X**\\)\\ :sup:`-1` **X**'"
"\\. Hübsch, nicht wahr?"

#: ../../Ch12/Ch12_Regression_11.rst:409
msgid ""
"Although, currently there isn’t a very easy way to do this in jamovi, so a "
"more powerful regression program such as the ``car`` package in ``R`` would "
"be better for this more advanced analysis"
msgstr ""
"Derzeit gibt es in Jamovi keine einfache Möglichkeit, dies zu tun, daher "
"wäre das ``car``-Paket in ``R`` besser für diese fortgeschrittenere Analyse"

#: ../../Ch12/Ch12_Regression_12.rst:4
msgid "Model selection"
msgstr "Modellauswahl"

#: ../../Ch12/Ch12_Regression_12.rst:6
msgid ""
"One fairly major problem that remains is the problem of “model selection”. "
"That is, if we have a data set that contains several variables, which ones "
"should we include as predictors, and which ones should we not include? In "
"other words, we have a problem of **variable selection**. In general, model "
"selection is a complex business but it’s made somewhat simpler if we "
"restrict ourselves to the problem of choosing a subset of the variables that "
"ought to be included in the model. Nevertheless, I’m not going to try "
"covering even this reduced topic in a lot of detail. Instead, I’ll talk "
"about two broad principles that you need to think about, and then discuss "
"one concrete tool that jamovi provides to help you select a subset of "
"variables to include in your model. First, the two principles:"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:19
msgid ""
"It’s nice to have an actual substantive basis for your choices. That is, in "
"a lot of situations you the researcher have good reasons to pick out a "
"smallish number of possible regression models that are of theoretical "
"interest. These models will have a sensible interpretation in the context of "
"your field. Never discount the importance of this. Statistics serves the "
"scientific process, not the other way around."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:27
msgid ""
"To the extent that your choices rely on statistical inference, there is a "
"trade off between simplicity and goodness of fit. As you add more predictors "
"to the model you make it more complex. Each predictor adds a new free "
"parameter (i.e., a new regression coefficient), and each new parameter "
"increases the model’s capacity to “absorb” random variations. So the "
"goodness of fit (e.g., *R²*) continues to rise, sometimes trivially or by "
"chance, as you add more predictors no matter what. If you want your model to "
"be able to generalise well to new observations you need to avoid throwing in "
"too many variables."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:37
msgid ""
"This latter principle is often referred to as **Ockham’s razor** and is "
"often summarised in terms of the following pithy saying: *do not multiply "
"entities beyond necessity*. In this context, it means don’t chuck in a bunch "
"of largely irrelevant predictors just to boost your *R²*. Hmm. Yeah, the "
"original was better."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:43
msgid ""
"In any case, what we need is an actual mathematical criterion that will "
"implement the qualitative principle behind Ockham’s razor in the context of "
"selecting a regression model. As it turns out there are several "
"possibilities. The one that I’ll talk about is the **Akaike information "
"criterion** (AIC; `Akaike, 1974 <../Other/References."
"html#akaike-1974>`__\\ ) simply because it’s available as an option in "
"jamovi."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:50
msgid ""
"In the context of a linear regression model (and ignoring terms that don’t "
"depend on the model in any way!), the AIC for a model that has K predictor "
"variables plus an intercept is"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:54
msgid ""
"\\mbox{AIC} = \\displaystyle\\frac{\\mbox{SS}_{res}}{\\hat{\\sigma}^2} + 2K\n"
"\n"
msgstr ""
"\\mbox{AIC} = \\displaystyle\\frac{\\mbox{SS}_{res}}{\\hat{\\sigma}^2} + 2K\n"
"\n"

#: ../../Ch12/Ch12_Regression_12.rst:56
msgid ""
"The smaller the AIC value, the better the model performance. If we ignore "
"the low level details it’s fairly obvious what the AIC does. On the left we "
"have a term that increases as the model predictions get worse; on the right "
"we have a term that increases as the model complexity increases. The best "
"model is the one that fits the data well (low residuals, left hand side) "
"using as few predictors as possible (low K, right hand side). In short, this "
"is a simple implementation of Ockham’s razor."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:65
msgid ""
"AIC can be added to the ``Model Fit Measures`` output Table when the ``AIC`` "
"checkbox is clicked, and a rather clunky way of assessing different models "
"is seeing if the ``AIC`` value is lower if you remove one or more of the "
"predictors in the regression model. This is the only way currently "
"implemented in jamovi, but there are alternatives in other more powerful "
"programmes, such as R. These alternative methods can automate the process of "
"selectively removing (or adding) predictor variables to find the best AIC. "
"Although these methods are not implemented in jamovi, I will mention them "
"briefly below just so you know about them."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:77
msgid "Backward elimination"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:79
msgid ""
"In backward elimination you start with the complete regression model, "
"including all possible predictors. Then, at each “step” we try all possible "
"ways of removing one of the variables, and whichever of these is best (in "
"terms of lowest AIC value) is accepted. This becomes our new regression "
"model, and we then try all possible deletions from the new model, again "
"choosing the option with lowest AIC. This process continues until we end up "
"with a model that has a lower AIC value than any of the other possible "
"models that you could produce by deleting one of its predictors."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:90
msgid "Forward selection"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:92
msgid ""
"As an alternative, you can also try **forward selection**. This time around "
"we start with the smallest possible model as our start point, and only "
"consider the possible additions to the model. However, there’s one "
"complication. You also need to specify what the largest possible model "
"you’re willing to entertain is."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:98
msgid ""
"Although backward and forward selection can lead to the same conclusion, "
"they don’t always."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:102
msgid "A caveat"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:104
msgid ""
"Automated variable selection methods are seductive things, especially when "
"they’re bundled up in (fairly) simple functions in powerful statistical "
"programmes. They provide an element of objectivity to your model selection, "
"and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse "
"for thoughtlessness. No longer do you have to think carefully about which "
"predictors to add to the model and what the theoretical basis for their "
"inclusion might be. Everything is solved by the magic of AIC. And if we "
"start throwing around phrases like Ockham’s razor, well it sounds like "
"everything is wrapped up in a nice neat little package that no-one can argue "
"with."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:115
msgid ""
"Or, perhaps not. Firstly, there’s very little agreement on what counts as an "
"appropriate model selection criterion. When I was taught backward "
"elimination as an undergraduate, we used *F*-tests to do it, because that "
"was the default method used by the software. I’ve described using AIC, and "
"since this is an introductory text that’s the only method I’ve described, "
"but the AIC is hardly the Word of the Gods of Statistics. It’s an "
"approximation, derived under certain assumptions, and it’s guaranteed to "
"work only for large samples when those assumptions are met. Alter those "
"assumptions and you get a different criterion, like the BIC for instance "
"(also available in jamovi). Take a different approach again and you get the "
"NML criterion. Decide that you’re a Bayesian and you get model selection "
"based on posterior odds ratios. Then there are a bunch of regression "
"specific tools that I haven’t mentioned. And so on. All of these different "
"methods have strengths and weaknesses, and some are easier to calculate than "
"others (AIC is probably the easiest of the lot, which might account for its "
"popularity). Almost all of them produce the same answers when the answer is "
"“obvious” but there’s a fair amount of disagreement when the model selection "
"problem becomes hard."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:135
msgid ""
"What does this mean in practice? Well, you *could* go and spend several "
"years teaching yourself the theory of model selection, learning all the ins "
"and outs of it so that you could finally decide on what you personally think "
"the right thing to do is. Speaking as someone who actually did that, I "
"wouldn’t recommend it. You’ll probably come out the other side even more "
"confused than when you started. A better strategy is to show a bit of common "
"sense. If you’re staring at the results of an automated backwards or "
"forwards selection procedure, and the model that makes sense is close to "
"having the smallest AIC but is narrowly defeated by a model that doesn’t "
"make any sense, then trust your instincts. Statistical model selection is an "
"inexact tool, and as I said at the beginning, *interpretability matters*."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:149
msgid "Comparing two regression models"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:151
msgid ""
"An alternative to using automated model selection procedures is for the "
"researcher to explicitly select two or more regression models to compare to "
"each other. You can do this in a few different ways, depending on what "
"research question you’re trying to answer. Suppose we want to know whether "
"or not the amount of sleep that my son got has any relationship to my "
"grumpiness, over and above what we might expect from the amount of sleep "
"that I got. We also want to make sure that the day on which we took the "
"measurement has no influence on the relationship. That is, we’re interested "
"in the relationship between ``baby.sleep`` and ``dani.grump``, and from that "
"perspective ``dani.sleep`` and ``day`` are nuisance variable or "
"**covariates** that we want to control for. In this situation, what we would "
"like to know is whether ``dani.grump ~ dani.sleep + day + baby.sleep`` "
"(which I’ll call Model 2, or ``M2``) is a better regression model for these "
"data than ``dani.grump ~ dani.sleep + day`` (which I’ll call Model 1, or "
"``M1``). There are two different ways we can compare these two models, one "
"based on a model selection criterion like AIC, and the other based on an "
"explicit hypothesis test. I’ll show you the AIC based approach first because "
"it’s simpler, and follows naturally from discussion in the last section. The "
"first thing I need to do is actually run the two regressions, note the AIC "
"for each one, and then select the model with the smaller AIC value as it is "
"judged to be the better model for these data. Actually, don’t do this just "
"yet. Read on because there is an easy way in jamovi to get the AIC values "
"for different models included in one table.\\ [#]_"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:177
msgid ""
"A somewhat different approach to the problem comes out of the hypothesis "
"testing framework. Suppose you have two regression models, where one of them "
"(Model 1) contains a *subset* of the predictors from the other one (Model "
"2). That is, Model 2 contains all of the predictors included in Model 1, "
"plus one or more additional predictors. When this happens we say that Model "
"1 is **nested** within Model 2, or possibly that Model 1 is a **submodel** "
"of Model 2. Regardless of the terminology, what this means is that we can "
"think of Model 1 as a null hypothesis and Model 2 as an alternative "
"hypothesis. And in fact we can construct an *F* test for this in a fairly "
"straightforward fashion."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:188
msgid ""
"We can fit both models to the data and obtain a residual sum of squares for "
"both models. I’ll denote these as SS\\ :sub:`res`\\ :sup:`(1)` and SS\\ :sub:"
"`res`\\ :sup:`(2)` respectively. The superscripting here just indicates "
"which model we’re talking about. Then our *F* statistic is"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:194
msgid ""
"F = \\frac{(\\mbox{SS}_{res}^{(1)} - \\mbox{SS}_{res}^{(1)})/k}{(\\mbox{SS}"
"_{res}^{(2)})/(N-p-1)}\n"
"\n"
msgstr ""
"F = \\frac{(\\mbox{SS}_{res}^{(1)} - \\mbox{SS}_{res}^{(1)})/k}{(\\mbox{SS}"
"_{res}^{(2)})/(N-p-1)}\n"
"\n"

#: ../../Ch12/Ch12_Regression_12.rst:196
msgid ""
"where *N* is the number of observations, *p* is the number of predictors in "
"the full model (not including the intercept), and k is the difference in the "
"number of parameters between the two models.\\ [#]_ The degrees of freedom "
"here are *k* and *N* - *p* - 1. Note that it’s often more convenient to "
"think about the difference between those two SS values as a sum of squares "
"in its own right. That is"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:204
msgid ""
"SS\\ :sub:`Δ` = SS\\ :sub:`res`\\ :sup:`(1)` - SS\\ :sub:`res`\\ :sup:`(2)`"
msgstr ""
"SS\\ :sub:`Δ` = SS\\ :sub:`res`\\ :sup:`(1)` - SS\\ :sub:`res`\\ :sup:`(2)`"

#: ../../Ch12/Ch12_Regression_12.rst:206
msgid ""
"The reason why this is helpful is that we can express SS\\ :sub:`Δ` as a "
"measure of the extent to which the two models make different predictions "
"about the the outcome variable. Specifically,"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:211
msgid ""
"SS\\ :sub:`Δ` = :math:`\\sum_{i} \\left(\\hat{y}_i^{(2)} - \\hat{y}_i^{(1)} "
"\\right)^2`"
msgstr ""
"SS\\ :sub:`Δ` = :math:`\\sum_{i} \\left(\\hat{y}_i^{(2)} - \\hat{y}_i^{(1)} "
"\\right)^2`"

#: ../../Ch12/Ch12_Regression_12.rst:213
msgid ""
"where *ŷ*\\ :sub:`i`\\ :sup:`(1)` is the fitted value for *y*\\ :sub:`i` "
"according to model M\\ :sub:`1` and *ŷ*\\ :sub:`i`\\ :sup:`(2)` is the "
"fitted value for *y*\\ :sub:`i` according to model M\\ :sub:`2`."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:223
msgid "Model comparison in jamovi using the ``Model Builder`` option"
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:227
msgid ""
"Okay, so that’s the hypothesis test that we use to compare two regression "
"models to one another. Now, how do we do it in jamovi? The answer is to use "
"the ``Model Builder`` option and specify the Model 1 predictors ``dani."
"sleep`` and ``day`` in ``Block 1`` and then add the additional predictor "
"from Model 2 (``baby.sleep``) in ``Block 2``, as in :numref:`fig-reg8`. This "
"shows, in the ``Model Comparisons`` Table, that for the comparisons between "
"Model 1 and Model 2, *F*\\ (1,96) = 0.00, *p* = 0.954. Since we have *p* > "
"0.05 we retain the null hypothesis (``M1``). This approach to regression, in "
"which we add all of our covariates into a null model, then *add* the "
"variables of interest into an alternative model, and then compare the two "
"models in a hypothesis testing framework, is often referred to as "
"**hierarchical regression**."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:240
msgid ""
"We can also use this ``Model Comparison`` option to display a table that "
"shows the AIC and BIC for each model, making it easy to compare and identify "
"which model has the lowest value, as in :numref:`fig-reg8`."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:247
msgid ""
"While I’m on this topic I should point out that the empirical evidence "
"suggests that BIC is a better criterion than AIC. In most simulation studies "
"that I’ve seen, BIC does a much better job of selecting the correct model."
msgstr ""

#: ../../Ch12/Ch12_Regression_12.rst:252
msgid ""
"It’s worth noting in passing that this same *F*-statistic can be used to "
"test a much broader range of hypotheses than those that I’m mentioning here. "
"Very briefly, notice that the nested model M1 corresponds to the full model "
"M2 when we constrain some of the regression coefficients to zero. It is "
"sometimes useful to construct sub-models by placing other kinds of "
"constraints on the regression coefficients. For instance, maybe two "
"different coefficients might have to sum to zero, or something like that. "
"You can construct hypothesis tests for those kind of constraints too, but it "
"is somewhat more complicated and the sampling distribution for *F* can end "
"up being something known as the non-central *F*-distribution, which is "
"waaaaay beyond the scope of this book! All I want to do is alert you to this "
"possibility."
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:4
msgid "Summary"
msgstr "Zusammenfassung"

#: ../../Ch12/Ch12_Regression_13.rst:6
msgid ""
"Want to know how strong the relationship is between two variables? Calculate "
"a :doc:`correlation <../Ch12/Ch12_Regression_01>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:9
msgid "Drawing :doc:`scatterplots <../Ch12/Ch12_Regression_02>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:11
msgid ""
":doc:`Basic ideas in linear regression <../Ch12/Ch12_Regression_03>` and :"
"doc:`how regression models are estimated <../Ch12/Ch12_Regression_04>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:14
msgid ":doc:`Multiple linear regression <../Ch12/Ch12_Regression_05>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:16
msgid ""
"Measuring the :doc:`overall performance of a regression model using R² <../"
"Ch12/Ch12_Regression_06>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:19
msgid ""
":doc:`Hypothesis tests for regression models <../Ch12/Ch12_Regression_07>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:21
msgid ""
":doc:`Calculating confidence intervals for regression coefficients and "
"standardised coefficients <../Ch12/Ch12_Regression_09>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:24
msgid ""
"The :doc:`assumptions of regression <../Ch12/Ch12_Regression_10>` and :doc:"
"`how to check them <../Ch12/Ch12_Regression_11>`"
msgstr ""

#: ../../Ch12/Ch12_Regression_13.rst:27
msgid ":doc:`Selecting a regression model <../Ch12/Ch12_Regression_12>`"
msgstr ""
