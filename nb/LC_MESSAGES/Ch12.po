msgid ""
msgstr ""
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: sebastian.jentschke@uib.no\n"
"POT-Creation-Date: 2025-06-12 11:37+0200\n"
"PO-Revision-Date: 2025-07-02 12:45+0000\n"
"Last-Translator: Anonymous <noreply@weblate.org>\n"
"Language-Team: Norwegian Bokmål <https://hosted.weblate.org/projects/lsjdocs/"
"ch12/nb_NO/>\n"
"Language: nb\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=n != 1;\n"
"X-Generator: Weblate 5.13-dev\n"
"Generated-By: Babel 2.17.0\n"

#: ../../Ch12/Ch12_Regression.rst:4
msgid "Correlation and linear regression"
msgstr "Korrelasjon og lineær regresjon"

#: ../../Ch12/Ch12_Regression.rst:24
msgid ""
"The goal in this chapter is to introduce **correlation** and **linear "
"regression**. These are the standard tools that statisticians rely on when "
"analysing the relationship between continuous predictors |continuous| and "
"continuous outcomes |continuous|."
msgstr ""
"Målet med dette kapittelet er å introdusere **korrelasjon** og **lineær "
"regresjon**. Dette er standardverktøyene som statistikere bruker når de "
"analyserer forholdet mellom kontinuerlige prediktorer |continuous| og "
"kontinuerlige utfall |continuous|."

#: ../../Ch12/Ch12_Regression.rst:31 ../../Ch12/Ch12_Regression_01.rst:423
msgid "continuous"
msgstr "continuous"

#: ../../Ch12/Ch12_Regression_01.rst:4
msgid "Correlations"
msgstr "Sammenhenger"

#: ../../Ch12/Ch12_Regression_01.rst:6
msgid ""
"In this section we’ll talk about how to describe the relationships *between* "
"variables in the data. To do that, we want to talk mostly about the "
"**correlation** between variables. But first, we need some data."
msgstr ""
"I denne delen skal vi snakke om hvordan vi kan beskrive sammenhengene "
"*mellom* variablene i dataene. For å gjøre det vil vi først og fremst snakke "
"om **korrelasjonen** mellom variablene. Men først trenger vi noen data."

#: ../../Ch12/Ch12_Regression_01.rst:12
msgid "The data"
msgstr "Dataene"

#: ../../Ch12/Ch12_Regression_01.rst:14
msgid "Descriptive statistics for the |parenthood|_ data set."
msgstr "Deskriptivstatistikk for datasettet |parenthood|_."

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "Variable"
msgstr "Variabel"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "min"
msgstr "min"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "max"
msgstr "maks"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "mean"
msgstr "gjennomsnitt"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "median"
msgstr "median"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "std. dev"
msgstr "std. avvik"

#: ../../Ch12/Ch12_Regression_01.rst:18
msgid "IQR"
msgstr "IQR"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "**Dani’s grumpiness**"
msgstr "**Danis grettenhet**"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "41"
msgstr "41"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "91"
msgstr "91"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "63.71"
msgstr "63.71"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "62"
msgstr "62"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "10.05"
msgstr "10.05"

#: ../../Ch12/Ch12_Regression_01.rst:20
msgid "14"
msgstr "14"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "**Dani’s hours slept**"
msgstr "**Dani har sovet**"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "4.84"
msgstr "4.84"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "9.00"
msgstr "9.00"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "6.97"
msgstr "6.97"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "7.03"
msgstr "7.03"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "1.02"
msgstr "1.02"

#: ../../Ch12/Ch12_Regression_01.rst:22
msgid "1.45"
msgstr "1.45"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "**Dani’s son’s hours slept**"
msgstr "**Danis sønns timer sov**"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "3.25"
msgstr "3.25"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "12.07"
msgstr "12.07"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "8.05"
msgstr "8.05"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "7.95"
msgstr "7.95"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "2.07"
msgstr "2.07"

#: ../../Ch12/Ch12_Regression_01.rst:24
msgid "3.21"
msgstr "3.21"

#: ../../Ch12/Ch12_Regression_01.rst:27
msgid ""
"Let’s turn to a topic close to every parent’s heart: sleep. The data set "
"we’ll use is fictitious, but based on real events. Suppose I’m curious to "
"find out how much my infant son’s sleeping habits affect my mood. Let’s say "
"that I can rate my grumpiness very precisely, on a scale from 0 (not at all "
"grumpy) to 100 (grumpy as a very, very grumpy old man or woman). And lets "
"also assume that I’ve been measuring my grumpiness, my sleeping patterns and "
"my son’s sleeping patterns for 100 days. The data are stored in the |"
"parenthood|_ data set, that contains four variables ``dani.sleep``, ``baby."
"sleep``, ``dani.grump`` and ``day``."
msgstr ""
"La oss ta for oss et tema som ligger alle foreldre på hjertet: søvn. "
"Datasettet vi skal bruke, er fiktivt, men basert på virkelige hendelser. "
"Anta at jeg er nysgjerrig på hvor mye søvnvanene til min lille sønn påvirker "
"humøret mitt. La oss si at jeg kan rangere grettenheten min svært nøyaktig, "
"på en skala fra 0 (ikke gretten i det hele tatt) til 100 (gretten som en "
"veldig, veldig gretten gammel mann eller kvinne). Og la oss også anta at jeg "
"har målt grettenheten min, søvnmønsteret mitt og søvnmønsteret til sønnen "
"min i 100 dager. Dataene er lagret i datasettet |parenthood|_, som "
"inneholder fire variabler: ``dani.sleep``, ``baby.sleep``, ``dani.grump`` og "
"``day``."

#: ../../Ch12/Ch12_Regression_01.rst:37
msgid ""
"Next, I’ll take a look at some basic descriptive statistics and, to give a "
"graphical depiction of what each of the three interesting variables looks "
"like, :numref:`fig-grumpHist` plots histograms. One thing to note: just "
"because jamovi can calculate dozens of different statistics doesn’t mean you "
"should report all of them. If I were writing this up for a report, I’d "
"probably pick out those statistics that are of most interest to me (and to "
"my readership), and then put them into a nice, simple table like the one in :"
"numref:`tab-parenthood`.\\ [#]_ Notice that when I put it into a table, I "
"gave everything “human readable” names. This is always good practice. Notice "
"also that I’m not getting enough sleep. This isn’t good practice, but other "
"parents tell me that it’s pretty standard."
msgstr ""
"Deretter tar jeg en titt på noen grunnleggende deskriptivstatistiske mål, og "
"for å gi en grafisk fremstilling av hvordan hver av de tre interessante "
"variablene ser ut, plotter :numref:`fig-grumpHist` histogrammer. En ting å "
"merke seg: Bare fordi jamovi kan beregne dusinvis av forskjellige "
"statistikker, betyr ikke det at du bør rapportere dem alle. Hvis jeg skulle "
"skrive dette for en rapport, ville jeg sannsynligvis plukket ut den "
"statistikken som er mest interessant for meg (og for leserne mine), og "
"deretter satt dem inn i en fin, enkel tabell som den i :numref:`tab-"
"parenthood`.\\ [#]_ Legg merke til at da jeg satte det inn i en tabell, ga "
"jeg alt «menneskelige lesbare» navn. Dette er alltid god praksis. Legg også "
"merke til at jeg ikke får nok søvn. Dette er ikke god praksis, men andre "
"foreldre forteller meg at det er ganske vanlig."

#: ../../Ch12/Ch12_Regression_01.rst:51
msgid "Histograms for three variables from the |parenthood|_ data set"
msgstr "Histogrammer for tre variabler fra datasettet |parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:55
msgid ""
"Histograms for the three interesting variables in the |parenthood|_ data set"
msgstr ""
"Histogrammer for de tre interessante variablene i datasettet |parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:61
msgid "The strength and direction of a relationship"
msgstr "Styrken og retningen på et forhold"

#: ../../Ch12/Ch12_Regression_01.rst:63
msgid ""
"We can draw scatterplots to give us a general sense of how closely related "
"two variables are. Ideally though, we might want to say a bit more about it "
"than that. For instance, let’s compare the relationship between ``dani."
"sleep`` and ``dani.grump`` (:numref:`fig-grumpCor1`, left) with that between "
"``baby.sleep`` and ``dani.grump`` (:numref:`fig-grumpCor1`, right). When "
"looking at these two plots side by side, it’s clear that the relationship is "
"*qualitatively* the same in both cases: more sleep equals less grump! "
"However, it’s also pretty obvious that the relationship between ``dani."
"sleep`` and ``dani.grump`` is *stronger* than the relationship between "
"``baby.sleep`` and ``dani.grump``. The plot on the left is “neater” than the "
"one on the right. What it feels like is that if you want to predict what my "
"mood is, it’d help you a little bit to know how many hours my son slept, but "
"it’d be more helpful to know how many hours I slept."
msgstr ""
"Vi kan tegne spredningsdiagrammer for å få en generell følelse av hvor nært "
"beslektede to variabler er. Ideelt sett vil vi kanskje si litt mer om det "
"enn det. La oss for eksempel sammenligne forholdet mellom ``dani.sleep`` og "
"``dani.grump`` (:numref:`fig-grumpCor1`, til venstre) med forholdet mellom "
"``baby.sleep`` og ``dani.grump`` (:numref:`fig-grumpCor1`, til høyre). Når "
"man ser på disse to plottene side om side, er det tydelig at forholdet er "
"*kvalitativt* det samme i begge tilfeller: mer søvn gir mindre grump! Det er "
"imidlertid også ganske åpenbart at forholdet mellom ``dani.sleep`` og ``dani."
"grump`` er *sterkere* enn forholdet mellom ``baby.sleep`` og ``dani.grump``. "
"Plottet til venstre er «penere» enn det til høyre. Det føles som om at hvis "
"du vil forutsi humøret mitt, vil det hjelpe deg litt å vite hvor mange timer "
"sønnen min har sovet, men det vil være mer nyttig å vite hvor mange timer "
"jeg har sovet."

#: ../../Ch12/Ch12_Regression_01.rst:80
msgid ""
"Scatterplots between ``dani.sleep`` and ``baby.sleep`` to ``dani.grump``"
msgstr ""
"Spredningsdiagrammer mellom ``dani.sleep`` og ``baby.sleep`` til ``dani."
"grump``"

#: ../../Ch12/Ch12_Regression_01.rst:84
msgid ""
"Scatterplots showing the relationship between ``dani.sleep`` and ``dani."
"grump`` (left panel) and the relationship between ``baby.sleep`` and ``dani."
"grump`` (right panel)."
msgstr ""
"Spredningsdiagrammer som viser forholdet mellom ``dani.sleep`` og ``dani."
"grump`` (venstre panel) og forholdet mellom ``baby.sleep`` og ``dani.grump`` "
"(høyre panel)."

#: ../../Ch12/Ch12_Regression_01.rst:90
msgid ""
"In contrast, let’s consider the two scatterplots shown in :numref:`fig-"
"grumpCor2`. If we compare the scatterplot of ``baby.sleep`` vs. ``dani."
"grump`` (left) to the scatterplot of ``baby.sleep`` vs. ``dani.sleep`` "
"(right), the overall strength of the relationship is the same, but the "
"direction is different. That is, if my son sleeps more, I get *more* sleep "
"(positive relationship, right hand side), but if he sleeps more then I get "
"*less* grumpy (negative relationship, left hand side)."
msgstr ""
"La oss derimot se på de to spredningsdiagrammene som vises i :numref:`fig-"
"grumpCor2`. Hvis vi sammenligner spredningsdiagrammet for ``baby.sleep`` vs. "
"``dani.grump`` (til venstre) med spredningsdiagrammet for ``baby.sleep`` vs. "
"``dani.sleep`` (til høyre), er den generelle styrken i forholdet den samme, "
"men retningen er forskjellig. Det vil si at hvis sønnen min sover mer, får "
"jeg *mer* søvn (positivt forhold, høyre side), men hvis han sover mer, blir "
"jeg *mindre* gretten (negativt forhold, venstre side)."

#: ../../Ch12/Ch12_Regression_01.rst:101
msgid "Scatterplots between baby.sleep to dani.grump and dani.sleep"
msgstr "Spredningsdiagrammer mellom baby.sleep til dani.grump og dani.sleep"

#: ../../Ch12/Ch12_Regression_01.rst:105
msgid ""
"Scatterplots showing the relationship between ``baby.sleep`` and ``dani."
"grump`` (left panel) and the relationship between ``baby.sleep`` and ``dani."
"sleep`` (right panel)."
msgstr ""
"Spredningsdiagrammer som viser forholdet mellom ``baby.sleep`` og ``dani."
"grump`` (venstre panel) og forholdet mellom ``baby.sleep`` og ``dani.sleep`` "
"(høyre panel)."

#: ../../Ch12/Ch12_Regression_01.rst:112
msgid "The correlation coefficient"
msgstr "Korrelasjonskoeffisienten"

#: ../../Ch12/Ch12_Regression_01.rst:114
msgid ""
"We can make these ideas a bit more explicit by introducing the idea of a "
"**correlation coefficient** (or, more specifically, Pearson’s correlation "
"coefficient), which is traditionally denoted as *r*. The correlation "
"coefficient between two variables *X* and *Y* (sometimes denoted r\\ :sub:"
"`XY`), which we’ll define more precisely in the next section, is a measure "
"that varies from -1 to 1. When *r* = -1 it means that we have a perfect "
"negative relationship, and when *r* = 1 it means we have a perfect positive "
"relationship. When *r* = 0, there’s no relationship at all. If you look at :"
"numref:`fig-corr`, you can see several plots showing what different "
"correlations look like."
msgstr ""
"Vi kan gjøre disse ideene litt mer eksplisitte ved å introdusere ideen om en "
"**korrelasjonskoeffisient** (eller, mer spesifikt, Pearsons "
"korrelasjonskoeffisient), som tradisjonelt betegnes som *r*. "
"Korrelasjonskoeffisienten mellom to variabler *X* og *Y* (noen ganger "
"betegnet r\\ :sub:`XY`), som vi skal definere mer presist i neste avsnitt, "
"er et mål som varierer fra -1 til 1. Når *r* = -1, betyr det at vi har et "
"perfekt negativt forhold, og når *r* = 1, betyr det at vi har et perfekt "
"positivt forhold. Når *r* = 0, er det ikke noe forhold i det hele tatt. Hvis "
"du ser på :numref:`fig-corr`, kan du se flere plott som viser hvordan ulike "
"korrelasjoner ser ut."

#: ../../Ch12/Ch12_Regression_01.rst:126
msgid "Effect of varying the strength and direction of a correlation"
msgstr "Effekten av å variere styrken og retningen på en korrelasjon"

#: ../../Ch12/Ch12_Regression_01.rst:130
msgid ""
"Illustration of the effect of varying the strength and direction of a "
"correlation. In the left hand column, the correlations are 0.00, 0.33, 0.67 "
"and 1.00 In the right hand column, the correlations are 0.00, -0.33, -0.67 "
"and -1.00."
msgstr ""
"Illustrasjon av effekten av å variere styrken og retningen på en "
"korrelasjon. I venstre kolonne er korrelasjonene 0,00, 0,33, 0,67 og 1,00. I "
"høyre kolonne er korrelasjonene 0,00, -0,33, -0,67 og -1,00."

#: ../../Ch12/Ch12_Regression_01.rst:137
msgid ""
"The formula for the Pearson’s correlation coefficient can be written in "
"several different ways. I think the simplest way to write down the formula "
"is to break it into two steps. Firstly, let’s introduce the idea of a "
"**covariance**. The covariance between two variables *X* and *Y* is a "
"generalisation of the notion of the variance amd is a mathematically simple "
"way of describing the relationship between two variables that isn’t terribly "
"informative to humans"
msgstr ""
"Formelen for Pearsons korrelasjonskoeffisient kan skrives på flere "
"forskjellige måter. Jeg tror den enkleste måten å skrive formelen på er å "
"dele den opp i to trinn. La oss først introdusere ideen om en **kovarians**. "
"Kovariansen mellom to variabler *X* og *Y* er en generalisering av "
"variansbegrepet og er en matematisk enkel måte å beskrive forholdet mellom "
"to variabler som ikke er så veldig informative for mennesker"

#: ../../Ch12/Ch12_Regression_01.rst:145
msgid ""
"\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(X_i - \\bar{X} "
"\\right) \\left(Y_i - \\bar{Y} \\right)\n"
"\n"
msgstr ""
"\\mbox{Cov}(X,Y) = \\frac{1}{N-1} \\sum_{i=1}^N \\left(X_i - \\bar{X} \\right"
") \\left(Y_i - \\bar{Y} \\right)\n"
"\n"

#: ../../Ch12/Ch12_Regression_01.rst:147
msgid ""
"Because we’re multiplying (i.e., taking the “product” of) a quantity that "
"depends on *X* by a quantity that depends on *Y* and then averaging,\\ [#]_ "
"you can think of the formula for the covariance as an “average cross "
"product” between *X* and *Y*."
msgstr ""
"Fordi vi multipliserer (dvs. tar «produktet» av) en størrelse som avhenger "
"av *X*, med en størrelse som avhenger av *Y*, og deretter tar gjennomsnittet,"
"\\ [#]_ kan du tenke på formelen for kovariansen som et «gjennomsnittlig "
"kryssprodukt» mellom *X* og *Y*."

#: ../../Ch12/Ch12_Regression_01.rst:152
msgid ""
"The covariance has the nice property that, if *X* and *Y* are entirely "
"unrelated, then the covariance is exactly zero. If the relationship between "
"them is positive (in the sense shown in :numref:`fig-corr`) then the "
"covariance is also positive, and if the relationship is negative then the "
"covariance is also negative. In other words, the covariance captures the "
"basic qualitative idea of correlation. Unfortunately, the raw magnitude of "
"the covariance isn’t easy to interpret as it depends on the units in which "
"*X* and *Y* are expressed and, worse yet, the actual units that the "
"covariance itself is expressed in are really weird. For instance, if *X* "
"refers to the ``dani.sleep`` variable (units: hours) and *Y* refers to the "
"``dani.grump`` variable (units: grumps), then the units for their covariance "
"are “hours × grumps”. And I have no freaking idea what that would even mean."
msgstr ""
"Kovariansen har den fine egenskapen at hvis *X* og *Y* er helt urelaterte, "
"så er kovariansen nøyaktig null. Hvis forholdet mellom dem er positivt (i "
"den betydningen som vises i :numref:`fig-corr`), er kovariansen også "
"positiv, og hvis forholdet er negativt, er kovariansen også negativ. "
"Kovariansen fanger med andre ord opp den grunnleggende kvalitative ideen om "
"korrelasjon. Dessverre er det ikke lett å tolke den rå størrelsen på "
"kovariansen, ettersom den avhenger av enhetene som *X* og *Y* uttrykkes i, "
"og, enda verre, de faktiske enhetene som selve kovariansen uttrykkes i, er "
"veldig rare. Hvis *X* for eksempel refererer til variabelen ``dani.sleep`` "
"(enheter: timer) og *Y* refererer til variabelen ``dani.grump`` (enheter: "
"grumps), så er enhetene for kovariansen «timer × grumps». Og jeg har ingen "
"anelse om hva det betyr."

#: ../../Ch12/Ch12_Regression_01.rst:165
msgid ""
"The Pearson correlation coefficient *r* fixes this interpretation problem by "
"standardising the covariance, in pretty much the exact same way that the *z*-"
"score standardises a raw score, by dividing by the standard deviation. "
"However, because we have two variables that contribute to the covariance, "
"the standardisation only works if we divide by both standard deviations.\\ "
"[#]_ In other words, the correlation between *X* and *Y* can be written as "
"follows:"
msgstr ""
"Pearson-korrelasjonskoeffisienten *r* løser dette tolkningsproblemet ved å "
"standardisere kovariansen, på omtrent samme måte som *z*-skåren "
"standardiserer en råskåre, ved å dividere med standardavviket. Men fordi vi "
"har to variabler som bidrar til kovariansen, fungerer standardiseringen bare "
"hvis vi dividerer med begge standardavvikene.\\ [#]_ Med andre ord kan "
"korrelasjonen mellom *X* og *Y* skrives på følgende måte:"

#: ../../Ch12/Ch12_Regression_01.rst:173
msgid ""
"r_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n"
"\n"
msgstr ""
"r_{XY}  = \\frac{\\mbox{Cov}(X,Y)}{ \\hat{\\sigma}_X \\ \\hat{\\sigma}_Y}\n"
"\n"

#: ../../Ch12/Ch12_Regression_01.rst:175
msgid ""
"By standardising the covariance, not only do we keep all of the nice "
"properties of the covariance discussed earlier, but the actual values of *r* "
"are on a meaningful scale: *r* = 1 implies a perfect positive relationship "
"and *r* = -1 implies a perfect negative relationship. I’ll expand a little "
"more on this point later, in subsection :ref:`Interpreting a correlation "
"<interpreting_a_correlation>`. But before I do, let’s look at how to "
"calculate correlations in jamovi."
msgstr ""
"Ved å standardisere kovariansen beholder vi ikke bare alle de gode "
"egenskapene til kovariansen som vi har diskutert tidligere, men de faktiske "
"verdiene av *r* ligger på en meningsfull skala: *r* = 1 innebærer et perfekt "
"positivt forhold, og *r* = -1 innebærer et perfekt negativt forhold. Jeg "
"skal utdype dette poenget litt mer senere, i underkapittelet :ref:`Tolke en "
"korrelasjon <interpreting_a_correlation>`. Men før jeg gjør det, la oss se "
"på hvordan man beregner korrelasjoner i jamovi."

#: ../../Ch12/Ch12_Regression_01.rst:184
msgid "Calculating correlations in jamovi"
msgstr "Beregning av korrelasjoner i jamovi"

#: ../../Ch12/Ch12_Regression_01.rst:186
msgid ""
"Calculating correlations in jamovi can be done by clicking on the "
"``Regression`` → ``Correlation Matrix`` button. Transfer all four continuous "
"variables |continuous| across into the box on the right to get the output "
"in :numref:`fig-correlations`."
msgstr ""
"Du kan beregne korrelasjoner i jamovi ved å klikke på knappen ``Regression`` "
"→ ``Correlation Matrix``. Overfør alle de fire kontinuerlige variablene |"
"continuous| over i boksen til høyre for å få utgaven i :numref:`fig-"
"correlations`."

#: ../../Ch12/Ch12_Regression_01.rst:193
msgid "jamovi screenshot with correlations in the |parenthood|_ data set"
msgstr "jamovi skjermbilde med korrelasjoner i datasettet |parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:197
msgid ""
"jamovi screenshot showing correlations between variables in the |parenthood|"
"_ data set"
msgstr ""
"jamovi-skjermbilde som viser korrelasjoner mellom variabler i datasettet |"
"parenthood|_"

#: ../../Ch12/Ch12_Regression_01.rst:205
msgid "Interpreting a correlation"
msgstr "Tolkning av en korrelasjon"

#: ../../Ch12/Ch12_Regression_01.rst:207
msgid ""
"Naturally, in real life you don’t see many correlations of 1. So how should "
"you interpret a correlation of, say, *r* = 0.4? The honest answer is that it "
"really depends on what you want to use the data for, and on how strong the "
"correlations in your field tend to be. A friend of mine in engineering once "
"argued that any correlation less than 0.95 is completely useless (I think he "
"was exaggerating, even for engineering). On the other hand, there are real "
"cases, even in psychology, where you should really expect correlations that "
"strong. For instance, one of the benchmark data sets used to test theories "
"of how people judge similarities is so clean that any theory that can’t "
"achieve a correlation of at least 0.9 really isn’t deemed to be successful. "
"However, when looking for (say) elementary correlates of intelligence (e.g., "
"inspection time, response time), if you get a correlation above 0.3 you’re "
"doing very very well. In short, the interpretation of a correlation depends "
"a lot on the context. That said, the rough guide in :numref:`tab-"
"interpretcorrelations` is pretty typical."
msgstr ""
"I det virkelige liv ser du naturligvis ikke mange korrelasjoner på 1. Så "
"hvordan skal du tolke en korrelasjon på, la oss si, *r* = 0,4? Det ærlige "
"svaret er at det kommer an på hva du vil bruke dataene til, og på hvor "
"sterke korrelasjonene pleier å være på ditt felt. En ingeniørvenn av meg "
"hevdet en gang at enhver korrelasjon under 0,95 er helt ubrukelig (jeg tror "
"han overdrev, selv for ingeniørfag). På den annen side finnes det reelle "
"tilfeller, selv innen psykologi, der du virkelig bør forvente så sterke "
"korrelasjoner. For eksempel er et av referansedatasettene som brukes til å "
"teste teorier om hvordan mennesker bedømmer likheter, så rent at enhver "
"teori som ikke oppnår en korrelasjon på minst 0,9, ikke anses som vellykket. "
"Men når man leter etter (la oss si) elementære korrelater til intelligens (f."
"eks. inspeksjonstid, responstid), gjør man det veldig, veldig bra hvis man "
"oppnår en korrelasjon på over 0,3. Kort sagt, tolkningen av en korrelasjon "
"avhenger mye av konteksten. Når det er sagt, er den grove veiledningen i :"
"numref:`tab-interpretcorrelations` ganske typisk."

#: ../../Ch12/Ch12_Regression_01.rst:223
msgid ""
"A rough guide to interpreting correlations. Note that I say a *rough* guide. "
"There aren’t hard and fast rules for what counts as strong or weak "
"relationships. It depends on the context."
msgstr ""
"En grov guide til tolkning av korrelasjoner. Merk at jeg sier en *grov* "
"veiledning. Det finnes ingen faste regler for hva som regnes som sterke "
"eller svake sammenhenger. Det avhenger av konteksten."

#: ../../Ch12/Ch12_Regression_01.rst:229
msgid "Correlation"
msgstr "Korrelasjon"

#: ../../Ch12/Ch12_Regression_01.rst:229
msgid "Strength"
msgstr "Styrke"

#: ../../Ch12/Ch12_Regression_01.rst:229
msgid "Direction"
msgstr "Retning"

#: ../../Ch12/Ch12_Regression_01.rst:231
msgid "-1.0 to -0.9"
msgstr "-1,0 til -0,9"

#: ../../Ch12/Ch12_Regression_01.rst:231 ../../Ch12/Ch12_Regression_01.rst:249
msgid "Very strong"
msgstr "Veldig sterk"

#: ../../Ch12/Ch12_Regression_01.rst:231 ../../Ch12/Ch12_Regression_01.rst:233
#: ../../Ch12/Ch12_Regression_01.rst:235 ../../Ch12/Ch12_Regression_01.rst:237
#: ../../Ch12/Ch12_Regression_01.rst:239
msgid "Negative"
msgstr "Negativ"

#: ../../Ch12/Ch12_Regression_01.rst:233
msgid "-0.9 to -0.7"
msgstr "-0,9 til -0,7"

#: ../../Ch12/Ch12_Regression_01.rst:233 ../../Ch12/Ch12_Regression_01.rst:247
msgid "Strong"
msgstr "Sterk"

#: ../../Ch12/Ch12_Regression_01.rst:235
msgid "-0.7 to -0.4"
msgstr "-0,7 til -0,4"

#: ../../Ch12/Ch12_Regression_01.rst:235 ../../Ch12/Ch12_Regression_01.rst:245
msgid "Moderate"
msgstr "Moderat"

#: ../../Ch12/Ch12_Regression_01.rst:237
msgid "-0.4 to -0.2"
msgstr "-0,4 til -0,2"

#: ../../Ch12/Ch12_Regression_01.rst:237 ../../Ch12/Ch12_Regression_01.rst:243
msgid "Weak"
msgstr "Svak"

#: ../../Ch12/Ch12_Regression_01.rst:239
msgid "-0.2 to  0.0"
msgstr "-0,2 til 0,0"

#: ../../Ch12/Ch12_Regression_01.rst:239 ../../Ch12/Ch12_Regression_01.rst:241
msgid "Negligible"
msgstr "Ubetydelig"

#: ../../Ch12/Ch12_Regression_01.rst:241
msgid "0.0 to  0.2"
msgstr "0,0 til 0,2"

#: ../../Ch12/Ch12_Regression_01.rst:241 ../../Ch12/Ch12_Regression_01.rst:243
#: ../../Ch12/Ch12_Regression_01.rst:245 ../../Ch12/Ch12_Regression_01.rst:247
#: ../../Ch12/Ch12_Regression_01.rst:249
msgid "Positive"
msgstr "Positiv"

#: ../../Ch12/Ch12_Regression_01.rst:243
msgid "0.2 to  0.4"
msgstr "0,2 til 0,4"

#: ../../Ch12/Ch12_Regression_01.rst:245
msgid "0.4 to  0.7"
msgstr "0,4 til 0,7"

#: ../../Ch12/Ch12_Regression_01.rst:247
msgid "0.7 to  0.9"
msgstr "0,7 til 0,9"

#: ../../Ch12/Ch12_Regression_01.rst:249
msgid "0.9 to  1.0"
msgstr "0,9 til 1,0"

#: ../../Ch12/Ch12_Regression_01.rst:255
msgid "Anscombe’s quartet"
msgstr "Anscombes kvartett"

#: ../../Ch12/Ch12_Regression_01.rst:259
msgid ""
"Anscombe’s quartet: All four of these data sets have a Pearson correlation "
"of *r* = 0.816, but they are qualitatively different from one another."
msgstr ""
"Anscombes kvartett: Alle disse fire datasettene har en Pearson-korrelasjon "
"på *r* = 0,816, men de er kvalitativt forskjellige fra hverandre."

#: ../../Ch12/Ch12_Regression_01.rst:264
msgid ""
"However, something that can never be stressed enough is that you should "
"*always* look at the scatterplot before attaching any interpretation to the "
"data. A correlation might not mean what you think it means. The classic "
"illustration of this is “Anscombe’s Quartet” (:ref:`Anscombe, 1973 "
"<Anscombe_1973>`), a collection of four data sets. Each data set has two "
"variables, an *X* and a *Y*. For all four data sets the mean value for *X* "
"is 9 and the mean for *Y* is 7.5. The standard deviations for all *X* "
"variables are almost identical, as are those for the *Y* variables. And in "
"each case the correlation between *X* and *Y* is *r* = 0.816`. You can "
"verify this yourself, since I happen to have saved it as a dataset called |"
"anscombe|_."
msgstr ""
"Noe som imidlertid aldri kan understrekes nok, er at du *alltid* bør se på "
"spredningsdiagrammet før du tolker dataene. Det er ikke sikkert at en "
"korrelasjon betyr det du tror den betyr. Den klassiske illustrasjonen på "
"dette er «Anscombes kvartett» (:ref:`Anscombe, 1973 <Anscombe_1973>`), en "
"samling av fire datasett. Hvert datasett har to variabler, en *X* og en *Y*. "
"For alle de fire datasettene er gjennomsnittsverdien for *X* 9, og "
"gjennomsnittsverdien for *Y* er 7,5. Standardavvikene for alle *X*-"
"variablene er nesten identiske, og det samme er standardavvikene for *Y*-"
"variablene. Og i begge tilfeller er korrelasjonen mellom *X* og *Y* *r* = "
"0,816`. Du kan sjekke dette selv, siden jeg tilfeldigvis har lagret det som "
"et datasett kalt |anscombe|_."

#: ../../Ch12/Ch12_Regression_01.rst:275
msgid ""
"You’d think that these four data sets would look pretty similar to one "
"another. They do not. If we draw scatterplots of *X* against *Y* for all "
"four variables, as shown in :numref:`fig-anscombe`, we see that all four of "
"these are *spectacularly* different to each other. The lesson here, which so "
"very many people seem to forget in real life, is *always graph your raw "
"data* (chapter :doc:`../Ch05/Ch05_Graphics`)."
msgstr ""
"Man skulle tro at disse fire datasettene var ganske like hverandre. Men det "
"gjør de ikke. Hvis vi tegner spredningsdiagrammer av *X* mot *Y* for alle "
"fire variablene, som vist i :numref:`fig-anscombe`, ser vi at alle disse "
"fire er *spektakulært* forskjellige fra hverandre. Lærdommen her, som så "
"veldig mange ser ut til å glemme i det virkelige liv, er *alltid å tegne "
"diagram av rådataene* (kapittel :doc:`../Ch05/Ch05_Graphics`)."

#: ../../Ch12/Ch12_Regression_01.rst:283
msgid "Spearman’s rank correlations"
msgstr "Spearmans rangkorrelasjoner"

#: ../../Ch12/Ch12_Regression_01.rst:285
msgid ""
"The Pearson correlation coefficient is useful for a lot of things, but it "
"does have shortcomings. One issue in particular stands out: what it actually "
"measures is the strength of the *linear* relationship between two variables. "
"In other words, what it gives you is a measure of the extent to which the "
"data all tend to fall on a single, perfectly straight line. Often, this is a "
"pretty good approximation to what we mean when we say “relationship”, and so "
"the Pearson correlation is a good thing to calculate. Sometimes though, it "
"isn’t."
msgstr ""
"Pearsons korrelasjonskoeffisient er nyttig for mange ting, men den har også "
"sine svakheter. Det er særlig én ting som skiller seg ut: Den måler egentlig "
"styrken på det *lineære* forholdet mellom to variabler. Med andre ord gir "
"den deg et mål på i hvilken grad dataene har en tendens til å falle på en "
"enkelt, perfekt rett linje. Ofte er dette en ganske god tilnærming til det "
"vi mener når vi sier «sammenheng», og derfor er Pearson-korrelasjonen en god "
"ting å beregne. Noen ganger er den imidlertid ikke det."

#: ../../Ch12/Ch12_Regression_01.rst:294
msgid ""
"One very common situation where the Pearson correlation isn’t quite the "
"right thing to use arises when an increase in one variable *X* really is "
"reflected in an increase in another variable *Y*, but the nature of the "
"relationship isn’t necessarily linear. An example of this might be the "
"relationship between effort and reward when studying for an exam. If you put "
"zero effort (*X*) into learning a subject then you should expect a grade of "
"0\\% (*Y*). However, a little bit of effort will cause a *massive* "
"improvement. Just turning up to lectures means that you learn a fair bit, "
"and if you just turn up to classes and scribble a few things down your grade "
"might rise to 35\\%, all without a lot of effort. However, you just don’t "
"get the same effect at the other end of the scale. As everyone knows, it "
"takes *a lot* more effort to get a grade of 90\\% than it takes to get a "
"grade of 55\\%. What this means is that, if I’ve got data looking at study "
"effort and grades, there’s a pretty good chance that Pearson correlations "
"will be misleading."
msgstr ""
"En svært vanlig situasjon der Pearson-korrelasjonen ikke er helt riktig å "
"bruke, oppstår når en økning i en variabel *X* virkelig gjenspeiles i en "
"økning i en annen variabel *Y*, men forholdet ikke nødvendigvis er lineært. "
"Et eksempel på dette kan være forholdet mellom innsats og belønning når du "
"leser til en eksamen. Hvis du legger ned null innsats (*X*) i å lære deg et "
"fag, kan du forvente en karakter på 0\\% (*Y*). En liten innsats vil "
"imidlertid føre til en *massiv* forbedring. Bare det å møte opp til "
"forelesninger betyr at du lærer en god del, og hvis du bare møter opp til "
"timene og skriver ned noen få ting, kan karakteren din stige til 35\\%, uten "
"at du trenger å anstrenge deg særlig mye. Men du får ikke den samme effekten "
"i den andre enden av skalaen. Som alle vet, kreves det *mye* mer innsats for "
"å få en karakter på 90\\% enn det kreves for å få en karakter på 55\\%. "
"Dette betyr at hvis jeg har data som ser på studieinnsats og karakterer, er "
"det en ganske stor sjanse for at Pearson-korrelasjoner vil være misvisende."

#: ../../Ch12/Ch12_Regression_01.rst:311
msgid ""
"To illustrate, consider the data plotted in :numref:`fig-"
"ordinalRelationship`, showing the relationship between hours worked and "
"grade received for 10 students taking some class. The curious thing about "
"this (highly fictitious) data set is that increasing your effort *always* "
"increases your grade. It might be by a lot or it might be by a little, but "
"increasing effort will never decrease your grade. If we run a standard "
"Pearson correlation, it shows a strong relationship between hours worked and "
"grade received, with a correlation coefficient of **0.91**. However, this "
"doesn’t actually capture the observation that increasing hours worked "
"*always* increases the grade. There’s a sense here in which we want to be "
"able to say that the correlation is *perfect* but for a somewhat different "
"notion of what a “relationship” is. What we’re looking for is something that "
"captures the fact that there is a perfect **ordinal relationship** here. "
"That is, if student 1 works more hours than student 2, then we can guarantee "
"that student 1 will get the better grade. That’s not what a correlation of "
"*r* = 0.91 says at all."
msgstr ""
"For å illustrere dette kan vi se på dataene i :numref:`fig-"
"ordinalRelationship`, som viser forholdet mellom arbeidstimer og karakterer "
"for 10 studenter som tar et fag. Det merkelige med dette (høyst fiktive) "
"datasettet er at økt innsats *alltid* øker karakteren din. Det kan være mye "
"eller lite, men økt innsats vil aldri redusere karakteren din. Hvis vi "
"kjører en standard Pearson-korrelasjon, viser den en sterk sammenheng mellom "
"arbeidstimer og karakter, med en korrelasjonskoeffisient på **0,91**. Dette "
"fanger imidlertid ikke opp observasjonen om at økt arbeidstid *alltid* øker "
"karakteren. Her ønsker vi på sett og vis å kunne si at sammenhengen er "
"*perfekt*, men vi har en litt annen oppfatning av hva en «sammenheng» er. "
"Det vi er ute etter, er noe som fanger opp det faktum at det er en perfekt "
"**ordinal sammenheng** her. Det vil si at hvis student 1 jobber flere timer "
"enn student 2, så kan vi garantere at student 1 vil få bedre karakter. Det "
"er ikke det en korrelasjon på *r* = 0,91 sier i det hele tatt."

#: ../../Ch12/Ch12_Regression_01.rst:330
msgid "relationship between hours worked and grade received"
msgstr "forholdet mellom arbeidstimer og mottatt karakter"

#: ../../Ch12/Ch12_Regression_01.rst:334
msgid ""
"The relationship between hours worked and grade received for a toy data set "
"consisting of only 10 students (each circle corresponds to one student). The "
"dashed line through the middle shows the linear relationship between the two "
"variables. This produces a strong Pearson correlation of *r* = 0.91. "
"However, the interesting thing to note here is that there’s actually a "
"perfect monotonic relationship between the two variables. In this toy "
"example, increasing the hours worked always increases the grade received, as "
"illustrated by the solid line. This is reflected in a Spearman correlation "
"of ρ = 1.00. With such a small data set, however, it’s an open question as "
"to which version better describes the actual relationship involved."
msgstr ""
"Sammenhengen mellom arbeidstimer og karakterer for et leketøysdatasett "
"bestående av kun 10 studenter (hver sirkel tilsvarer én student). Den "
"stiplede linjen i midten viser det lineære forholdet mellom de to "
"variablene. Dette gir en sterk Pearson-korrelasjon på *r* = 0,91. Det "
"interessante her er imidlertid at det faktisk er et perfekt monotont forhold "
"mellom de to variablene. I dette lekeeksempelet øker alltid karakteren ved å "
"øke antall arbeidstimer, som illustrert av den heltrukne linjen. Dette "
"gjenspeiles i en Spearman-korrelasjon på ρ = 1,00. Med et så lite datasett "
"er det imidlertid et åpent spørsmål hvilken versjon som best beskriver det "
"faktiske forholdet."

#: ../../Ch12/Ch12_Regression_01.rst:348
msgid ""
"How should we address this? Actually, it’s really easy. If we’re looking for "
"ordinal relationships all we have to do is treat the data as if it were "
"ordinal scale |ordinal|! So, instead of measuring effort in terms of “hours "
"worked”, lets rank all 10 of our students in order of hours worked. That is, "
"student 1 did the least work out of anyone (2 hours) so they get the lowest "
"rank (rank = 1). Student 4 was the next laziest, putting in only 6 hours of "
"work over the whole semester, so they get the next lowest rank (rank = 2). "
"Notice that I’m using “rank = 1” to mean “low rank”. Sometimes in everyday "
"language we talk about “rank = 1” to mean “top rank” rather than “bottom "
"rank”. So be careful, you can rank “from smallest value to largest value” (i."
"e., small equals rank 1) or you can rank “from largest value to smallest "
"value” (i.e., large equals rank 1). In this case, I’m ranking from smallest "
"to largest, but as it’s really easy to forget which way you set things up "
"you have to put a bit of effort into remembering!"
msgstr ""
"Hvordan skal vi løse dette? Det er faktisk veldig enkelt. Hvis vi leter "
"etter ordinale sammenhenger, er alt vi trenger å gjøre å behandle dataene "
"som om de er ordinalskalerte |ordinal|! Så i stedet for å måle innsatsen i "
"form av «arbeidstimer», kan vi rangere alle de ti studentene i rekkefølge "
"etter antall arbeidstimer. Det vil si at elev 1 gjorde minst arbeid av alle ("
"2 timer), så vedkommende får den laveste rangeringen (rang = 1). Student 4 "
"var den nest lateste, med bare 6 timers arbeid i løpet av hele semesteret, "
"og får dermed den nest laveste rangeringen (rang = 2). Legg merke til at jeg "
"bruker «rang = 1» i betydningen «lav rang». Noen ganger snakker vi i "
"dagligtalen om «rang = 1» for å bety «topprangering» i stedet for "
"«bunnrangering». Så vær forsiktig, du kan rangere «fra minste verdi til "
"største verdi» (dvs. liten er lik rang 1), eller du kan rangere «fra største "
"verdi til minste verdi» (dvs. stor er lik rang 1). I dette tilfellet "
"rangerer jeg fra minst til størst, men siden det er veldig lett å glemme "
"hvilken måte man setter opp ting på, må man anstrenge seg litt for å huske "
"det!"

#: ../../Ch12/Ch12_Regression_01.rst:426
msgid "ordinal"
msgstr "ordinal"

#: ../../Ch12/Ch12_Regression_01.rst:363
msgid ""
"Okay, so let’s have a look at our students when we rank them from worst to "
"best in terms of effort and reward:"
msgstr ""
"La oss ta en titt på elevene våre når vi rangerer dem fra dårligst til best "
"når det gjelder innsats og belønning:"

#: ../../Ch12/Ch12_Regression_01.rst:367
msgid "rank (hours worked)"
msgstr "rang (antall arbeidstimer)"

#: ../../Ch12/Ch12_Regression_01.rst:367
msgid "rank (grade received)"
msgstr "rang (mottatt karakter)"

#: ../../Ch12/Ch12_Regression_01.rst:369
msgid "**student 1**"
msgstr "**student 1**"

#: ../../Ch12/Ch12_Regression_01.rst:369
msgid "1"
msgstr "1"

#: ../../Ch12/Ch12_Regression_01.rst:371
msgid "**student 2**"
msgstr "**student 2**"

#: ../../Ch12/Ch12_Regression_01.rst:371
msgid "10"
msgstr "10"

#: ../../Ch12/Ch12_Regression_01.rst:373
msgid "**student 3**"
msgstr "**student 3**"

#: ../../Ch12/Ch12_Regression_01.rst:373
msgid "6"
msgstr "6"

#: ../../Ch12/Ch12_Regression_01.rst:375
msgid "**student 4**"
msgstr "**student 4**"

#: ../../Ch12/Ch12_Regression_01.rst:375
msgid "2"
msgstr "2"

#: ../../Ch12/Ch12_Regression_01.rst:377
msgid "**student 5**"
msgstr "**student 5**"

#: ../../Ch12/Ch12_Regression_01.rst:377
msgid "3"
msgstr "3"

#: ../../Ch12/Ch12_Regression_01.rst:379
msgid "**student 6**"
msgstr "**student 6**"

#: ../../Ch12/Ch12_Regression_01.rst:379
msgid "5"
msgstr "5"

#: ../../Ch12/Ch12_Regression_01.rst:381
msgid "**student 7**"
msgstr "**student 7**"

#: ../../Ch12/Ch12_Regression_01.rst:381
msgid "4"
msgstr "4"

#: ../../Ch12/Ch12_Regression_01.rst:383
msgid "**student 8**"
msgstr "**student 8**"

#: ../../Ch12/Ch12_Regression_01.rst:383
msgid "8"
msgstr "8"

#: ../../Ch12/Ch12_Regression_01.rst:385
msgid "**student 9**"
msgstr "**student 9**"

#: ../../Ch12/Ch12_Regression_01.rst:385
msgid "7"
msgstr "7"

#: ../../Ch12/Ch12_Regression_01.rst:387
msgid "**student 10**"
msgstr "**student 10**"

#: ../../Ch12/Ch12_Regression_01.rst:387
msgid "9"
msgstr "9"

#: ../../Ch12/Ch12_Regression_01.rst:390
msgid ""
"Hmm. These are *identical*. The student who put in the most effort got the "
"best grade, the student with the least effort got the worst grade, etc. As "
"the table above shows, these two rankings are identical, so if we now "
"correlate them we get a perfect relationship, with a correlation of **1.0**."
msgstr ""
"De er identiske. Disse er *identiske*. Den studenten som anstrengte seg "
"mest, fikk den beste karakteren, den studenten som anstrengte seg minst, "
"fikk den dårligste karakteren osv. Som tabellen over viser, er disse to "
"rangeringene identiske, så hvis vi nå korrelerer dem, får vi et perfekt "
"forhold, med en korrelasjon på **1,0**."

#: ../../Ch12/Ch12_Regression_01.rst:396
msgid ""
"What we’ve just re-invented is **Spearman’s rank order correlation**, "
"usually denoted ρ to distinguish it from the Pearson correlation *r*. We can "
"calculate Spearman’s ρ using jamovi simply by clicking the ``Spearman`` "
"check box in the ``Correlation Matrix`` options panel."
msgstr ""
"Det vi nettopp har funnet opp på nytt, er **Spearmans rangkorrelasjon**, "
"vanligvis betegnet ρ for å skille den fra Pearson-korrelasjonen *r*. Vi kan "
"beregne Spearmans ρ ved hjelp av jamovi ved å klikke på avkrysningsboksen "
"``Spearman`` i analysepanelet til ``Correlation Matrix``."

#: ../../Ch12/Ch12_Regression_01.rst:404
msgid ""
"Actually, even that table is more than I’d bother with. In practice, most "
"people pick *one* measure of central tendency, and *one* measure of "
"variability only."
msgstr ""
"Selv den tabellen er faktisk mer enn jeg ville brydd meg med. I praksis "
"velger de fleste *ett* mål for sentraltendens og *ett* mål for variabilitet."

#: ../../Ch12/Ch12_Regression_01.rst:409
msgid ""
"Just like we saw with the variance and the standard deviation, in practice "
"we divide by *N* - 1 rather than *N*."
msgstr ""
"Akkurat som vi så med variansen og standardavviket, dividerer vi i praksis "
"med *N* - 1 i stedet for *N*."

#: ../../Ch12/Ch12_Regression_01.rst:413
msgid "This is an oversimplification, but it’ll do for our purposes."
msgstr "Dette er en forenkling, men det holder for vårt formål."

#: ../../Ch12/Ch12_Regression_02.rst:4
msgid "Scatterplots"
msgstr "Spredningsdiagrammer"

#: ../../Ch12/Ch12_Regression_02.rst:6
msgid ""
"**Scatterplots** are a simple but effective tool for visualising the "
"relationship between *two* variables, like we saw with the figures in the "
"section on correlation (section :doc:`Ch12_Regression_01`). It’s this latter "
"application that we usually have in mind when we use the term “scatterplot”. "
"In this kind of plot each observation corresponds to one dot. The horizontal "
"location of the dot plots the value of the observation on one variable, and "
"the vertical location displays its value on the other variable. In many "
"situations you don’t really have a clear opinions about what the *causal* "
"relationship is (e.g., does A cause B, or does B cause A, or does some other "
"variable C control both A and B). If that’s the case, it doesn’t really "
"matter which variable you plot on the x-axis and which one you plot on the y-"
"axis. However, in many situations you do have a pretty strong idea which "
"variable you think is most likely to be causal, or at least you have some "
"suspicions in that direction. If so, then it’s conventional to plot the "
"cause variable on the x-axis, and the effect variable on the y-axis. With "
"that in mind, let’s look at how to draw scatterplots in jamovi, using the "
"same |parenthood|_ data set that I used when introducing correlations."
msgstr ""
"**Spredningsdiagrammer** (*scatterplots*) er et enkelt, men effektivt "
"verktøy for å visualisere forholdet mellom *to* variabler, slik vi så med "
"figurene i avsnittet om korrelasjon (avsnitt :doc:`Ch12_Regression_01`). Det "
"er denne sistnevnte anvendelsen vi vanligvis tenker på når vi bruker "
"begrepet «scatterplot». I denne typen plott tilsvarer hver observasjon en "
"prikk. Den horisontale plasseringen av prikken viser verdien av "
"observasjonen på én variabel, og den vertikale plasseringen viser verdien på "
"den andre variabelen. I mange situasjoner har du ingen klar formening om hva "
"som er den *kausale* sammenhengen (f.eks. om A forårsaker B, eller om B "
"forårsaker A, eller om en annen variabel C styrer både A og B). Hvis det er "
"tilfellet, spiller det ingen rolle hvilken variabel du plotter på x-aksen og "
"hvilken du plotter på y-aksen. I mange situasjoner har du imidlertid en "
"ganske sterk idé om hvilken variabel du tror det er mest sannsynlig at er "
"årsakssammenhengen, eller i det minste har du noen mistanker i den "
"retningen. I så fall er det vanlig å plotte årsaksvariabelen på x-aksen og "
"effektvariabelen på y-aksen. Med det i bakhodet skal vi se på hvordan man "
"tegner spredningsdiagrammer i jamovi, ved å bruke det samme datasettet |"
"parenthood|_ som jeg brukte da jeg introduserte korrelasjoner."

#: ../../Ch12/Ch12_Regression_02.rst:24
msgid ""
"Suppose my goal is to draw a scatterplot displaying the relationship between "
"the amount of sleep that I get (``dani.sleep``) and how grumpy I am the next "
"day (``dani.grump``). There are two different ways in which we can use "
"jamovi to get the plot that we’re after. The first way is to use the "
"``Plot`` option under the ``Regression`` → ``Correlation Matrix`` button, "
"giving us the output shown in :numref:`fig-scatterplot1`. Note that jamovi "
"draws a line through the points, we’ll come onto this a bit later in "
"section :doc:`Ch12_Regression_03`. Plotting a scatterplot in this way also "
"allow you to specify ``Densities for variables`` and this option adds a "
"density curve showing how the data in each variable is distributed."
msgstr ""
"Anta at målet mitt er å tegne et spredningsdiagram som viser forholdet "
"mellom hvor mye søvn jeg får (``dani.sleep``) og hvor gretten jeg er neste "
"dag (``dani.grump``). Vi kan bruke jamovi på to forskjellige måter for å få "
"det plottet vi er ute etter. Den første måten er å bruke alternativet "
"``Plot`` under knappen ``Regression`` → ``Correlation Matrix``, noe som gir "
"oss resultatet som vist i :numref:`fig-scatterplot1`. Legg merke til at "
"jamovi trekker en linje gjennom punktene, vi kommer tilbake til dette litt "
"senere i avsnitt :doc:`Ch12_Regression_03`. Når du plotter et "
"spredningsdiagram på denne måten, kan du også spesifisere ``Densities for "
"variables``, og dette alternativet legger til en tetthetskurve som viser "
"hvordan dataene i hver variabel er fordelt."

#: ../../Ch12/Ch12_Regression_02.rst:37 ../../Ch12/Ch12_Regression_02.rst:41
msgid "Scatterplot created with the ``Correlation Matrix`` analysis in jamovi"
msgstr "Spredningsdiagram laget med ``Correlation Matrix``-analysen i jamovi"

#: ../../Ch12/Ch12_Regression_02.rst:45
msgid ""
"The second way do to it is to use one of the jamovi add-on modules. This "
"module is called ``scatr`` and you can install it by clicking on the large "
"``+`` icon in the top right of the jamovi screen, opening the jamovi "
"library, scrolling down until you find ``scatr`` and clicking ``Install``. "
"When you have done this, you will find a new ``Scatterplot`` command "
"available under the ``Exploration`` button. This plot is a bit different "
"than the first way, see :numref:`fig-scatterplot2`, but the important "
"information is the same."
msgstr ""
"Den andre måten å gjøre det på er å bruke en av jamovi-tilleggsmodulene. "
"Denne modulen heter ``scatr``, og du kan installere den ved å klikke på det "
"store ``+``-ikonet øverst til høyre på jamovi-skjermen, åpne jamovi-"
"biblioteket, bla nedover til du finner ``scatr`` og klikke på ``Install``. "
"Når du har gjort dette, vil du finne en ny ``Scatterplot``-kommando "
"tilgjengelig under ``Exploration``-knappen. Dette plottet er litt annerledes "
"enn den første måten, se :numref:`fig-scatterplot2`, men den viktige "
"informasjonen er den samme."

#: ../../Ch12/Ch12_Regression_02.rst:56 ../../Ch12/Ch12_Regression_02.rst:60
msgid "Scatterplot cretaed with the ``scatr`` add-on module in jamovi"
msgstr "Scatterplot laget med tilleggsmodulen ``scatr`` i jamovi"

#: ../../Ch12/Ch12_Regression_02.rst:65
msgid "More elaborate options"
msgstr "Mer forseggjorte alternativer"

#: ../../Ch12/Ch12_Regression_02.rst:67
msgid ""
"Often you will want to look at the relationships between several variables "
"at once, using a **scatterplot matrix** (in jamovi via the ``Correlation "
"Matrix`` - ``Plot`` command). Just add another variable, for example ``baby."
"sleep`` to the list of variables to be correlated, and jamovi will create a "
"scatterplot matrix for you, just like the one in :numref:`fig-scatterplot3`."
msgstr ""
"Ofte vil du se på forholdet mellom flere variabler samtidig, ved hjelp av en "
"**spredningsmatrise** (i jamovi via kommandoen ``Correlation Matrix`` - "
"``Plot``). Bare legg til en ny variabel, for eksempel ``baby.sleep``, i "
"listen over variabler som skal korreleres, så lager jamovi en scatterplot-"
"matrise for deg, akkurat som den i :numref:`fig-scatterplot3`."

#: ../../Ch12/Ch12_Regression_02.rst:76
msgid "Matrix of scatterplots cretaed with the ``Correlation Matrix`` analysis"
msgstr ""
"Matrise av spredningsdiagrammer laget med ``Correlation Matrix``-analysen"

#: ../../Ch12/Ch12_Regression_02.rst:80
msgid ""
"Matrix of scatterplots cretaed with the ``Correlation Matrix`` analysis in "
"jamovi."
msgstr ""
"Matrise av spredningsdiagrammer laget med ``Correlation Matrix``-analyse i "
"jamovi."

#: ../../Ch12/Ch12_Regression_03.rst:4
msgid "What is a linear regression model?"
msgstr "Hva er en lineær regresjonsmodell?"

#: ../../Ch12/Ch12_Regression_03.rst:6
msgid ""
"Stripped to its bare essentials, linear regression models are basically a "
"slightly fancier version of the Pearson correlation (section :doc:"
"`Ch12_Regression_01`), though as we’ll see regression models are much more "
"powerful tools."
msgstr ""
"Lineære regresjonsmodeller er i bunn og grunn en litt mer avansert versjon "
"av Pearson-korrelasjonen (se :doc:`Ch12_Regression_01`), men som vi skal se, "
"er regresjonsmodeller et mye mer kraftfullt verktøy."

#: ../../Ch12/Ch12_Regression_03.rst:25
msgid ""
"Since the basic ideas in regression are closely tied to correlation, we’ll "
"return to the |parenthood|_ data set that we were using to illustrate how "
"correlations work. Recall that, in this data set we were trying to find out "
"why Dani is so very grumpy all the time and our working hypothesis was that "
"I’m not getting enough sleep. We drew some scatterplots to help us examine "
"the relationship between the amount of sleep I get and my grumpiness the "
"following day, as in :numref:`fig-scatterplot2`, and as we saw previously "
"this corresponds to a correlation of *r* = -0.90, but what we find ourselves "
"secretly imagining is something that looks closer to :numref:`fig-"
"regression1` (left panel). That is, we mentally draw a straight line through "
"the middle of the data. In statistics, this line that we’re drawing is "
"called a **regression line**. Notice that, since we’re not idiots, the "
"regression line goes through the middle of the data. We don’t find ourselves "
"imagining anything like the rather silly plot shown in :numref:`fig-"
"regression1` (right panel)."
msgstr ""
"Siden de grunnleggende ideene i regresjon er nært knyttet til korrelasjon, "
"skal vi gå tilbake til datasettet |parenthood|_ som vi brukte for å "
"illustrere hvordan korrelasjoner fungerer. Husk at vi i dette datasettet "
"prøvde å finne ut hvorfor Dani er så gretten hele tiden, og at "
"arbeidshypotesen vår var at jeg ikke får nok søvn. Vi tegnet noen "
"spredningsdiagrammer for å hjelpe oss med å undersøke forholdet mellom hvor "
"mye søvn jeg får og hvor gretten jeg er dagen etter, som i :numref:`fig-"
"scatterplot2`, og som vi så tidligere, tilsvarer dette en korrelasjon på *r* "
"= -0,90, men det vi i hemmelighet ser for oss, er noe som ligner mer på :"
"numref:`fig-regression1` (venstre panel). Det vil si at vi mentalt tegner en "
"rett linje gjennom midten av dataene. I statistikk kalles denne linjen vi "
"tegner for en **regresjonslinje**. Legg merke til at siden vi ikke er "
"idioter, går regresjonslinjen gjennom midten av dataene. Vi ser ikke for oss "
"noe lignende det ganske tåpelige plottet som vises i :numref:`fig-"
"regression1` (høyre panel)."

#: ../../Ch12/Ch12_Regression_03.rst:29
msgid "Best and poor choice of regression line"
msgstr "Beste og dårligste valg av regresjonslinje"

#: ../../Ch12/Ch12_Regression_03.rst:33
msgid ""
"The left panel shows the scatterplot of ``dani.sleep`` and ``dani.grump`` "
"from :numref:`fig-scatterplot2` with the best fitting regression line drawn "
"over the top. Not surprisingly, the line goes through the middle of the "
"data. In contrast, the right panel shows the same data, but with a very poor "
"choice of regression line drawn over the top."
msgstr ""
"Det venstre panelet viser spredningsdiagrammet for ``dani.sleep`` og ``dani."
"grump`` fra :numref:`fig-scatterplot2` med den best tilpassede "
"regresjonslinjen tegnet over toppen. Ikke overraskende går linjen gjennom "
"midten av dataene. Det høyre panelet viser derimot de samme dataene, men med "
"et svært dårlig valg av regresjonslinje tegnet over toppen."

#: ../../Ch12/Ch12_Regression_03.rst:41
msgid ""
"This is not highly surprising. The line that I’ve drawn in :numref:`fig-"
"regression1` (right panel) doesn’t “fit” the data very well, so it doesn’t "
"make a lot of sense to propose it as a way of summarising the data, right? "
"This is a very simple observation to make, but it turns out to be very "
"powerful when we start trying to wrap just a little bit of maths around it. "
"To do so, let’s start with a refresher of some high school maths. The "
"formula for a straight line is usually written like this:"
msgstr ""
"Dette er ikke veldig overraskende. Linjen jeg har tegnet i :numref:`fig-"
"regression1` (høyre panel) «passer» ikke særlig godt til dataene, så det gir "
"ikke så mye mening å foreslå den som en måte å oppsummere dataene på, ikke "
"sant? Dette er en veldig enkel observasjon, men den viser seg å være veldig "
"kraftfull når vi begynner å prøve å pakke den inn i litt matematikk. La oss "
"starte med en oppfriskning av litt matematikk fra videregående skole. "
"Formelen for en rett linje skrives vanligvis slik:"

#: ../../Ch12/Ch12_Regression_03.rst:49
msgid "*y* = *a* + *bx*"
msgstr "*y* = *a* + *bx*"

#: ../../Ch12/Ch12_Regression_03.rst:51
msgid ""
"Or, at least, that’s what it was when I went to high school all those years "
"ago. The two *variables* are *x* and *y*, and we have two *coefficients*, "
"*a* and *b*\\.\\ [#]_ The coefficient *a* represents the **y-intercept** of "
"the line, and coefficient *b* represents the *slope* of the line. Digging "
"further back into our decaying memories of high school (sorry, for some of "
"us high school was a long time ago), we remember that the intercept is "
"interpreted as “the value of *y* that you get when *x* = 0”. Similarly, a "
"slope of *b* means that if you increase the *x*-value by 1 unit, then the "
"*y*-value goes up by *b* units, and a negative slope means that the *y*-"
"value would go down rather than up. Ah yes, it’s all coming back to me now. "
"Now that we’ve remembered that it should come as no surprise to discover "
"that we use the exact same formula for a regression line. If *Y* is the "
"outcome variable (the DV) and *X* is the predictor variable (the IV), then "
"the formula that describes our regression is written like this:"
msgstr ""
"Det var i hvert fall slik det var da jeg gikk på videregående for mange år "
"siden. De to *variablene* er *x* og *y*, og vi har to *koeffisienter*, *a* "
"og *b*\\.\\ [#]_ Koeffisienten *a* representerer linjens **y-"
"skjæringspunkt** (*intercept*), og koeffisienten *b* representerer linjens "
"*stigning*. Når vi graver oss lenger tilbake i våre forvitrende minner fra "
"videregående skole (beklager, for noen av oss er det lenge siden "
"videregående skole), husker vi at skjæringspunktet tolkes som «verdien av "
"*y* som du får når *x* = 0». På samme måte betyr en stigning på *b* at hvis "
"du øker *x*-verdien med 1 enhet, så øker *y*-verdien med *b* enheter, og en "
"negativ stigning betyr at *y*-verdien går ned i stedet for opp. Å ja, nå "
"husker jeg alt sammen. Nå som vi har husket det, bør det ikke komme som noen "
"overraskelse at vi bruker nøyaktig samme formel for en regresjonslinje. Hvis "
"*Y* er utfallsvariabelen (DV) og *X* er prediktorvariabelen (IV), er "
"formelen som beskriver regresjonen vår, skrevet slik:"

#: ../../Ch12/Ch12_Regression_03.rst:66
msgid "*Ŷ*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i`"
msgstr "*Ŷ*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_03.rst:68
msgid ""
"Hmm. Looks like the same formula, but there’s some extra frilly bits in this "
"version. Let’s make sure we understand them. Firstly, notice that I’ve "
"written *X*\\ :sub:`i` and *Y*\\ :sub:`i` rather than just plain old *X* and "
"*Y*. This is because we want to remember that we’re dealing with actual "
"data. In this equation, *X*\\ :sub:`i` is the value of predictor variable "
"for the i\\ th observation (i.e., the number of hours of sleep that I got on "
"day i of my little study), and *Y*\\ :sub:`i` is the corresponding value of "
"the outcome variable (i.e., my grumpiness on that day). And although I "
"haven’t said so explicitly in the equation, what we’re assuming is that this "
"formula works for all observations in the data set (i.e., for all i). "
"Secondly, notice that I wrote *Ŷ*\\ :sub:`i` and not *Y*\\ :sub:`i`. This is "
"because we want to make the distinction between the *actual data* *Y*\\ :sub:"
"`i`, and the *estimate* *Ŷ*\\ :sub:`i` (i.e., the prediction that our "
"regression line is making). Thirdly, I changed the letters used to describe "
"the coefficients from *a* and *b* to *b*\\ :sub:`0` and *b*\\ :sub:`1`. "
"That’s just the way that statisticians like to refer to the coefficients in "
"a regression model. I’ve no idea why they chose *b*, but that’s what they "
"did. In any case *b*\\ :sub:`0` always refers to the intercept term, and "
"*b*\\ :sub:`1` refers to the slope."
msgstr ""
"Det ser ut som samme oppskrift. Det ser ut som samme formel, men det er noen "
"ekstra detaljer i denne versjonen. La oss sørge for at vi forstår dem. For "
"det første, legg merke til at jeg har skrevet *X*\\ :sub:`i` og *Y*\\ :sub:"
"`i` i stedet for bare *X* og *Y*. Dette er fordi vi vil huske at vi har med "
"faktiske data å gjøre. I denne ligningen er *X**\\ :sub:`i` verdien av "
"prediktorvariabelen for den i-te observasjonen (dvs. antall timer søvn jeg "
"fikk på dag i av min lille studie), og *Y**\\ :sub:`i` er den tilsvarende "
"verdien av utfallsvariabelen (dvs. hvor gretten jeg var den dagen). Og selv "
"om jeg ikke har sagt det eksplisitt i ligningen, antar vi at denne formelen "
"fungerer for alle observasjonene i datasettet (dvs. for alle i). For det "
"andre, legg merke til at jeg skrev *Ŷ*\\ :sub:`i` og ikke *Y*\\ :sub:`i`. "
"Dette er fordi vi ønsker å skille mellom de *faktiske dataene* *Y*\\ :sub:"
"`i`, og *estimatet* *Ŷ*\\ :sub:`i` (dvs. prediksjonen som regresjonslinjen "
"vår gjør). For det tredje endret jeg bokstavene som brukes til å beskrive "
"koeffisientene fra *a* og *b* til *b*\\ :sub:`0` og *b*\\ :sub:`1`. Det er "
"bare slik statistikere liker å referere til koeffisientene i en "
"regresjonsmodell. Jeg aner ikke hvorfor de valgte *b*, men det var det de "
"gjorde. Uansett refererer *b*\\ :sub:`0` alltid til termen for intercept, og "
"*b*\\ :sub:`1` refererer til stigningen."

#: ../../Ch12/Ch12_Regression_03.rst:89
msgid ""
"Excellent, excellent. Next, I can’t help but notice that, regardless of "
"whether we’re talking about the good regression line or the bad one, the "
"data don’t fall perfectly on the line. Or, to say it another way, the data "
"*Y*\\ :sub:`i` are not identical to the predictions of the regression model "
"*Ŷ*\\ :sub:`i`. Since statisticians love to attach letters, names and "
"numbers to everything, let’s refer to the difference between the model "
"prediction and that actual data point as a *residual*, and we’ll refer to it "
"as ε\\ :sub:`i`.\\ [#]_ Written using mathematics, the residuals are defined "
"as"
msgstr ""
"Utmerket, utmerket. Deretter kan jeg ikke unngå å legge merke til at dataene "
"ikke faller perfekt på linjen, uansett om vi snakker om den gode eller den "
"dårlige regresjonslinjen. Eller, for å si det på en annen måte, dataene "
"*Y*\\ :sub:`i` er ikke identiske med prediksjonene til regresjonsmodellen "
"*Ŷ*\\ :sub:`i`. Siden statistikere elsker å knytte bokstaver, navn og tall "
"til alt, kan vi kalle forskjellen mellom modellens prediksjon og det "
"faktiske datapunktet for et *residuum*, og vi kaller den ε\\ :sub:`i`.\\ "
"[#]_ Skrevet med matematikk defineres residuene som"

#: ../../Ch12/Ch12_Regression_03.rst:99 ../../Ch12/Ch12_Regression_11.rst:60
msgid "ε\\ :sub:`i` = *Y*\\ :sub:`i` - *Ŷ*\\ :sub:`i`"
msgstr "ε\\ :sub:`i` = *Y*\\ :sub:`i` - *Ŷ*\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_03.rst:101
msgid ""
"which in turn means that we can write down the complete linear regression "
"model as"
msgstr ""
"som igjen betyr at vi kan skrive ned den komplette lineære "
"regresjonsmodellen som"

#: ../../Ch12/Ch12_Regression_03.rst:104
msgid ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i` + ε\\ :sub:"
"`i`"
msgstr ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i` + ε\\ "
":sub:`i`"

#: ../../Ch12/Ch12_Regression_03.rst:109
msgid ""
"Also sometimes written as *y* = mx + c where m is the slope coefficient and "
"c is the intercept (constant) coefficient."
msgstr ""
"Skrives også noen ganger som *y* = mx + c, der m er stigningskoeffisienten "
"og c er skjæringspunktet (konstant; *intercept*)."

#: ../../Ch12/Ch12_Regression_03.rst:113
msgid ""
"The ε symbol is the Greek letter epsilon. It’s traditional to use ε\\ :sub:"
"`i` or e\\ :sub:`i` to denote a residual."
msgstr ""
"Symbolet ε er den greske bokstaven epsilon. Det er tradisjon for å bruke "
"ε\\ :sub:`i` eller e\\ :sub:`i` for å betegne et residuum."

#: ../../Ch12/Ch12_Regression_04.rst:4
msgid "Estimating a linear regression model"
msgstr "Estimering av en lineær regresjonsmodell"

#: ../../Ch12/Ch12_Regression_04.rst:8
msgid "Residuals associated with the best and with a poor regression line"
msgstr "Residualer forbundet med den beste og den dårligste regresjonslinjen"

#: ../../Ch12/Ch12_Regression_04.rst:12
msgid ""
"Depiction of the residuals associated with the best fitting regression line "
"(left panel), and the residuals associated with a poor regression line "
"(right panel). The residuals are much smaller for the good regression line. "
"Again, this is no surprise given that the good line is the one that goes "
"right through the middle of the data."
msgstr ""
"Illustrasjon av residuene forbundet med den best tilpassede regresjonslinjen "
"(venstre panel) og residuene forbundet med en dårlig regresjonslinje (høyre "
"panel). Residualene er mye mindre for den gode regresjonslinjen. Igjen er "
"dette ingen overraskelse, siden den gode linjen er den som går midt gjennom "
"dataene."

#: ../../Ch12/Ch12_Regression_04.rst:20
msgid ""
"Okay, now let’s redraw our pictures but this time I’ll add some lines to "
"show the size of the residual for all observations. When the regression line "
"is good, our residuals (the lengths of the solid black lines) all look "
"pretty small, as shown in :numref:`fig-regression3` (left panel), but when "
"the regression line is a bad one the residuals are a lot larger, as you can "
"see from looking at :numref:`fig-regression3` (right panel). Hmm. Maybe what "
"we “want” in a regression model is *small* residuals. Yes, that does seem to "
"make sense. In fact, I think I’ll go so far as to say that the “best "
"fitting” regression line is the one that has the smallest residuals. Or, "
"better yet, since statisticians seem to like to take squares of everything "
"why not say that: The estimated regression coefficients, :math:`\\hat{b}_0` "
"and :math:`\\hat{b}_1`, are those that minimise the sum of the squared "
"residuals, which we could either write as"
msgstr ""
"Ok, la oss nå tegne bildene våre på nytt, men denne gangen legger jeg til "
"noen linjer for å vise størrelsen på residuene for alle observasjonene. Når "
"regresjonslinjen er god, ser residuene våre (lengden på de heltrukne svarte "
"linjene) ganske små ut, som vist i :numref:`fig-regression3` (venstre "
"panel), men når regresjonslinjen er dårlig, er residuene mye større, som du "
"kan se ved å se på :numref:`fig-regression3` (høyre panel). Hmm. Kanskje det "
"vi «vil ha» i en regresjonsmodell er *små* residuer. Ja, det ser ut til å gi "
"mening. Jeg tror faktisk jeg vil gå så langt som å si at den «best "
"tilpassede» regresjonslinjen er den som har de minste residuene. Eller, enda "
"bedre, siden statistikere ser ut til å like å ta kvadrater av alt, hvorfor "
"ikke si det: De estimerte regresjonskoeffisientene, :math:`\\hat{b}_0` og :"
"math:`\\hat{b}_1`, er de som minimerer summen av de kvadrerte residuene, som "
"vi enten kan skrive som"

#: ../../Ch12/Ch12_Regression_04.rst:34
msgid ""
"\\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"
msgstr ""
"\\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_04.rst:36
msgid "or as"
msgstr "eller som"

#: ../../Ch12/Ch12_Regression_04.rst:38
msgid ""
"\\sum_i \\epsilon_{i}^2\n"
"\n"
msgstr ""
"\\sum_i \\epsilon_{i}^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_04.rst:40
msgid ""
"Yes, yes that sounds even better. And since I’ve indented it like that, it "
"probably means that this is the right answer. And since this is the right "
"answer, it’s probably worth making a note of the fact that our regression "
"coefficients are *estimates* (we’re trying to guess the parameters that "
"describe a population!), which is why I’ve added the little hats, so that we "
"get :math:`\\hat{b}_0` and :math:`\\hat{b}_1` rather than *b*\\ :sub:`0` and "
"*b*\\ :sub:`1`. Finally, I should also note that, since there’s actually "
"more than one way to estimate a regression model, the more technical name "
"for this estimation process is **ordinary least squares (OLS) regression**."
msgstr ""
"Ja, ja, det høres enda bedre ut. Og siden jeg har innrykket det slik, betyr "
"det sannsynligvis at dette er det riktige svaret. Og siden dette er det "
"riktige svaret, er det nok verdt å merke seg at regresjonskoeffisientene "
"våre er *estimater* (vi prøver å gjette parameterne som beskriver en "
"populasjon!), og det er derfor jeg har lagt til de små hattene, slik at vi "
"får :math:`\\hat{b}_0` og :math:`\\hat{b}_1` i stedet for *b*\\ :sub:`0` og "
"*b*\\ :sub:`1`. Til slutt vil jeg også bemerke at siden det faktisk finnes "
"mer enn én måte å estimere en regresjonsmodell på, er det mer tekniske "
"navnet på denne estimeringsprosessen **vanlig minste kvadraters metode (OLS) "
"regresjon**."

#: ../../Ch12/Ch12_Regression_04.rst:51
msgid ""
"At this point, we now have a concrete definition for what counts as our "
"“best” choice of regression coefficients, :math:`\\hat{b}_0` and :math:"
"`\\hat{b}_1`. The natural question to ask next is, if our optimal regression "
"coefficients are those that minimise the sum squared residuals, how do we "
"*find* these wonderful numbers? The actual answer to this question is "
"complicated and doesn’t help you understand the logic of regression.\\ [#]_ "
"This time I’m going to let you off the hook. Instead of showing you the long "
"and tedious way first and then “revealing” the wonderful shortcut that "
"jamovi provides, let’s cut straight to the chase and just use jamovi to do "
"all the heavy lifting."
msgstr ""
"Nå har vi en konkret definisjon av hva som er vårt «beste» valg av "
"regresjonskoeffisienter, :math:`\\hat{b}_0` og :math:`\\hat{b}_1`. Det neste "
"naturlige spørsmålet å stille er: Hvis de optimale regresjonskoeffisientene "
"er de som minimerer summen av de kvadrerte residuene, hvordan *finner* vi da "
"disse fantastiske tallene? Det faktiske svaret på dette spørsmålet er "
"komplisert og hjelper deg ikke med å forstå logikken i regresjon.\\ [#]_ I "
"stedet for å vise deg den lange og kjedelige veien først og deretter "
"«avsløre» den fantastiske snarveien som jamovi gir deg, skal vi gå rett på "
"sak og bare bruke jamovi til å gjøre alt det tunge arbeidet."

#: ../../Ch12/Ch12_Regression_04.rst:63
msgid "Linear regression in jamovi"
msgstr "Lineær regresjon i jamovi"

#: ../../Ch12/Ch12_Regression_04.rst:67 ../../Ch12/Ch12_Regression_04.rst:71
msgid "jamovi screenshot showing a simple linear regression analysis"
msgstr "Skjermbilde fra jamovi som viser en enkel lineær regresjonsanalyse"

#: ../../Ch12/Ch12_Regression_04.rst:75
msgid ""
"To run my linear regression, open up the ``Regression`` - ``Linear "
"Regression`` analysis in jamovi, using the |parenthood|_ data set. Then "
"specify ``dani.grump`` as the ``Dependent Variable`` and ``dani.sleep`` as "
"the variable entered in the ``Covariates`` box. This gives the results shown "
"in :numref:`fig-reg1`, showing an intercept :math:`\\hat{b}_0` = 125.96 and "
"the slope :math:`\\hat{b}_1` = -8.94. In other words, the best-fitting "
"regression line that I plotted in :numref:`fig-regression1` has this formula:"
msgstr ""
"For å kjøre den lineære regresjonen min, åpner du analysen ``Regression`` - "
"``Linear Regression`` i jamovi, ved å bruke datasettet |parenthood|_. Angi "
"deretter ``dani.grump`` som ``Dependent Variable`` og ``dani.sleep`` som "
"variabel i ``Covariates``-boksen. Dette gir resultatene som vises i :numref:"
"`fig-reg1`, som viser et skjæringspunkt (*intercept*) :math:`\\hat{b}_0` = "
"125,96 og en stigning :math:`\\hat{b}_1` = -8,94. Med andre ord har den best "
"tilpassede regresjonslinjen som jeg plottet i :numref:`fig-regression1`, "
"denne formelen:"

#: ../../Ch12/Ch12_Regression_04.rst:83
msgid "*Ŷ*\\ :sub:`i` = 125.96 + (-8.94 \\ *X*\\ :sub:`i`)"
msgstr "*Ŷ*\\ :sub:`i` = 125.96 + (-8.94 \\ *X*\\ :sub:`i`)"

#: ../../Ch12/Ch12_Regression_04.rst:86
msgid "Interpreting the estimated model"
msgstr "Tolkning av den estimerte modellen"

#: ../../Ch12/Ch12_Regression_04.rst:88
msgid ""
"The most important thing to be able to understand is how to interpret these "
"coefficients. Let’s start with :math:`\\hat{b}_1`, the slope. If we remember "
"the definition of the slope, a regression coefficient of :math:`\\hat{b}_1` "
"= -8.94 means that if I increase *X*\\ :sub:`i` by 1, then I’m decreasing "
"*Y*\\ :sub:`i` by 8.94. That is, each additional hour of sleep that I gain "
"will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What "
"about the intercept? Well, since :math:`\\hat{b}_0` corresponds to “the "
"expected value of *Y*\\ :sub:`i` when *X*\\ :sub:`i` equals 0”, it’s pretty "
"straightforward. It implies that if I get zero hours of sleep (*X*\\ :sub:"
"`i` = 0) then my grumpiness will go off the scale, to an insane value of "
"(*Y*\\ :sub:`i` = \\125.96). Best to be avoided, I think."
msgstr ""
"Det viktigste å forstå er hvordan disse koeffisientene skal tolkes. La oss "
"begynne med :math:`\\hat{b}_1`, stigningen. Hvis vi husker definisjonen av "
"stigningen, betyr en regresjonskoeffisient på :math:`\\hat{b}_1` = -8,94 at "
"hvis jeg øker *X*\\ :sub:`i` med 1, så reduserer jeg *Y*\\ :sub:`i` med "
"8,94. Det vil si at hver ekstra time med søvn jeg får, vil forbedre humøret "
"mitt og redusere grettenheten min med 8,94 grettenhetspoeng. Hva med "
"skjæringspunktet? Vel, siden :math:`\\hat{b}_0` tilsvarer «den forventede "
"verdien av *Y*\\ :sub:`i` når *X*\\ :sub:`i` er lik 0», er det ganske "
"enkelt. Det innebærer at hvis jeg får null timers søvn (*X*\\ :sub:`i` = 0), "
"så vil grettenheten min gå helt av skaftet, til en vanvittig verdi på "
"(*Y*\\ :sub:`i` = \\125,96). Det er best å unngå, tror jeg."

#: ../../Ch12/Ch12_Regression_04.rst:103
msgid ""
"Or at least, I’m assuming that it doesn’t help most people. But on the off-"
"chance that someone reading this is a proper kung fu master of linear "
"algebra (and to be fair, I always have a few of these people in my "
"intorductory statistics class), it *will* help *you* to know that the "
"solution to the estimation problem turns out to be :math:`\\hat{b} = "
"(\\mathbf{X}^\\prime\\mathbf{X})^{-1} \\mathbf{X}^\\prime y`, where :math:"
"`\\hat{b}` is a vector containing the estimated regression coefficients, "
"**X** is the “design matrix” that contains the predictor variables (plus an "
"additional column containing all ones; strictly **X** is a matrix of the "
"regressors, but I haven’t discussed the distinction yet), and *y* is a "
"vector containing the outcome variable. For everyone else, this isn’t "
"exactly helpful and can be downright scary. However, since quite a few "
"things in linear regression can be written in linear algebra terms, you’ll "
"see a bunch of footnotes like this one in this chapter. If you can follow "
"the maths in them, great. If not, ignore it."
msgstr ""
"Jeg antar i hvert fall at det ikke hjelper folk flest. Men hvis noen som "
"leser dette er en skikkelig kung fu-mester i lineær algebra (og for å være "
"rettferdig, jeg har alltid noen få av disse menneskene i innføringskurset "
"mitt i statistikk), vil det *hjelpe* *deg* å vite at løsningen på "
"estimeringsproblemet viser seg å være :math:`\\hat{b} = (\\mathbf{X}"
"^\\prime\\mathbf{X})^{-1} \\mathbf{X}^\\prime y`, hvor :math:`\\hat{b}` er "
"en vektor som inneholder de estimerte regresjonskoeffisientene, **X** er "
"«designmatrisen» som inneholder prediktorvariablene (pluss en ekstra kolonne "
"som inneholder alle enere; strengt tatt er **X** en matrise av regressorene, "
"men jeg har ikke diskutert skillet ennå), og *y* er en vektor som inneholder "
"utfallsvariabelen. For alle andre er dette ikke akkurat til hjelp, og det "
"kan være direkte skremmende. Men siden ganske mange ting i lineær regresjon "
"kan skrives i lineær algebra, vil du se en rekke fotnoter som denne i dette "
"kapittelet. Hvis du kan følge matematikken i dem, er det flott. Hvis ikke, "
"kan du ignorere dem."

#: ../../Ch12/Ch12_Regression_05.rst:4
msgid "Multiple linear regression"
msgstr "Multippel lineær regresjon"

#: ../../Ch12/Ch12_Regression_05.rst:6
msgid ""
"The simple linear regression model that we’ve discussed up to this point "
"assumes that there’s a single predictor variable that you’re interested in, "
"in this case ``dani.sleep``. In fact, up to this point *every* statistical "
"tool that we’ve talked about has assumed that your analysis uses one "
"predictor variable and one outcome variable. However, in many (perhaps most) "
"research projects you actually have multiple predictors that you want to "
"examine. If so, it would be nice to be able to extend the linear regression "
"framework to be able to include multiple predictors. Perhaps some kind of "
"**multiple regression** model would be in order?"
msgstr ""
"Den enkle lineære regresjonsmodellen som vi har diskutert frem til nå, "
"forutsetter at det er én enkelt prediktorvariabel du er interessert i, i "
"dette tilfellet ``dani.sleep``. Faktisk har *alle* statistiske verktøy vi "
"har snakket om til nå, forutsatt at analysen din bruker én prediktorvariabel "
"og én utfallsvariabel. Men i mange (kanskje de fleste) forskningsprosjekter "
"har du faktisk flere prediktorer som du ønsker å undersøke. I så fall ville "
"det vært fint å kunne utvide det lineære regresjonsrammeverket til å "
"inkludere flere prediktorer. Kanskje en slags **multippel regresjonsmodell** "
"ville være på sin plass?"

#: ../../Ch12/Ch12_Regression_05.rst:17
msgid ""
"Multiple regression is conceptually very simple. All we do is add more terms "
"to our regression equation. Let’s suppose that we’ve got two variables that "
"we’re interested in; perhaps we want to use both ``dani.sleep`` and ``baby."
"sleep`` to predict the ``dani.grump`` variable. As before, we let *Y*\\ :sub:"
"`i` refer to my grumpiness on the i-th day. But now we have two *X* "
"variables: the first corresponding to the amount of sleep I got and the "
"second corresponding to the amount of sleep my son got. So we’ll let *X*\\ :"
"sub:`i1` refer to the hours I slept on the i-th day and *X*\\ :sub:`i2` "
"refers to the hours that the baby slept on that day. If so, then we can "
"write our regression model like this:"
msgstr ""
"Multippel regresjon er konseptuelt sett veldig enkelt. Alt vi gjør er å "
"legge til flere termer i regresjonsligningen. La oss anta at vi har to "
"variabler som vi er interessert i; kanskje vi ønsker å bruke både ``dani."
"sleep`` og ``baby.sleep`` til å predikere variabelen ``dani.grump``. Som før "
"lar vi *Y*\\ :sub:`i` referere til min grettenhet på den i-te dagen. Men nå "
"har vi to *X*-variabler: den første tilsvarer søvnmengden jeg har fått, og "
"den andre tilsvarer søvnmengden sønnen min har fått. Så vi lar *X*\\ :sub:"
"`i1` referere til de timene jeg sov den i-te dagen, og *X*\\ :sub:`i2` til "
"de timene babyen sov den dagen. I så fall kan vi skrive regresjonsmodellen "
"vår slik:"

#: ../../Ch12/Ch12_Regression_05.rst:29
msgid ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i1` + *b*\\ :"
"sub:`2` *X*\\ :sub:`i2` + ε\\ :sub:`i`"
msgstr ""
"*Y*\\ :sub:`i` = *b*\\ :sub:`0` + *b*\\ :sub:`1` *X*\\ :sub:`i1` + *b*\\ "
":sub:`2` *X*\\ :sub:`i2` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_05.rst:31
msgid ""
"As before, ε\\ :sub:`i` is the residual associated with the i-th "
"observation, :math:`{\\epsilon}_i = {Y}_i - \\hat{Y}_i`. In this model, we "
"now have three coefficients that need to be estimated: *b*\\ :sub:`0` is the "
"intercept, *b*\\ :sub:`1` is the coefficient associated with my sleep, and "
"*b*\\ :sub:`2` is the coefficient associated with my son’s sleep. However, "
"although the number of coefficients that need to be estimated has changed, "
"the basic idea of how the estimation works is unchanged: our estimated "
"coefficients :math:`\\hat{b}_0`, :math:`\\hat{b}_1` and :math:`\\hat{b}_2` "
"are those that minimise the sum squared residuals."
msgstr ""
"Som tidligere er ε\\ :sub:`i` residuumet knyttet til den i-te "
"observasjonen, :math:`{\\epsilon}_i = {Y}_i - \\hat{Y}_i`. I denne modellen "
"har vi nå tre koeffisienter som må estimeres: *b*\\ :sub:`0` er "
"skjæringspunktet (*intercept*), *b*\\ :sub:`1` er koeffisienten som er "
"knyttet til min søvn, og *b*\\ :sub:`2` er koeffisienten som er knyttet til "
"min sønns søvn. Selv om antallet koeffisienter som må estimeres, har endret "
"seg, er den grunnleggende ideen om hvordan estimeringen fungerer uendret: De "
"estimerte koeffisientene :math:`\\hat{b}_0`, :math:`\\hat{b}_1` og :math:"
"`\\hat{b}_2` er de koeffisientene som minimerer summen av de kvadrerte "
"residuene."

#: ../../Ch12/Ch12_Regression_05.rst:42
msgid "Doing it in jamovi"
msgstr "Gjør det i jamovi"

#: ../../Ch12/Ch12_Regression_05.rst:44
msgid ""
"Multiple regression in jamovi is no different to simple regression. All we "
"have to do is add additional variables to the ``Covariates`` box in jamovi. "
"For example, if we want to use both ``dani.sleep`` and ``baby.sleep`` as "
"predictors in our attempt to explain why I’m so grumpy, then move ``baby."
"sleep`` across into the ``Covariates`` box alongside ``dani.sleep``. By "
"default, jamovi assumes that the model should include an intercept. The "
"coefficients we get this time are:"
msgstr ""
"Multippel regresjon i jamovi er ikke annerledes enn enkel regresjon. Alt vi "
"trenger å gjøre, er å legge til flere variabler i boksen ``Covariates`` i "
"jamovi. Hvis vi for eksempel vil bruke både ``dani.sleep`` og ``baby.sleep`` "
"som prediktorer i forsøket på å forklare hvorfor jeg er så gretten, flytter "
"vi ``baby.sleep`` over i ``Covariates``-boksen ved siden av ``dani.sleep``. "
"Som standard antar jamovi at modellen skal inneholde et skjæringspunkt. "
"Koeffisientene vi får denne gangen er:"

#: ../../Ch12/Ch12_Regression_05.rst:52
msgid ""
"Model coefficients for the linear model predicting ``dani.grump`` using "
"``baby.sleep`` and ``dani.sleep`` (from the |parenthood|_ data set)."
msgstr ""
"Modellkoeffisienter for den lineære modellen som predikerer ``dani.grump`` "
"ved hjelp av ``baby.sleep`` og ``dani.sleep`` (fra datasettet |parenthood|_)."

#: ../../Ch12/Ch12_Regression_05.rst:57
msgid "Predictor"
msgstr "Prediktor"

#: ../../Ch12/Ch12_Regression_05.rst:57
msgid "Estimate"
msgstr "Estimat"

#: ../../Ch12/Ch12_Regression_05.rst:59
msgid "Intercept"
msgstr "Skjæringspunkt"

#: ../../Ch12/Ch12_Regression_05.rst:59
msgid "125.966"
msgstr "125.966"

#: ../../Ch12/Ch12_Regression_05.rst:61
msgid "``dani.sleep``"
msgstr "``dani.sleep``"

#: ../../Ch12/Ch12_Regression_05.rst:61
msgid "-8.950"
msgstr "-8.950"

#: ../../Ch12/Ch12_Regression_05.rst:63
msgid "``baby.sleep``"
msgstr "``baby.sleep``"

#: ../../Ch12/Ch12_Regression_05.rst:63
msgid "0.011"
msgstr "0.011"

#: ../../Ch12/Ch12_Regression_05.rst:66
msgid ""
"The coefficient associated with ``dani.sleep`` is quite large, suggesting "
"that every hour of sleep I lose makes me a lot grumpier. However, the "
"coefficient for ``baby.sleep`` is very small, suggesting that it doesn’t "
"really matter how much sleep my son gets. What matters as far as my "
"grumpiness goes is how much sleep *I* get. To get a sense of what this "
"multiple regression model looks like, :numref:`fig-scatter3d_1` shows a 3D "
"plot that plots all three variables, along with the regression model itself."
msgstr ""
"Koeffisienten knyttet til ``dani.sleep`` er ganske stor, noe som tyder på at "
"hver time med søvn jeg mister, gjør meg mye grinete. Koeffisienten for "
"``baby.sleep`` er imidlertid svært liten, noe som tyder på at det egentlig "
"ikke spiller noen rolle hvor mye søvn sønnen min får. Det som betyr noe for "
"hvor gretten jeg er, er hvor mye søvn *jeg* får. For å få en følelse av "
"hvordan denne multiple regresjonsmodellen ser ut, viser :numref:`fig-"
"scatter3d_1` et 3D-plott som plotter alle de tre variablene, sammen med "
"selve regresjonsmodellen."

#: ../../Ch12/Ch12_Regression_05.rst:77
msgid "3D visualisation of a multiple regression model"
msgstr "3D-visualisering av en multippel regresjonsmodell"

#: ../../Ch12/Ch12_Regression_05.rst:81
msgid ""
"3D visualisation of a multiple regression model: There are two predictors in "
"the model, ``dani.sleep`` and ``baby.sleep`` and the outcome variable is "
"``dani.grump``. Together, these three variables form a 3D space. Each "
"observation (dot) is a point in this space. In much the same way that a "
"simple linear regression model forms a line in 2D space, this multiple "
"regression model forms a plane in 3D space. When we estimate the regression "
"coefficients what we’re trying to do is find a plane that is as close to all "
"the blue dots as possible."
msgstr ""
"3D-visualisering av en multippel regresjonsmodell: Det er to prediktorer i "
"modellen, ``dani.sleep`` og ``baby.sleep``, og utfallsvariabelen er ``dani."
"grump``. Sammen danner disse tre variablene et 3D-rom. Hver observasjon "
"(prikk) er et punkt i dette rommet. På samme måte som en enkel lineær "
"regresjonsmodell danner en linje i et 2D-rom, danner denne multiple "
"regresjonsmodellen et plan i et 3D-rom. Når vi estimerer "
"regresjonskoeffisientene, prøver vi å finne et plan som ligger så nær alle "
"de blå prikkene som mulig."

#: ../../Ch12/Ch12_Regression_05.rst:93
msgid "Formula for the general case"
msgstr "Formel for det generelle tilfellet"

#: ../../Ch12/Ch12_Regression_05.rst:95
msgid ""
"The equation that I gave above shows you what a multiple regression model "
"looks like when you include two predictors. Not surprisingly, then, if you "
"want more than two predictors all you have to do is add more *X* terms and "
"more *b* coefficients. In other words, if you have *K* predictor variables "
"in the model then the regression equation looks like this:"
msgstr ""
"Ligningen jeg viste ovenfor, viser hvordan en multippel regresjonsmodell ser "
"ut når du inkluderer to prediktorer. Ikke overraskende er det slik at hvis "
"du vil ha flere enn to prediktorer, er alt du trenger å gjøre å legge til "
"flere *X*-termer og flere *b*-koeffisienter. Med andre ord, hvis du har *K* "
"prediktorvariabler i modellen, ser regresjonsligningen slik ut:"

#: ../../Ch12/Ch12_Regression_05.rst:102
msgid ""
"Y_i = b_0 + \\left( \\sum_{k=1}^K b_{k} X_{ik} \\right) + \\epsilon_i\n"
"\n"
msgstr ""
"Y_i = b_0 + \\left( \\sum_{k=1}^K b_{k} X_{ik} \\right) + \\epsilon_i\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:4
msgid "Quantifying the fit of the regression model"
msgstr "Kvantifisering av regresjonsmodellens tilpasning"

#: ../../Ch12/Ch12_Regression_06.rst:6
msgid ""
"So we now know how to estimate the coefficients of a linear regression "
"model. The problem is, we don’t yet know if this regression model is any "
"good. For example, the ``regression.1`` model *claims* that every hour of "
"sleep will improve my mood by quite a lot, but it might just be rubbish. "
"Remember, the regression model only produces a prediction *Ŷ*\\ :sub:`i` "
"about what my mood is like, but my actual mood is *Y*\\ :sub:`i`. If these "
"two are very close, then the regression model has done a good job. If they "
"are very different, then it has done a bad job."
msgstr ""
"Nå vet vi altså hvordan vi kan estimere koeffisientene i en lineær "
"regresjonsmodell. Problemet er at vi ennå ikke vet om denne "
"regresjonsmodellen er god. For eksempel *påstår* modellen ``regression.1`` "
"at hver times søvn vil forbedre humøret mitt ganske mye, men det kan være at "
"det bare er tull. Husk at regresjonsmodellen bare gir en prediksjon *Ŷ*\\ :"
"sub:`i` om hvordan humøret mitt er, mens det faktiske humøret mitt er *Y*\\ :"
"sub:`i`. Hvis disse to ligger svært nær hverandre, har regresjonsmodellen "
"gjort en god jobb. Hvis de er svært forskjellige, har den gjort en dårlig "
"jobb."

#: ../../Ch12/Ch12_Regression_06.rst:16
msgid "The *R*\\² (R-squared) value"
msgstr "Verdien *R*\\² (R-kvadrat)"

#: ../../Ch12/Ch12_Regression_06.rst:18
msgid ""
"Once again, let’s wrap a little bit of mathematics around this. Firstly, "
"we’ve got the sum of the squared residuals"
msgstr ""
"La oss igjen bruke litt matematikk rundt dette. For det første har vi summen "
"av de kvadrerte residuene (en forkortelse av kvadratsumme hvor vi "
"oppsummerer flere kvadrerte verdier er SS; *sum of squares*)"

#: ../../Ch12/Ch12_Regression_06.rst:21
msgid ""
"\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"
msgstr ""
"\\mbox{SS}_{res} = \\sum_i (Y_i - \\hat{Y}_i)^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:23
msgid ""
"which we would hope to be pretty small. Specifically, what we’d like is for "
"it to be very small in comparison to the total variability in the outcome "
"variable"
msgstr ""
"som vi håper er ganske liten. Nærmere bestemt ønsker vi at den skal være "
"svært liten i forhold til den totale variabiliteten i utfallsvariabelen"

#: ../../Ch12/Ch12_Regression_06.rst:27
msgid ""
"\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2\n"
"\n"
msgstr ""
"\\mbox{SS}_{tot} = \\sum_i (Y_i - \\bar{Y})^2\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:29
msgid ""
"While we’re here, let’s calculate these values ourselves, not by hand "
"though. Let’s use jamovi instead. Open up the |parenthood|_ data set in so "
"that we can work in it. The first thing to do is calculate the *Ŷ* values, "
"and for the simple model that uses only a single predictor we would do the "
"following:"
msgstr ""
"Mens vi er her, la oss beregne disse verdiene selv, men ikke for hånd. La "
"oss bruke jamovi i stedet. Åpne datasettet |parenthood|_ slik at vi kan "
"jobbe i det. Det første vi må gjøre er å beregne *Ŷ*-verdiene, og for den "
"enkle modellen som bare bruker én enkelt prediktor, gjør vi følgende:"

#: ../../Ch12/Ch12_Regression_06.rst:34
msgid ""
"Go to an empty column (at the end of the data set) and double click on the "
"column header, choose ``New computed variable`` and enter ``Y_pred`` in the "
"first line and the formula ``125.97 + (-8.94 * dani.sleep)`` in the line "
"starting with ``=`` (next to the *f*\\ :sub:`x`)."
msgstr ""
"Gå til en tom kolonne (på slutten av datasettet) og dobbeltklikk på "
"kolonneoverskriften, velg ``New computed variable`` og skriv inn ``Y_pred`` "
"i den første linjen og formelen ``125.97 + (-8.94 * dani.sleep)`` i linjen "
"som starter med ``=`` (ved siden av *f*\\ :sub:`x`)."

#: ../../Ch12/Ch12_Regression_06.rst:39
msgid ""
"Okay, now that we’ve got a variable which stores the regression model "
"predictions for how grumpy I will be on any given day, let’s calculate our "
"sum of squared residuals. We would do that using the following formula:"
msgstr ""
"Nå som vi har en variabel som lagrer regresjonsmodellens prediksjoner for "
"hvor gretten jeg vil være på en gitt dag, kan vi beregne summen av de "
"kvadrerte residuene. Det gjør vi ved hjelp av følgende formel:"

#: ../../Ch12/Ch12_Regression_06.rst:44
msgid ""
"Calculate the squared residuals by creating a new column called ``sq_resid`` "
"using the formula ``(dani.grump - Y_pred) ^ 2``. The values in this column "
"are later summed up to obtain SS\\ :sub:`res`."
msgstr ""
"Beregn de kvadrerte residuene ved å opprette en ny kolonne kalt ``sq_resid`` "
"ved hjelp av formelen ``(dani.grump - Y_pred) ^ 2``. Verdiene i denne "
"kolonnen summeres senere for å få SS\\ :sub:`res`."

#: ../../Ch12/Ch12_Regression_06.rst:48
msgid ""
"Calculate the squared deviation from the mean by creating yet another column "
"called ``sq_total`` using the formula ``(dani.grump - VMEAN(dani.grump)) ^ "
"2``. The values in this column are later summed up to obtain SS\\ :sub:`tot`."
msgstr ""
"Beregn det kvadrerte avviket fra gjennomsnittet ved å opprette enda en "
"kolonne kalt ``sq_total`` ved hjelp av formelen ``(dani.grump - VMEAN(dani."
"grump)) ^ 2``. Verdiene i denne kolonnen summeres senere for å få SS\\ :sub:"
"`tot`."

#: ../../Ch12/Ch12_Regression_06.rst:53
msgid ""
"To calculate the sum of these values, click ``Descriptives`` → ``Descriptive "
"Statistics`` and move ``sq_resid`` and ``sq_total`` to the ``Variables`` "
"box. You’ll then need to select ``Sum`` from the ``Statistics`` drop-down "
"menu below. The sum of ``sq_resid`` has a value of **1838.722**. This is a "
"big number, however, that doesn’t mean very much. The sum of ``sq_total`` "
"has a value of **9998.590**. Well, it’s a much (about five times) bigger "
"number than the last one, so this does suggest that our regression model was "
"making good predictions (that is, it has greatly reduced the residual error "
"compared to the model that uses the mean as a single predictor). But it’s "
"not very interpretable."
msgstr ""
"For å beregne summen av disse verdiene klikker du på ``Descriptives`` → "
"``Descriptive Statistics`` og flytter ``sq_resid`` og ``sq_total`` til "
"``Variables``-boksen. Deretter må du velge ``Sum`` fra rullegardinmenyen "
"``Statistics`` nedenfor. Summen av ``sq_resid`` har en verdi på "
"**1838.722**. Dette er et stort tall, men det betyr ikke så mye. Summen av "
"``sq_total`` har en verdi på **9998.590**. Vel, det er et mye (omtrent fem "
"ganger) større tall enn det forrige, så dette tyder på at regresjonsmodellen "
"vår gjorde gode prediksjoner (det vil si at den har redusert feilen "
"tilskrevet til residuenekraftig sammenlignet med modellen som bruker "
"gjennomsnittet som den eneste prediktoren). Men det er ikke særlig tolkbart."

#: ../../Ch12/Ch12_Regression_06.rst:64
msgid ""
"To can fix this, we’d like to convert these two fairly meaningless numbers "
"into one number. A nice, interpretable number, which for no particular "
"reason we’ll call *R*\\². What we would like is for the value of *R*\\² to "
"be equal to 1 if the regression model makes no errors in predicting the "
"data. In other words, if it turns out that the residual errors are zero. "
"That is, if SS\\ :sub:`res` = 0 then we expect *R*\\² = 1. Similarly, if the "
"model is completely useless, we would like *R*\\² to be equal to 0. What do "
"I mean by “useless”? Tempting as it is to demand that the regression model "
"move out of the house, cut its hair and get a real job, I’m probably going "
"to have to pick a more practical definition. In this case, all I mean is "
"that the residual sum of squares is no smaller than the total sum of "
"squares, SS\\ :sub:`res` = SS\\ :sub:`tot`. Wait, why don’t we do exactly "
"that? The formula that provides us with our *R*\\² value is pretty simple to "
"write down, and equally simple to calculate by hand:\\ [#]_"
msgstr ""
"For å fikse dette, vil vi gjerne konvertere disse to ganske meningsløse "
"tallene til ett tall. Et fint, tolkbart tall, som vi uten noen spesiell "
"grunn kaller *R*\\². Det vi ønsker, er at verdien av *R*\\² skal være lik 1 "
"hvis regresjonsmodellen ikke gjør noen feil når den predikerer dataene. Med "
"andre ord, hvis det viser seg at feilene for residuene er null. Det vil si "
"at hvis SS\\ :sub:`res` = 0, forventer vi at *R*\\² = 1. Tilsvarende, hvis "
"modellen er helt ubrukelig, ønsker vi at *R*\\² skal være lik 0. Hva mener "
"jeg med «ubrukelig»? Selv om det er fristende å kreve at regresjonsmodellen "
"flytter ut av huset, klipper håret og skaffer seg en ordentlig jobb, må jeg "
"nok velge en mer praktisk definisjon. I dette tilfellet mener jeg bare at "
"kvadratsummen for residuene ikke er mindre enn den totale kvadratsummen, "
"SS\\ :sub:`res` = SS\\ :sub:`tot`. Vent, hvorfor gjør vi ikke akkurat det? "
"Formelen som gir oss *R*\\²-verdien er ganske enkel å skrive ned, og like "
"enkel å regne ut for hånd:\\ [#]_"

#: ../../Ch12/Ch12_Regression_06.rst:78
msgid "*R*\\² = 1 - (SS\\ :sub:`res` / SS\\ :sub:`tot`)"
msgstr "*R*\\² = 1 - (SS\\ :sub:`res` / SS\\ :sub:`tot`)"

#: ../../Ch12/Ch12_Regression_06.rst:79
msgid "*R*\\² = 1 - (1838.722 / 9998.590)"
msgstr "*R*\\² = 1 - (1838.722 / 9998.590)"

#: ../../Ch12/Ch12_Regression_06.rst:80
msgid "*R*\\² = 1 - 0.184"
msgstr "*R*\\² = 1 - 0.184"

#: ../../Ch12/Ch12_Regression_06.rst:82
msgid ""
"This gives a value for *R*\\² of **0.816**. The *R*\\² value, sometimes "
"called the **coefficient of determination**\\ [#]_ has a simple "
"interpretation: it is the *proportion* of the variance in the outcome "
"variable that can be accounted for by the predictor. So, in this case the "
"fact that we have obtained *R*\\² = 0.816 means that the predictor (``dani."
"sleep``) explains 81.6\\% of the variance in the outcome (``dani.grump``).\\ "
"[#]_"
msgstr ""
"Dette gir en verdi for *R*\\² på **0,816**. Verdien *R*\\², som noen ganger "
"kalles **determinasjonskoeffisienten**,\\ [#]_ har en enkel tolkning: Det er "
"*andelen* av variansen i utfallsvariabelen som kan forklares av prediktoren. "
"I dette tilfellet betyr altså det faktum at vi har fått *R*\\² = 0,816 at "
"prediktoren (``dani.sleep``) forklarer 81,6\\% av variansen i utfallet "
"(``dani.grump``).\\ [#]_"

#: ../../Ch12/Ch12_Regression_06.rst:89
msgid ""
"Naturally, you don’t actually need to do all these calculations yourself if "
"you want to obtain the *R*\\² value for your regression model. As we’ll see "
"later on in :ref:`Running the hypothesis tests in jamovi "
"<coefficients_in_jamovi>`, all you need to do is specify this as an option "
"in jamovi. However, let’s put that to one side for the moment. There’s "
"another property of *R*\\² that I want to point out."
msgstr ""
"Du trenger naturligvis ikke å gjøre alle disse beregningene selv hvis du "
"ønsker å få *R*\\²-verdien for regresjonsmodellen din. Som vi skal se senere "
"i :ref:`Kjøring av hypotesetester i jamovi <coefficients_in_jamovi>`, er alt "
"du trenger å gjøre å spesifisere dette som et alternativ i jamovi. La oss "
"imidlertid legge det til side for øyeblikket. Det er en annen egenskap ved "
"*R*\\² som jeg vil påpeke."

#: ../../Ch12/Ch12_Regression_06.rst:97
msgid "The relationship between regression and correlation"
msgstr "Forholdet mellom regresjon og korrelasjon"

#: ../../Ch12/Ch12_Regression_06.rst:99
msgid ""
"At this point we can revisit my earlier claim that regression, in this very "
"simple form that I’ve discussed so far, is basically the same thing as a "
"correlation. Previously, we used the symbol *r* to denote a Pearson "
"correlation. Might there be some relationship between the value of the "
"correlation coefficient *r* and the *R*\\² value from linear regression? Of "
"course there is: the squared correlation *R*\\² is identical to the *R*\\² "
"value for a linear regression with only a single predictor. In other words, "
"running a Pearson correlation is more or less equivalent to running a linear "
"regression model that uses only one predictor variable."
msgstr ""
"Her kan vi ta opp igjen min tidligere påstand om at regresjon, i denne svært "
"enkle formen jeg har diskutert så langt, i bunn og grunn er det samme som en "
"korrelasjon. Tidligere brukte vi symbolet *r* for å betegne en Pearson-"
"korrelasjon. Kan det være noen sammenheng mellom verdien av "
"korrelasjonskoeffisienten *r* og *R*\\²-verdien fra lineær regresjon? "
"Selvsagt er det det: Den kvadrerte korrelasjonen *R*\\² er identisk med "
"*R*\\²-verdien for en lineær regresjon med bare én enkelt prediktor. Med "
"andre ord er det å kjøre en Pearson-korrelasjon mer eller mindre det samme "
"som å kjøre en lineær regresjonsmodell med bare én prediktorvariabel."

#: ../../Ch12/Ch12_Regression_06.rst:110
msgid "The adjusted *R*\\² (R-squared) value"
msgstr "Den justerte *R*\\²-verdien (R-kvadrat)"

#: ../../Ch12/Ch12_Regression_06.rst:112
msgid ""
"One final thing to point out before moving on. It’s quite common for people "
"to report a slightly different measure of model performance, known as "
"“adjusted *R*\\²”. The motivation behind calculating the adjusted *R*\\² "
"value is the observation that adding more predictors into the model will "
"*always* cause the *R*\\² value to increase (or at least not decrease)."
msgstr ""
"En siste ting å påpeke før vi går videre. Det er ganske vanlig at folk "
"rapporterer et litt annet mål på modellens ytelse, kjent som «justert "
"*R*\\²». Motivasjonen bak beregningen av den justerte *R*\\²-verdien er "
"observasjonen om at det å legge til flere prediktorer i modellen *alltid* "
"vil føre til at *R*\\²-verdien øker (eller i det minste ikke synker)."

#: ../../Ch12/Ch12_Regression_06.rst:119
msgid ""
"The adjusted *R*\\² value introduces a slight change to the calculation, as "
"follows. For a regression model with *K* predictors, fit to a data set "
"containing *N* observations, the adjusted *R*\\² is:"
msgstr ""
"Den justerte *R*\\²-verdien innebærer en liten endring i beregningen, som "
"følger. For en regresjonsmodell med *K* prediktorer, tilpasset et datasett "
"som inneholder *N* observasjoner, er den justerte *R*\\²-verdien:"

#: ../../Ch12/Ch12_Regression_06.rst:124
msgid ""
"\\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} "
"\\times \\frac{N - 1}{N - K - 1} \\right)\n"
"\n"
msgstr ""
"\\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} "
"\\times \\frac{N - 1}{N - K - 1} \\right)\n"
"\n"

#: ../../Ch12/Ch12_Regression_06.rst:126
msgid ""
"This adjustment is an attempt to take the degrees of freedom into account. "
"The big advantage of the adjusted *R*\\² value is that when you add more "
"predictors to the model, the adjusted *R*\\² value will only increase if the "
"new variables improve the model performance more than you’d expect by "
"chance. The big disadvantage is that the adjusted *R*\\² value *can’t* be "
"interpreted in the elegant way that *R*\\² can. *R*\\² has a simple "
"interpretation as the proportion of variance in the outcome variable that is "
"explained by the regression model. To my knowledge, no equivalent "
"interpretation exists for adjusted *R*\\²."
msgstr ""
"Denne justeringen er et forsøk på å ta hensyn til frihetsgradene. Den store "
"fordelen med den justerte *R*\\²-verdien er at når du legger til flere "
"prediktorer i modellen, vil den justerte *R*\\²-verdien bare øke hvis de nye "
"variablene forbedrer modellens ytelse mer enn du ville forvente ved en "
"tilfeldighet. Den store ulempen er at den justerte *R*\\²-verdien *ikke* kan "
"tolkes på den elegante måten som *R*\\² kan. *R*\\² har en enkel tolkning "
"som andelen av variansen i utfallsvariabelen som forklares av "
"regresjonsmodellen. Så vidt jeg vet, finnes det ingen tilsvarende tolkning "
"for justert *R*\\²."

#: ../../Ch12/Ch12_Regression_06.rst:137
msgid ""
"An obvious question then is whether you should report *R*\\² or adjusted "
"*R*\\². This is probably a matter of personal preference. If you care more "
"about interpretability, then *R*\\² is better. If you care more about "
"correcting for bias, then adjusted *R*\\² is probably better. Speaking just "
"for myself, I prefer *R*\\². My feeling is that it’s more important to be "
"able to interpret your measure of model performance. Besides, as we’ll see "
"in section :doc:`Ch12_Regression_07`, if you’re worried that the improvement "
"in *R*\\² that you get by adding a predictor is just due to chance and not "
"because it’s a better model, well we’ve got hypothesis tests for that."
msgstr ""
"Et nærliggende spørsmål er da om du bør rapportere *R*\\² eller justert "
"*R*\\². Dette er sannsynligvis et spørsmål om personlige preferanser. Hvis "
"du er mer opptatt av tolkbarhet, er *R*\\² bedre. Hvis du er mer opptatt av "
"å korrigere for skjevheter, er justert *R*\\² sannsynligvis bedre. Selv "
"foretrekker jeg *R*\\². Jeg mener at det er viktigere å enkle kunne tolke "
"målet på modellens kvalitet. Dessuten, som vi skal se i avsnitt :doc:"
"`Ch12_Regression_07`, hvis du er bekymret for at forbedringen i *R*\\² som "
"du får ved å legge til en prediktor, bare skyldes tilfeldigheter og ikke "
"fordi det er en bedre modell, har vi hypotesetester for det."

#: ../../Ch12/Ch12_Regression_06.rst:150
msgid ""
"If you don't want to do these calculations by hand, just create another "
"computed variable called, e.g., ``R2``, and containing the formula ``1 - "
"VSUM(sq_resid) / VSUM(sq_total)``. But then you have a whole column "
"containing *R*\\²."
msgstr ""
"Hvis du ikke vil gjøre disse beregningene for hånd, kan du bare opprette en "
"annen beregnet variabel som heter for eksempel ``R2``, og som inneholder "
"formelen ``1 - VSUM(sq_resid) / VSUM(sq_total)``. Men da har du en hel "
"kolonne som inneholder *R*\\²."

#: ../../Ch12/Ch12_Regression_06.rst:156
msgid ""
"And by “sometimes” I mean “almost never”. In practice everyone just calls it "
"“*R*-squared”."
msgstr ""
"Og med «noen ganger» mener jeg «nesten aldri». I praksis kaller alle det "
"bare «*R*-kvadrat»."

#: ../../Ch12/Ch12_Regression_06.rst:160
msgid ""
"If you made a mistake or could not follow the explanations, you can simply "
"download and open the |parenthood_r2|_ data set."
msgstr ""
"Hvis du har gjort en feil eller ikke kunne følge forklaringene, kan du bare "
"laste ned og åpne datasettet |parenthood_r2|_."

#: ../../Ch12/Ch12_Regression_07.rst:4
msgid "Hypothesis tests for regression models"
msgstr "Hypotesetester for regresjonsmodeller"

#: ../../Ch12/Ch12_Regression_07.rst:6
msgid ""
"So far we’ve talked about what a regression model is, how the coefficients "
"of a regression model are estimated, and how we quantify the performance of "
"the model (the last of these, incidentally, is basically our measure of "
"effect size). The next thing we need to talk about is hypothesis tests. "
"There are two different (but related) kinds of hypothesis tests that we need "
"to talk about: those in which we test whether the regression model as a "
"whole is performing significantly better than a null model, and those in "
"which we test whether a particular regression coefficient is significantly "
"different from zero."
msgstr ""
"Så langt har vi snakket om hva en regresjonsmodell er, hvordan "
"koeffisientene i en regresjonsmodell estimeres, og hvordan vi kvantifiserer "
"modellens ytelse (det siste er for øvrig i bunn og grunn vårt mål på "
"effektstørrelse). Det neste vi må snakke om, er hypotesetester. Det er to "
"ulike (men beslektede) typer hypotesetester vi må snakke om: de der vi "
"tester om regresjonsmodellen som helhet presterer signifikant bedre enn en "
"nullmodell, og de der vi tester om en bestemt regresjonskoeffisient er "
"signifikant forskjellig fra null."

#: ../../Ch12/Ch12_Regression_07.rst:17
msgid "Testing the model as a whole"
msgstr "Testing av modellen som helhet"

#: ../../Ch12/Ch12_Regression_07.rst:19
msgid ""
"Okay, suppose you’ve estimated your regression model. The first hypothesis "
"test you might try is the null hypothesis that there is *no relationship* "
"between the predictors and the outcome, and the alternative hypothesis that "
"*the data are distributed in exactly the way that the regression model "
"predicts*."
msgstr ""
"Anta at du har estimert regresjonsmodellen din. Den første hypotesetesten du "
"kan prøve, er nullhypotesen om at det er *ingen sammenheng* mellom "
"prediktorene og utfallet, og alternativhypotesen om at *dataene er fordelt "
"på nøyaktig den måten regresjonsmodellen forutsier*."

#: ../../Ch12/Ch12_Regression_07.rst:25
msgid ""
"Formally, our “null model” corresponds to the fairly trivial “regression” "
"model in which we include 0 predictors and only include the intercept term "
"*b*\\ :sub:`0`:"
msgstr ""
"Formelt sett tilsvarer «nullmodellen» vår den ganske trivielle "
"«regresjonsmodellen» der vi inkluderer 0 prediktorer og bare inkluderer "
"termen for intercept *b*\\ :sub:`0`:"

#: ../../Ch12/Ch12_Regression_07.rst:29
msgid "H\\ :sub:`0`: *Y*\\ :sub:`i` = b\\ :sub:`0` + ε\\ :sub:`i`"
msgstr "H\\ :sub:`0`: *Y*\\ :sub:`i` = b\\ :sub:`0` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_07.rst:31
msgid ""
"If our regression model has *K* predictors, the “alternative model” is "
"described using the usual formula for a multiple regression model:"
msgstr ""
"Hvis regresjonsmodellen vår har *K* prediktorer, beskrives den «alternative "
"modellen» ved hjelp av den vanlige formelen for en multippel "
"regresjonsmodell:"

#: ../../Ch12/Ch12_Regression_07.rst:35
msgid ""
"H\\ :sub:`1`: *Y*\\ :sub:`i` = b\\ :sub:`0` + math:`\\left( \\sum_{k=1}^K "
"b_{k} X_{ik} \\right)` + ε\\ :sub:`i`"
msgstr ""
"H\\ :sub:`1`: *Y*\\ :sub:`i` = b\\ :sub:`0` + math:`\\left( \\sum_{k=1}^K "
"b_{k} X_{ik} \\right)` + ε\\ :sub:`i`"

#: ../../Ch12/Ch12_Regression_07.rst:37
msgid ""
"How can we test these two hypotheses against each other? The trick is to "
"understand that it’s possible to divide up the total variance SS\\ :sub:"
"`tot` into the sum of the residual variance SS\\ :sub:`res` and the "
"regression model variance SS\\ :sub:`mod`. I’ll skip over the "
"technicalities, since we’ll get to that later when we look at ANOVA in "
"chapter :doc:`../Ch13/Ch13_ANOVA`. But just note that"
msgstr ""
"Hvordan kan vi teste disse to hypotesene opp mot hverandre? Trikset er å "
"forstå at det er mulig å dele opp den totale variansen SS\\ :sub:`tot` i "
"summen av variansen til residuene SS\\ :sub:`res` og "
"regresjonsmodellvariansen SS\\ :sub:`mod`. Jeg hopper over de tekniske "
"detaljene, siden vi kommer til det senere når vi ser på ANOVA i kapittel :"
"doc:`../Ch13/Ch13_ANOVA`. Men legg bare merke til at"

#: ../../Ch12/Ch12_Regression_07.rst:44
msgid "SS\\ :sub:`mod` = SS\\ :sub:`tot` - SS\\ :sub:`res`"
msgstr "SS\\ :sub:`mod` = SS\\ :sub:`tot` - SS\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:46
msgid ""
"And we can convert the sums of squares into mean squares by dividing by the "
"degrees of freedom."
msgstr ""
"Og vi kan konvertere kvadratsummene til gjennomsnittlige kvadratsummene "
"(*mean squares*; MS) ved å dividere med frihetsgradene."

#: ../../Ch12/Ch12_Regression_07.rst:49
msgid "MS\\ :sub:`mod` = SS\\ :sub:`mod` / *df*\\ :sub:`mod`"
msgstr "MS\\ :sub:`mod` = SS\\ :sub:`mod` / *df*\\ :sub:`mod`"

#: ../../Ch12/Ch12_Regression_07.rst:50
msgid "SS\\ :sub:`res` = SS\\ :sub:`res` / *df*\\ :sub:`res`"
msgstr "SS\\ :sub:`res` = SS\\ :sub:`res` / *df*\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:52
msgid ""
"So, how many degrees of freedom do we have? As you might expect the *df* "
"associated with the model is closely tied to the number of predictors that "
"we’ve included. In fact, it turns out that *df*\\ :sub:`mod` = *K*. For the "
"residuals the total degrees of freedom is *df*\\ :sub:`res` = *N* - *K* - 1."
msgstr ""
"Hvor mange frihetsgrader har vi? Som du kanskje forventer, er *df* knyttet "
"til modellen nært knyttet til antallet prediktorer vi har inkludert. Faktisk "
"viser det seg at *df*\\ :sub:`mod` = *K*. For residuene er de totale "
"frihetsgradene *df*\\ :sub:`res` = *N* - *K* - 1."

#: ../../Ch12/Ch12_Regression_07.rst:58
msgid ""
"Now that we’ve got our mean square values we can calculate an *F*-statistic "
"like this:"
msgstr ""
"Nå som vi har fått våre gjennomsnittlige kvadratsummene, kan vi beregne en "
"*F*-statistikk på denne måten:"

#: ../../Ch12/Ch12_Regression_07.rst:61
msgid "F = MS\\ :sub:`mod` / SS\\ :sub:`res`"
msgstr "F = MS\\ :sub:`mod` / SS\\ :sub:`res`"

#: ../../Ch12/Ch12_Regression_07.rst:63
msgid ""
"and the degrees of freedom associated with this are *K* and *N* - *K* - 1."
msgstr "og frihetsgradene knyttet til dette er *K* og *N* - *K* - 1."

#: ../../Ch12/Ch12_Regression_07.rst:66
msgid ""
"We’ll see much more of the *F*-statistic in chapter :doc:`../Ch13/"
"Ch13_ANOVA`, but for now just know that we can interpret large *F*-values as "
"indicating that the null hypothesis is performing poorly in comparison to "
"the alternative hypothesis. In a moment I’ll show you how to do the test in "
"jamovi the easy way, but first let’s have a look at the tests for the "
"individual regression coefficients."
msgstr ""
"Vi skal se mye mer om *F*-statistikken i kapittel :doc:`../Ch13/Ch13_ANOVA`, "
"men inntil videre skal du bare vite at vi kan tolke store *F*-verdier som en "
"indikasjon på at nullhypotesen gjør det dårlig i forhold til "
"alternativhypotesen. Om et øyeblikk skal jeg vise deg hvordan du gjør testen "
"i jamovi på den enkle måten, men la oss først ta en titt på testene for de "
"enkelte regresjonskoeffisientene."

#: ../../Ch12/Ch12_Regression_07.rst:74
msgid "Tests for individual coefficients"
msgstr "Tester for individuelle koeffisienter"

#: ../../Ch12/Ch12_Regression_07.rst:76
msgid ""
"The *F*-test that we’ve just introduced is useful for checking that the "
"model as a whole is performing better than chance. If your regression model "
"doesn’t produce a significant result for the *F*-test then you probably "
"don’t have a very good regression model (or, quite possibly, you don’t have "
"very good data). However, while failing this test is a pretty strong "
"indicator that the model has problems, *passing* the test (i.e., rejecting "
"the null) doesn’t imply that the model is good! Why is that, you might be "
"wondering? The answer to that can be found by looking at the coefficients "
"for the multiple regression model we have already looked at :numref:`tab-"
"parent_coeff` in section :doc:`Ch12_Regression_05` above, where the "
"coefficients we got were 125.966 (for the intercept), -8.950 (for ``dani."
"sleep``) and 0.011 (for ``baby.sleep``). I can’t help but notice that the "
"estimated regression coefficient for the ``baby.sleep`` variable is tiny, "
"relative to the value that we get for ``dani.sleep``. Given that these two "
"variables are absolutely on the same scale (they’re both measured in “hours "
"slept”), I find this illuminating. In fact, I’m beginning to suspect that "
"it’s really only the amount of sleep that *I* get that matters in order to "
"predict my grumpiness."
msgstr ""
"*F*-testen som vi nettopp har introdusert, er nyttig for å kontrollere at "
"modellen som helhet presterer bedre enn tilfeldighetene. Hvis "
"regresjonsmodellen din ikke gir et signifikant resultat for *F*-testen, har "
"du sannsynligvis ikke en særlig god regresjonsmodell (eller, muligens, du "
"har ikke særlig gode data). Men selv om det å mislykkes i denne testen er en "
"ganske sterk indikator på at modellen har problemer, betyr ikke det å "
"*passe* testen (dvs. å forkaste nullhypotesen) at modellen er god! Hvorfor "
"er det slik, lurer du kanskje på? Svaret på det finner du ved å se på "
"koeffisientene for den multiple regresjonsmodellen vi allerede har sett på :"
"numref:`tab-parent_coeff` i avsnitt :doc:`Ch12_Regression_05` ovenfor, der "
"koeffisientene vi fikk var 125,966 (for skjæringspunktet, *intercept*), "
"-8,950 (for ``dani.sleep``) og 0,011 (for ``baby.sleep``). Jeg kan ikke "
"unngå å legge merke til at den estimerte regresjonskoeffisienten for "
"variabelen ``baby.sleep`` er bitteliten i forhold til verdien vi får for "
"``dani.sleep``. Gitt at disse to variablene er helt på samme skala (de måles "
"begge i «timer sovet»), synes jeg dette er opplysende. Jeg begynner faktisk "
"å mistenke at det egentlig bare er søvnmengden *jeg* får som betyr noe for å "
"forutsi hvor gretten jeg er."

#: ../../Ch12/Ch12_Regression_07.rst:94
msgid ""
"We can re-use a hypothesis test that we discussed earlier, the *t*-test. The "
"test that we’re interested in has a null hypothesis that the true regression "
"coefficient is zero (*b* = 0), which is to be tested against the alternative "
"hypothesis that it isn’t (*b* ≠ 0). That is:"
msgstr ""
"Vi kan gjenbruke en hypotesetest som vi diskuterte tidligere, *t*-testen. "
"Testen vi er interessert i, har en nullhypotese om at den sanne "
"regresjonskoeffisienten er null (*b* = 0), som skal testes mot "
"alternativhypotesen om at den ikke er det (*b* ≠ 0). Det vil si at:"

#: ../../Ch12/Ch12_Regression_07.rst:100
msgid "H\\ :sub:`0`: *b* = 0"
msgstr "H\\ :sub:`0`: *b* = 0"

#: ../../Ch12/Ch12_Regression_07.rst:101
msgid "H\\ :sub:`1`: *b* ≠ 0"
msgstr "H\\ :sub:`1`: *b* ≠ 0"

#: ../../Ch12/Ch12_Regression_07.rst:103
msgid ""
"How can we test this? Well, if the central limit theorem is kind to us we "
"might be able to guess that the sampling distribution of :math:`\\hat{b}`, "
"the estimated regression coefficient, is a normal distribution with mean "
"centred on *b*. What that would mean is that if the null hypothesis were "
"true, then the sampling distribution of :math:`\\hat{b}` has mean zero and "
"unknown standard deviation. Assuming that we can come up with a good "
"estimate for the standard error of the regression coefficient, :math:"
"`SE(\\hat{b})`, then we’re in luck. That’s *exactly* the situation for which "
"we introduced the one-sample *t*-test way back in chapter :doc:`../Ch11/"
"Ch11_tTest`. So let’s define a *t*-statistic like this:"
msgstr ""
"Hvordan kan vi teste dette? Hvis sentralgrenseteoremet er snilt mot oss, kan "
"vi kanskje gjette at utvalgsfordelingen av :math:`\\hat{b}`, den estimerte "
"regresjonskoeffisienten, er en normalfordeling med gjennomsnitt sentrert på "
"*b*. Det vil si at hvis nullhypotesen er sann, har utvalgsfordelingen av :"
"math:`\\hat{b}` et gjennomsnitt på null og et ukjent standardavvik. Hvis vi "
"antar at vi kan finne et godt estimat for standardfeilen til "
"regresjonskoeffisienten, :math:`SE(\\hat{b})`, så er vi heldige. Det er "
"*nøyaktig* den situasjonen vi introduserte *t*-testen for i kapittel :doc:"
"`../Ch11/Ch11_tTest`. Så la oss definere en *t*-statistikk slik:"

#: ../../Ch12/Ch12_Regression_07.rst:114
msgid ""
"t = \\frac{\\hat{b}}{SE(\\hat{b})}\n"
"\n"
msgstr ""
"t = \\frac{\\hat{b}}{SE(\\hat{b})}\n"
"\n"

#: ../../Ch12/Ch12_Regression_07.rst:116
msgid ""
"I’ll skip over the reasons why, but our degrees of freedom in this case are "
"*df* = *N* - *K* - 1. Irritatingly, the estimate of the standard error of "
"the regression coefficient, :math:`SE(\\hat{b})`, is not as easy to "
"calculate as the standard error of the mean that we used for the simpler *t*-"
"tests in chapter :doc:`../Ch11/Ch11_tTest`. In fact, the formula is somewhat "
"ugly, and not terribly helpful to look at.\\ [#]_ For our purposes it’s "
"sufficient to point out that the standard error of the estimated regression "
"coefficient depends on both the predictor and outcome variables, and it is "
"somewhat sensitive to violations of the homogeneity of variance assumption "
"(discussed shortly)."
msgstr ""
"Jeg skal ikke gå nærmere inn på hvorfor, men frihetsgradene våre i dette "
"tilfellet er *df* = *N* - *K* - 1. Irriterende nok er estimatet av "
"standardfeilen til regresjonskoeffisienten, :math:`SE(\\hat{b})`, ikke like "
"lett å beregne som standardfeilen til gjennomsnittet som vi brukte for de "
"enklere *t*-testene i kapittel :doc:`../Ch11/Ch11_tTest`. Faktisk er "
"formelen litt stygg, og ikke så veldig nyttig å se på.\\ [#]_ For vårt "
"formål er det tilstrekkelig å påpeke at standardfeilen til den estimerte "
"regresjonskoeffisienten avhenger av både prediktor- og utfallsvariablene, og "
"at den er noe følsom for brudd på forutsetningen om varianshomogenitet (som "
"vi skal diskutere om litt senere)."

#: ../../Ch12/Ch12_Regression_07.rst:126
msgid ""
"In any case, this *t*-statistic can be interpreted in the same way as the "
"*t*-statistics that we discussed in chapter :doc:`../Ch11/Ch11_tTest`. "
"Assuming that you have a two-sided alternative (i.e., you don’t really care "
"if *b* > 0 or *b* < 0), then it’s the extreme values of *t* (i.e., a lot "
"less than zero or a lot greater than zero) that suggest that you should "
"reject the null hypothesis."
msgstr ""
"Uansett kan denne *t*-statistikken tolkes på samme måte som *t*-statistikken "
"som vi diskuterte i kapittel :doc:`../Ch11/Ch11_tTest`. Hvis vi antar at du "
"har et tosidig alternativ (dvs. at du egentlig ikke bryr deg om *b* > 0 "
"eller *b* < 0), er det de ekstreme verdiene av *t* (dvs. mye mindre enn null "
"eller mye større enn null) som tilsier at du bør forkaste nullhypotesen."

#: ../../Ch12/Ch12_Regression_07.rst:136
msgid "Running the hypothesis tests in jamovi"
msgstr "Kjører hypotesetestene i jamovi"

#: ../../Ch12/Ch12_Regression_07.rst:138
msgid ""
"To compute all of the statistics that we have talked about so far, all you "
"need to do is make sure the relevant options are checked in jamovi and then "
"run the regression. If we do that, as in :numref:`fig-reg2`, we get a whole "
"bunch of useful output."
msgstr ""
"For å beregne all statistikken vi har snakket om så langt, trenger du bare å "
"sørge for at de relevante alternativene er krysset av i jamovi, og deretter "
"kjøre regresjonen. Hvis vi gjør det, som i :numref:`fig-reg2`, får vi en hel "
"haug med nyttig output."

#: ../../Ch12/Ch12_Regression_07.rst:145
msgid "jamovi screenshot showing a multiple linear regression"
msgstr "jamovi skjermbilde som viser en multippel lineær regresjon"

#: ../../Ch12/Ch12_Regression_07.rst:149
msgid ""
"jamovi screenshot showing a multiple linear regression analysis, with some "
"useful options checked."
msgstr ""
"Skjermbilde fra jamovi som viser en multippel lineær regresjonsanalyse, med "
"noen nyttige alternativer avkrysset."

#: ../../Ch12/Ch12_Regression_07.rst:154
msgid ""
"The ``Model Coefficients`` at the bottom of the jamovi analysis results "
"shown in :numref:`fig-reg2` provides the coefficients of the regression "
"model. Each row in this table refers to one of the coefficients in the "
"regression model. The first row is the intercept term, and the later ones "
"look at each of the predictors. The columns give you all of the relevant "
"information. The first column is the actual estimate of *b* (e.g., 125.97 "
"for the intercept, and -8.95 for the ``dani.sleep`` predictor). The second "
"column is the standard error estimate :math:`\\hat\\sigma_b`. The third and "
"fourth columns provide the lower and upper values for the 95\\% confidence "
"interval around the *b* estimate (more on this later). The fifth column "
"gives you the *t*-statistic, and it’s worth noticing that in this table :"
"math:`t= \\hat{b} / SE(\\hat{b})` every time. Finally, the last column gives "
"you the actual *p*-value for each of these tests.\\ [#]_"
msgstr ""
"Tabellen ``Model Coefficients`` nederst i jamovi-analyseresultatene som "
"vises i :numref:`fig-reg2`, inneholder koeffisientene til "
"regresjonsmodellen. Hver rad i denne tabellen refererer til en av "
"koeffisientene i regresjonsmodellen. Den første raden er skjæringspunktet, "
"og de neste radene tar for seg hver av prediktorene. Kolonnene gir deg all "
"relevant informasjon. Den første kolonnen er det faktiske estimatet av *b* ("
"f.eks. 125,97 for skjæringspunktet og -8,95 for prediktoren ``dani.sleep``). "
"Den andre kolonnen er estimatet for standardfeilen :math:`\\hat\\sigma_b`. "
"Den tredje og fjerde kolonnen gir de nedre og øvre verdiene for 95\\%-"
"konfidensintervall rundt *b*-estimatet (mer om dette senere). Den femte "
"kolonnen gir deg *t*-statistikken, og det er verdt å legge merke til at i "
"denne tabellen er :math:`t= \\hat{b} / SE(\\hat{b})` hver gang. Til slutt, i "
"den siste kolonnen, finner du den faktiske *p*-verdien for hver av disse "
"testene.\\ [#]_"

#: ../../Ch12/Ch12_Regression_07.rst:168
msgid ""
"The only thing that the coefficients table itself doesn’t list is the "
"degrees of freedom used in the *t*-test, which is always *N* - *K* - 1 and "
"is listed in the table at the top of the output, labelled ``Model Fit "
"Measures``. We can see from this table that the model performs significantly "
"better than you’d expect by chance (*F*\\(2,97) = 215.24, *p* < 0.001), "
"which isn’t all that surprising: the *R*\\² = 0.81 value indicate that the "
"regression model accounts for 81\\% of the variability in the outcome "
"measure (and 82\\% for the adjusted *R*\\²). However, when we look back up "
"at the *t*-tests for each of the individual coefficients, we have pretty "
"strong evidence that the ``baby.sleep`` variable has no significant effect. "
"All the work in this model is being done by the ``dani.sleep`` variable. "
"Taken together, these results suggest that this regression model is actually "
"the wrong model for the data. You’d probably be better off dropping the "
"``baby.sleep`` predictor entirely. In other words, the simple regression "
"model that we started with is the better model."
msgstr ""
"Det eneste som ikke fremgår av selve koeffisienttabellen, er frihetsgradene "
"som brukes i *t*-testen, som alltid er *N* - *K* - 1, og som er oppført i "
"tabellen øverst i utgaven, merket ``Model Fit Measures``. Vi kan se av denne "
"tabellen at modellen presterer betydelig bedre enn man skulle forvente ved "
"en tilfeldighet (*F*\\(2,97) = 215,24, *p* < 0,001), noe som ikke er så "
"overraskende: Verdien *R*\\² = 0,81 indikerer at regresjonsmodellen "
"forklarer 81\\% av variasjonen i utfallsmålet (og 82\\% for den justerte "
"*R*\\²). Når vi ser tilbake på *t*-testene for hver av de individuelle "
"koeffisientene, har vi imidlertid ganske sterke bevis for at variabelen "
"``baby.sleep`` ikke har noen signifikant effekt. Alt arbeidet i denne "
"modellen gjøres av variabelen ``dani.sleep``. Til sammen tyder disse "
"resultatene på at denne regresjonsmodellen faktisk er feil modell for "
"dataene. Det er sannsynligvis bedre å droppe prediktoren ``baby.sleep`` "
"helt. Med andre ord er den enkle regresjonsmodellen som vi startet med, den "
"beste modellen."

#: ../../Ch12/Ch12_Regression_07.rst:188
msgid ""
"For advanced readers only. The vector of residuals is :math:`\\epsilon = y - "
"X \\hat{b}`. For *K* predictors plus the intercept, the estimated residual "
"variance is :math:`\\hat\\sigma^2 = \\epsilon^\\prime\\epsilon / (N - K - "
"1)`. The estimated covariance matrix of the coefficients is :math:"
"`\\hat\\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}`, the main diagonal of "
"which is :math:`SE(\\hat{b})`, our estimated standard errors."
msgstr ""
"Kun for avanserte lesere. Vektoren av residuer er :math:`\\epsilon = y - X "
"\\hat{b}`. For *K* prediktorer pluss skjæringspunktet er den estimerte "
"variansen av residuene :math:`\\hat\\sigma^2 = \\epsilon^\\prime\\epsilon / "
"(N - K - 1)`. Den estimerte kovariansmatrisen til koeffisientene er :math:"
"`\\hat\\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}`, der hoveddiagonalen "
"er :math:`SE(\\hat{b})`, våre estimerte standardfeil."

#: ../../Ch12/Ch12_Regression_07.rst:198
msgid ""
"Note that, although jamovi has done multiple tests here, it hasn’t done a "
"Bonferroni correction or anything. These are standard one-sample *t*-tests "
"with a two-sided alternative. If you want to make corrections for multiple "
"tests, you need to do that yourself."
msgstr ""
"Merk at selv om jamovi har utført flere tester her, har den ikke utført en "
"Bonferroni-korreksjon eller noe annet. Dette er standard *t*-tester med ett "
"utvalg og et tosidig alternativ. Hvis du vil korrigere for flere tester, må "
"du gjøre det selv."

#: ../../Ch12/Ch12_Regression_09.rst:4
msgid "Regarding regression coefficients"
msgstr "Når det gjelder regresjonskoeffisienter"

#: ../../Ch12/Ch12_Regression_09.rst:6
msgid ""
"Before moving on to discuss the assumptions underlying linear regression and "
"what you can do to check if they’re being met, there’s two more topics I "
"want to briefly discuss, both of which relate to the regression "
"coefficients. The first thing to talk about is calculating confidence "
"intervals for the coefficients. After that, I’ll discuss the somewhat murky "
"question of how to determine which predictor is most important."
msgstr ""
"Før vi går videre til å diskutere forutsetningene som ligger til grunn for "
"lineær regresjon, og hva du kan gjøre for å sjekke om de er oppfylt, vil jeg "
"kort ta for meg to andre emner, som begge er knyttet til "
"regresjonskoeffisientene. Det første jeg skal snakke om, er beregning av "
"konfidensintervaller for koeffisientene. Deretter vil jeg ta for meg det noe "
"uklare spørsmålet om hvordan man kan avgjøre hvilken prediktor som er "
"viktigst."

#: ../../Ch12/Ch12_Regression_09.rst:14
msgid "Confidence intervals for the coefficients"
msgstr "Konfidensintervaller for koeffisientene"

#: ../../Ch12/Ch12_Regression_09.rst:16
msgid ""
"Like any population parameter, the regression coefficients *b* cannot be "
"estimated with complete precision from a sample of data; that’s part of why "
"we need hypothesis tests. Given this, it’s quite useful to be able to report "
"confidence intervals that capture our uncertainty about the true value of "
"*b*. This is especially useful when the research question focuses heavily on "
"an attempt to find out *how* strongly variable *X* is related to variable "
"*Y*, since in those situations the interest is primarily in the regression "
"weight *b*."
msgstr ""
"Som alle populasjonsparametere kan ikke regresjonskoeffisientene *b* "
"estimeres med full presisjon ut fra et utvalg av data; det er blant annet "
"derfor vi trenger hypotesetester. Derfor er det nyttig å kunne rapportere "
"konfidensintervaller som fanger opp usikkerheten om den sanne verdien av "
"*b*. Dette er spesielt nyttig når forskningsspørsmålet i stor grad fokuserer "
"på å finne ut *hvor* sterkt variabel *X* er relatert til variabel *Y*, siden "
"interessen i slike situasjoner først og fremst er knyttet til "
"regresjonsvekten *b*."

#: ../../Ch12/Ch12_Regression_09.rst:26
msgid ""
"Fortunately, confidence intervals for the regression weights can be "
"constructed in the usual fashion"
msgstr ""
"Heldigvis kan konfidensintervallene for regresjonsvektene beregnes på vanlig "
"måte"

#: ../../Ch12/Ch12_Regression_09.rst:29
msgid ""
"\\mbox{CI}(b) = \\hat{b} \\pm \\left( t_{crit} \\times SE(\\hat{b})  "
"\\right)\n"
"\n"
msgstr ""
"\\mbox{CI}(b) = \\hat{b} \\pm \\left( t_{crit} \\times SE(\\hat{b})  \\right)"
"\n"
"\n"

#: ../../Ch12/Ch12_Regression_09.rst:31
msgid ""
"where :math:`SE(\\hat{b})` is the standard error of the regression "
"coefficient, and *t*\\ :sub:`crit` is the relevant critical value of the "
"appropriate *t*-distribution. For instance, if it’s a 95\\% confidence "
"interval that we want, then the critical value is the 97.5th quantile of a "
"*t* distribution with *N* - *K* - 1 degrees of freedom. In other words, this "
"is basically the same approach to calculating confidence intervals that "
"we’ve used throughout."
msgstr ""
"hvor :math:`SE(\\hat{b})` er standardfeilen til regresjonskoeffisienten, og "
"*t*\\ :sub:`crit` er den relevante kritiske verdien til den aktuelle *t*-"
"fordelingen. Hvis vi for eksempel ønsker et 95\\%-konfidensintervall, er den "
"kritiske verdien den 97,5. kvantilen i en *t*-fordeling med *N* - *K* - 1 "
"frihetsgrader. Med andre ord er dette i bunn og grunn den samme tilnærmingen "
"til beregning av konfidensintervaller som vi har brukt hele tiden."

#: ../../Ch12/Ch12_Regression_09.rst:38
msgid ""
"In jamovi we had already specified the ``95\\% Confidence interval`` as "
"shown in :numref:`fig-reg2`, although we could easily have chosen another "
"value, say a ``99\\% Confidence interval`` if that is what we decided on."
msgstr ""
"I jamovi hadde vi allerede spesifisert ``95\\% Confidence interval`` (95\\%-"
"konfidensinterval) som vist i :numref:`fig-reg2`, selv om vi enkelt kunne ha "
"valgt en annen verdi, for eksempel ``99\\% Confidence interval``, hvis det "
"var det vi bestemte oss for."

#: ../../Ch12/Ch12_Regression_09.rst:43
msgid "Calculating standardised regression coefficients"
msgstr "Beregning av standardiserte regresjonskoeffisienter"

#: ../../Ch12/Ch12_Regression_09.rst:45
msgid ""
"One more thing that you might want to do is to calculate “standardised” "
"regression coefficients, often denoted β. The rationale behind standardised "
"coefficients goes like this. In a lot of situations, your variables are on "
"fundamentally different scales. Suppose, for example, my regression model "
"aims to predict people’s IQ scores using their educational attainment "
"(number of years of education) and their income as predictors. Obviously, "
"educational attainment and income are not on the same scales. The number of "
"years of schooling might only vary by 10s of years, whereas income can vary "
"by 10,000s of dollars (or more). The units of measurement have a big "
"influence on the regression coefficients. The *b* coefficients only make "
"sense when interpreted in light of the units, both of the predictor "
"variables and the outcome variable. This makes it very difficult to compare "
"the coefficients of different predictors. Yet there are situations where you "
"really do want to make comparisons between different coefficients. "
"Specifically, you might want some kind of standard measure of which "
"predictors have the strongest relationship to the outcome. This is what "
"**standardised coefficients** aim to do."
msgstr ""
"En annen ting det kan være lurt å gjøre, er å beregne «standardiserte» "
"regresjonskoeffisienter, ofte betegnet β. Tanken bak standardiserte "
"koeffisienter er som følger. I mange situasjoner er variablene dine på "
"fundamentalt forskjellige skalaer. Anta for eksempel at regresjonsmodellen "
"min tar sikte på å forutsi folks IQ-skårer ved hjelp av utdanningsnivå "
"(antall år med utdanning) og inntekt som prediktorer. Det er klart at "
"utdanningsnivå og inntekt ikke befinner seg på samme skala. Antall år med "
"skolegang varierer kanskje bare med noen titalls år, mens inntekten kan "
"variere med 10 000-talls dollar (eller mer). Måleenhetene har stor "
"innflytelse på regresjonskoeffisientene. *b*-koeffisientene gir bare mening "
"når de tolkes i lys av enhetene, både for prediktorvariablene og "
"utfallsvariabelen. Dette gjør det svært vanskelig å sammenligne "
"koeffisientene til ulike prediktorer. Likevel finnes det situasjoner der man "
"virkelig ønsker å sammenligne ulike koeffisienter. Det kan være at man "
"ønsker et slags standardmål for hvilke prediktorer som har den sterkeste "
"sammenhengen med utfallet. Det er dette **standardiserte koeffisienter** tar "
"sikte på å gjøre."

#: ../../Ch12/Ch12_Regression_09.rst:64
msgid ""
"The basic idea is quite simple; the standardised coefficients are the "
"coefficients that you would have obtained if you’d converted all the "
"variables to *z*-scores before running the regression.\\ [#]_ The idea here "
"is that, by converting all the predictors to *z*-scores, they all go into "
"the regression on the same scale, thereby removing the problem of having "
"variables on different scales. Regardless of what the original variables "
"were, a β value of 1 means that an increase in the predictor of 1 standard "
"deviation will produce a corresponding 1 standard deviation increase in the "
"outcome variable. Therefore, if variable A has a larger absolute value of β "
"than variable B, it is deemed to have a stronger relationship with the "
"outcome. Or at least that’s the idea. It’s worth being a little cautious "
"here, since this does rely very heavily on the assumption that “a 1 standard "
"deviation change” is fundamentally the same kind of thing for all variables. "
"It’s not always obvious that this is true."
msgstr ""
"Den grunnleggende ideen er ganske enkel; de standardiserte koeffisientene er "
"koeffisientene du ville ha fått hvis du hadde konvertert alle variablene til "
"*z*-skårer før du kjørte regresjonen.\\ [#]_ Ideen her er at ved å "
"konvertere alle prediktorene til *z*-skårer, går de alle inn i regresjonen "
"på samme skala, og dermed fjerner du problemet med å ha variabler på ulike "
"skalaer. Uavhengig av hva de opprinnelige variablene var, betyr en β-verdi "
"på 1 at en økning i prediktoren på 1 standardavvik vil gi en tilsvarende "
"økning på 1 standardavvik i utfallsvariabelen. Hvis variabel A har en større "
"absoluttverdi av β enn variabel B, anses den derfor å ha en sterkere "
"sammenheng med utfallet. Det er i hvert fall tanken. Det er verdt å være "
"litt forsiktig her, siden dette i stor grad bygger på en antakelse om at «en "
"endring på 1 standardavvik» i bunn og grunn er det samme for alle variabler. "
"Det er ikke alltid opplagt at dette stemmer."

#: ../../Ch12/Ch12_Regression_09.rst:80
msgid ""
"Leaving aside the interpretation issues, let’s look at how it’s calculated. "
"What you could do is standardise all the variables yourself and then run a "
"regression, but there’s a much simpler way to do it. As it turns out, the β "
"coefficient for a predictor *X* and outcome *Y* has a very simple formula, "
"namely"
msgstr ""
"La oss se på hvordan den beregnes, bortsett fra tolkningsspørsmålene. Du kan "
"standardisere alle variablene selv og deretter kjøre en regresjon, men det "
"finnes en mye enklere måte å gjøre det på. Det viser seg at β-koeffisienten "
"for en prediktor *X* og et utfall *Y* har en veldig enkel formel, nemlig"

#: ../../Ch12/Ch12_Regression_09.rst:86
msgid "β\\ :sub:`X` = *b*\\ :sub:`X` × (σ\\ :sub:`X` / σ\\ :sub:`Y`)"
msgstr "β\\ :sub:`X` = *b*\\ :sub:`X` × (σ\\ :sub:`X` / σ\\ :sub:`Y`)"

#: ../../Ch12/Ch12_Regression_09.rst:88
msgid ""
"where σ\\ :sub:`X` is the standard deviation of the predictor, and σ\\ :sub:"
"`Y` is the standard deviation of the outcome variable *Y*. This makes "
"matters a lot simpler."
msgstr ""
"hvor σ\\ :sub:`X` er standardavviket til prediktoren, og σ\\ :sub:`Y` er "
"standardavviket til utfallsvariabelen *Y*. Dette gjør det hele mye enklere."

#: ../../Ch12/Ch12_Regression_09.rst:92
msgid ""
"To make things even simpler, jamovi has an option that computes the β "
"coefficients for you using the ``Standardized estimate`` checkbox in the "
"``Model Coefficients`` options, see results in :numref:`fig-reg3`."
msgstr ""
"For å gjøre ting enda enklere har jamovi et alternativ som beregner β-"
"koeffisientene for deg ved hjelp av avkrysningsboksen ``Standardized "
"estimate`` i ``Model Coefficients``-alternativene, se resultatene i :numref:"
"`fig-reg3`."

#: ../../Ch12/Ch12_Regression_09.rst:98
msgid "Standardised coefficients with 95\\% confidence intervals"
msgstr "Standardiserte koeffisienter med 95\\%-konfidensintervall"

#: ../../Ch12/Ch12_Regression_09.rst:102
msgid ""
"Standardised coefficients, with 95\\% confidence intervals, for multiple "
"linear regression"
msgstr ""
"Standardiserte koeffisienter, med 95\\%-konfidensintervall, for multippel "
"lineær regresjon"

#: ../../Ch12/Ch12_Regression_09.rst:107
msgid ""
"These results clearly show that the ``dani.sleep`` variable has a much "
"stronger effect than the ``baby.sleep`` variable. However, this is a perfect "
"example of a situation where it would probably make sense to use the "
"original coefficients *b* rather than the standardised coefficients β. After "
"all, my sleep and the baby’s sleep are *already* on the same scale: number "
"of hours slept. Why complicate matters by converting these to *z*-scores?"
msgstr ""
"Disse resultatene viser tydelig at variabelen ``dani.sleep`` har en mye "
"sterkere effekt enn variabelen ``baby.sleep``. Dette er imidlertid et "
"perfekt eksempel på en situasjon der det sannsynligvis ville være fornuftig "
"å bruke de opprinnelige koeffisientene *b* i stedet for de standardiserte "
"koeffisientene β. Når alt kommer til alt, er min søvn og babyens søvn "
"*allerede* på samme skala: antall timer jeg har sovet. Hvorfor komplisere "
"saken ved å konvertere disse til *z*-skårer?"

#: ../../Ch12/Ch12_Regression_09.rst:117
msgid ""
"Strictly, you standardise all the *regressors*. That is, every “thing” that "
"has a regression coefficient associated with it in the model. For the "
"regression models that I’ve talked about so far, each predictor variable "
"maps onto exactly one regressor, and vice versa. However, that’s not "
"actually true in general and we’ll see some examples of this in chapter :doc:"
"`../Ch14/Ch14_ANOVA2`. But, for now we don’t need to care too much about "
"this distinction."
msgstr ""
"Strengt tatt standardiserer du alle *regressorene*. Det vil si alle «ting» "
"som har en regresjonskoeffisient knyttet til seg i modellen. For "
"regresjonsmodellene jeg har snakket om så langt, passer hver "
"prediktorvariabel til nøyaktig én regressor, og omvendt. Det er imidlertid "
"ikke sant generelt, og vi skal se noen eksempler på dette i kapittel :doc:"
"`../Ch14/Ch14_ANOVA2`. Men foreløpig trenger vi ikke å bry oss så mye om "
"dette skillet."

#: ../../Ch12/Ch12_Regression_10.rst:4
msgid "Assumptions of regression"
msgstr "Forutsetninger for regresjon"

#: ../../Ch12/Ch12_Regression_10.rst:6
msgid ""
"The linear regression model that I’ve been discussing relies on several "
"assumptions. In section :doc:`Ch12_Regression_11` we’ll talk a lot more "
"about how to check that these assumptions are being met, but first let’s "
"have a look at each of them."
msgstr ""
"Den lineære regresjonsmodellen som jeg har diskutert, bygger på flere "
"forutsetninger. I avsnitt :doc:`Ch12_Regression_11` skal vi snakke mye mer "
"om hvordan vi kan sjekke at disse forutsetningene er oppfylt, men la oss "
"først ta en titt på hver av dem."

#: ../../Ch12/Ch12_Regression_10.rst:11
msgid ""
"*Normality*. Like many of the models in statistics, basic simple or multiple "
"linear regression relies on an assumption of normality. Specifically, it "
"assumes that the *residuals* are normally distributed. It’s actually okay if "
"the predictors *X* and the outcome *Y* are non-normal, so long as the "
"residuals ε are normal. See section :ref:`Checking the normality of the "
"residuals <checking_normality_residuals>`."
msgstr ""
"*Normalfordeling*. I likhet med mange av modellene i statistikk bygger den "
"enkle eller multiple lineær regresjonen på en antakelse om normalfordeling. "
"Nærmere bestemt forutsettes det at *residuene* er normalfordelte. Det er "
"faktisk greit om prediktorene *X* og utfallet *Y* ikke er normalfordelte, så "
"lenge residuene ε er normalfordelt. Se avsnitt :ref:`Sjekke om residuene er "
"normalfordelt <checking_normality_residuals>`."

#: ../../Ch12/Ch12_Regression_10.rst:18
msgid ""
"*Linearity*. A pretty fundamental assumption of the linear regression model "
"is that the relationship between *X* and *Y* actually is linear! Regardless "
"of whether it’s a simple regression or a multiple regression, we assume that "
"the relationships involved are linear."
msgstr ""
"*Linearitet*. En ganske grunnleggende antakelse i den lineære "
"regresjonsmodellen er at forholdet mellom *X* og *Y* faktisk er lineært! "
"Uansett om det dreier seg om en enkel regresjon eller en multippel "
"regresjon, antar vi at de involverte sammenhengene er lineære."

#: ../../Ch12/Ch12_Regression_10.rst:23
msgid ""
"*Homogeneity of variance*. Strictly speaking, the regression model assumes "
"that each residual ε\\ :sub:`i` is generated from a normal distribution with "
"mean 0, and (more importantly for the current purposes) with a standard "
"deviation σ that is the same for every single residual. In practice, it’s "
"impossible to test the assumption that every residual is identically "
"distributed. Instead, what we care about is that the standard deviation of "
"the residual is the same for all values of *Ŷ*, and (if we’re being "
"especially paranoid) all values of every predictor *X* in the model."
msgstr ""
"*Varianshomogenitet*. Strengt tatt forutsetter regresjonsmodellen at hver "
"residuum ε\\ :sub:`i` er generert fra en normalfordeling med gjennomsnitt 0, "
"og (enda viktigere for dette formålet) med et standardavvik σ som er det "
"samme for hver enkelt residuum. I praksis er det umulig å sjekke "
"forutsetningen om at alle residuene er identisk fordelt. I stedet er vi "
"opptatt av at standardavviket til residuene er det samme for alle verdier av "
"*Ŷ*, og (hvis vi skal være spesielt paranoide) alle verdier av alle "
"prediktorene *X* i modellen."

#: ../../Ch12/Ch12_Regression_10.rst:32
msgid ""
"*Uncorrelated predictors*. The idea here is that, in a multiple regression "
"model, you don’t want your predictors to be too strongly correlated with "
"each other. This isn’t “technically” an assumption of the regression model, "
"but in practice it’s required. Predictors that are too strongly correlated "
"with each other (referred to as “collinearity”) can cause problems when "
"evaluating the model. See section :ref:`Checking for collinearity "
"<checking_collinearity>`."
msgstr ""
"*Ukorrelerte prediktorer*. Tanken her er at du i en multippel "
"regresjonsmodell ikke ønsker at prediktorene skal være for sterkt korrelert "
"med hverandre. Dette er ikke «teknisk sett» en forutsetning for "
"regresjonsmodellen, men i praksis er det et krav. For sterk korrelasjon "
"mellom prediktorene (såkalt «kollinearitet») kan skape problemer når "
"modellen skal evalueres. Se avsnitt :ref:`Sjekk for kollinearitet "
"<checking_collinearity>`."

#: ../../Ch12/Ch12_Regression_10.rst:40
msgid ""
"*Residuals are independent of each other*. This is really just a “catch all” "
"assumption, to the effect that “there’s nothing else funny going on in the "
"residuals”. If there is something weird (e.g., the residuals all depend "
"heavily on some other unmeasured variable) going on, it might screw things "
"up."
msgstr ""
"*Residuene er uavhengige av hverandre*. Dette er egentlig bare en «*catch "
"all*»-antakelse, som går ut på at «det ikke skjer noe annet rart i "
"residuene». Hvis det skjer noe rart (f.eks. at alle residuene er sterkt "
"avhengige av en annen umålt variabel), kan det ødelegge ting."

#: ../../Ch12/Ch12_Regression_10.rst:46
msgid ""
"*No “bad” outliers*. Again, not actually a technical assumption of the model "
"(or rather, it’s sort of implied by all the others), but there is an "
"implicit assumption that your regression model isn’t being too strongly "
"influenced by one or two anomalous data points because this raises questions "
"about the adequacy of the model and the trustworthiness of the data in some "
"cases. See section :ref:`Three kinds of anomalous data <anomalous_data>`."
msgstr ""
"*Ingen «dårlige» ekstremverdier*. Igjen, dette er egentlig ikke en teknisk "
"forutsetning for modellen (eller rettere sagt, det er på en måte "
"underforstått av alle de andre), men det er en implisitt forutsetning at "
"regresjonsmodellen ikke blir for sterkt påvirket av ett eller to avvikende "
"datapunkter, fordi dette i noen tilfeller kan reise spørsmål om modellens "
"egnethet og dataenes troverdighet. Se avsnitt :ref:`Tre typer avvikende data "
"<anomalous_data>`."

#: ../../Ch12/Ch12_Regression_11.rst:4
msgid "Model checking"
msgstr "Modellkontroll"

#: ../../Ch12/Ch12_Regression_11.rst:6
msgid ""
"The main focus of this section is **regression diagnostics**, a term that "
"refers to the art of checking that the assumptions of your regression model "
"have been met, figuring out how to fix the model if the assumptions are "
"violated, and generally to check that nothing “funny” is going on. I refer "
"to this as the “art” of model checking with good reason. It’s not easy, and "
"while there are a lot of fairly standardised tools that you can use to "
"diagnose and maybe even cure the problems that ail your model (if there are "
"any, that is!), you really do need to exercise a certain amount of judgement "
"when doing this. It’s easy to get lost in all the details of checking this "
"thing or that thing, and it’s quite exhausting to try to remember what all "
"the different things are. This has the very nasty side effect that a lot of "
"people get frustrated when trying to learn *all* the tools, so instead they "
"decide not to do *any* model checking. This is a bit of a worry!"
msgstr ""
"Hovedfokuset i denne delen er **regresjonsdiagnostikk**, et begrep som "
"refererer til kunsten å kontrollere at forutsetningene for "
"regresjonsmodellen din er oppfylt, finne ut hvordan du kan fikse modellen "
"hvis forutsetningene brytes, og generelt sjekke at det ikke foregår noe "
"«rart». Det er ikke uten grunn at jeg kaller dette «kunsten» å modellsjekke. "
"Det er ikke lett, og selv om det finnes mange standardiserte verktøy som du "
"kan bruke til å diagnostisere og kanskje til og med kurere problemene med "
"modellen din (hvis det finnes noen!), må du utvise en viss grad av skjønn "
"når du gjør dette. Det er lett å gå seg vill i alle detaljene når man skal "
"sjekke det ene eller det andre, og det er ganske slitsomt å prøve å huske "
"hva alle de forskjellige tingene er. Dette har den ubehagelige bivirkningen "
"at mange blir frustrerte når de prøver å lære seg *alle* verktøyene, og i "
"stedet bestemmer de seg for ikke å gjøre *noen* modellsjekking. Dette er "
"litt bekymringsfullt!"

#: ../../Ch12/Ch12_Regression_11.rst:21
msgid ""
"In this section I describe several different things you can do to check that "
"your regression model is doing what it’s supposed to. It doesn’t cover the "
"full space of things you could do, but it’s still much more detailed than "
"what I see a lot of people doing in practice, and even I don’t usually cover "
"all of this in my intro stats class either. However, I do think it’s "
"important that you get a sense of what tools are at your disposal, so I’ll "
"try to introduce a bunch of them here. Finally, I should note that this "
"section draws quite heavily from :ref:`Fox and Weisberg (2011) <Fox_2011>`, "
"the book associated with the ``car`` package that is used to conduct "
"regression analysis in R. The ``car`` package is notable for providing some "
"excellent tools for regression diagnostics, and the book itself talks about "
"them in an admirably clear fashion. I don’t want to sound too gushy about "
"it, but I do think that :ref:`Fox and Weisberg (2011) <Fox_2011>` is well "
"worth reading, even if some of the advanced diagnostic techniques are only "
"available in R and not jamovi."
msgstr ""
"I denne delen beskriver jeg flere forskjellige ting du kan gjøre for å "
"sjekke at regresjonsmodellen din gjør det den skal. Det dekker ikke hele "
"spekteret av ting du kan gjøre, men det er likevel mye mer detaljert enn det "
"jeg ser mange gjøre i praksis, og jeg pleier heller ikke å gå gjennom alt "
"dette i introduksjonskurset mitt i statistikk. Jeg synes imidlertid at det "
"er viktig at du får en følelse av hvilke verktøy du har til rådighet, så jeg "
"skal prøve å introdusere noen av dem her. Til slutt vil jeg bemerke at denne "
"delen i stor grad bygger på :ref:`Fox and Weisberg (2011) <Fox_2011>`, boken "
"som er knyttet til ``car``-pakken som brukes til å utføre regresjonsanalyse "
"i R. ``car``-pakken er kjent for å tilby noen utmerkede verktøy for "
"regresjonsdiagnostikk, og selve boken omtaler dem på en beundringsverdig "
"klar måte. Jeg vil ikke høres for gushy ut, men jeg synes at :ref:`Fox and "
"Weisberg (2011) <Fox_2011>` er vel verdt å lese, selv om noen av de "
"avanserte diagnostiske teknikkene bare er tilgjengelige i R og ikke i jamovi."

#: ../../Ch12/Ch12_Regression_11.rst:38
msgid "Three kinds of residuals"
msgstr "Tre typer residuer"

#: ../../Ch12/Ch12_Regression_11.rst:40
msgid ""
"The majority of regression diagnostics revolve around looking at the "
"residuals, and by now you’ve probably formed a sufficiently pessimistic "
"theory of statistics to be able to guess that, precisely *because* of the "
"fact that we care a lot about the residuals, there are several different "
"kinds of residual that we might consider. In particular, the following three "
"kinds of residuals are referred to in this section: “ordinary residuals”, "
"“standardised residuals”, and “Studentised residuals”. There is a fourth "
"kind that you’ll see referred to in some of the Figures, and that’s the "
"“Pearson residual”. However, for the models that we’re talking about in this "
"chapter, the Pearson residual is identical to the ordinary residual."
msgstr ""
"De fleste regresjonsdiagnoser dreier seg om å se på residuene, og nå har du "
"sikkert dannet deg en tilstrekkelig pessimistisk teori om statistikk til å "
"kunne gjette at nettopp *på grunn av* det faktum at vi bryr oss mye om "
"residuene, finnes det flere forskjellige typer residuer som vi kan ta i "
"betraktning. I dette avsnittet vil vi særlig referere til følgende tre typer "
"residuer: «ordinære residuer», «standardiserte residuer» og «studentiserte "
"residuer». Det finnes en fjerde type som du vil se referert til i noen av "
"figurene, og det er «Pearson-residuum». For de modellene vi snakker om i "
"dette kapittelet, er imidlertid Pearson-residuumet identisk med det ordinære "
"residuumet."

#: ../../Ch12/Ch12_Regression_11.rst:52
msgid ""
"The first and simplest kind of residuals that we care about are **ordinary "
"residuals**. These are the actual raw residuals that I’ve been talking about "
"throughout this chapter so far. The ordinary residual is just the difference "
"between the fitted value *Ŷ*\\ :sub:`i` and the observed value *Y*\\ :sub:"
"`i`. I’ve been using the notation ε\\ :sub:`i` to refer to the i-th ordinary "
"residual, and by gum I’m going to stick to it. With this in mind, we have "
"the very simple equation:"
msgstr ""
"Den første og enkleste typen residuer vi bryr oss om, er **ordinære "
"residuer**. Dette er de faktiske rå residuene som jeg har snakket om gjennom "
"hele dette kapitlet så langt. Det ordinære residuumet er bare differansen "
"mellom den tilpassede verdien *Ŷ*\\ :sub:`i` og den observerte verdien "
"*Y*\\ :sub:`i`. Jeg har brukt notasjonen ε\\ :sub:`i` for å referere til det "
"i-te ordinære residuumet, og jeg kommer til å holde meg til den. Med dette i "
"bakhodet har vi den veldig enkle ligningen:"

#: ../../Ch12/Ch12_Regression_11.rst:62
msgid ""
"This is of course what we saw earlier, and unless I specifically refer to "
"some other kind of residual, this is the one I’m talking about. So there’s "
"nothing new here. I just wanted to repeat myself. One drawback to using "
"ordinary residuals is that they’re always on a different scale, depending on "
"what the outcome variable is and how good the regression model is. That is, "
"unless you’ve decided to run a regression model without an intercept term, "
"the ordinary residuals will have mean 0 but the variance is different for "
"every regression. In a lot of contexts, especially where you’re only "
"interested in the *pattern* of the residuals and not their actual values, "
"it’s convenient to estimate the **standardised residuals**, which are "
"normalised in such a way as to have standard deviation 1."
msgstr ""
"Dette er selvfølgelig det vi har sett tidligere, og med mindre jeg spesifikt "
"henviser til en annen type residuum, er det denne jeg snakker om. Så det er "
"ikke noe nytt her. Jeg ville bare gjenta meg selv. En ulempe med å bruke "
"vanlige residuer er at de alltid er på en annen skala, avhengig av hva "
"utfallsvariabelen er og hvor god regresjonsmodellen er. Det vil si at med "
"mindre du har bestemt deg for å kjøre en regresjonsmodell uten en term for "
"intercept, vil de ordinære residuene ha gjennomsnitt 0, men variansen er "
"forskjellig for hver regresjon. I mange sammenhenger, spesielt der du bare "
"er interessert i *mønsteret* til residuene og ikke de faktiske verdiene, er "
"det praktisk å estimere **standardiserte residuer**, som er normalisert slik "
"at de har standardavvik 1."

#: ../../Ch12/Ch12_Regression_11.rst:75
msgid ""
"The way we calculate these is to divide the ordinary residual by an estimate "
"of the (population) standard deviation of these residuals. For technical "
"reasons, mumble mumble, the formula for this is:"
msgstr ""
"Måten vi beregner disse på, er å dividere det ordinære residuumet med et "
"estimat av (populasjons-)standardavviket til disse residuene. Av tekniske "
"grunner, mumle mumle, er formelen for dette:"

#: ../../Ch12/Ch12_Regression_11.rst:79
msgid ""
"ε\\ :sub:`i`\\' = :math:`\\frac{\\epsilon_i}{\\hat{\\sigma} \\sqrt{1-h_i}}`"
msgstr ""
"ε\\ :sub:`i`\\' = :math:`\\frac{\\epsilon_i}{\\hat{\\sigma} \\sqrt{1-h_i}}`"

#: ../../Ch12/Ch12_Regression_11.rst:81
msgid ""
"where :math:`\\hat\\sigma` in this context is the estimated population "
"standard deviation of the ordinary residuals, and h\\ :sub:`i` is the “hat "
"value” of the *i*-th observation. I haven’t explained hat values to you yet "
"(but have no fear,\\ [#]_ it’s coming shortly), so this won’t make a lot of "
"sense. For now, it’s enough to interpret the standardised residuals as if "
"we’d converted the ordinary residuals to *z*-scores. In fact, that is more "
"or less the truth, it’s just that we’re being a bit fancier."
msgstr ""
"hvor :math:`\\hat\\sigma` i denne sammenhengen er det estimerte "
"populasjonsstandardavviket til de ordinære residuene, og h\\ :sub:`i` er "
"«hat-verdien» til den *i*-te observasjonen. Jeg har ikke forklart hat-"
"verdier for deg ennå (men vær ikke redd,\\ [#]_ det kommer snart), så dette "
"vil ikke gi så mye mening. Inntil videre holder det å tolke de "
"standardiserte residuene som om vi hadde konvertert de ordinære residuene "
"til *z*-skårer. Det er faktisk mer eller mindre sannheten, det er bare det "
"at vi er litt mer fancy."

#: ../../Ch12/Ch12_Regression_11.rst:89
msgid ""
"The third kind of residuals are **Studentised residuals** (also called "
"“jackknifed residuals”) and they’re even fancier than standardised "
"residuals. Again, the idea is to take the ordinary residual and divide it by "
"some quantity in order to estimate some standardised notion of the residual."
msgstr ""
"Den tredje typen residuer er **Studentiserte residuer** (også kalt "
"«*jackknifed residuals*»), og de er enda mer avanserte enn standardiserte "
"residuer. Igjen er ideen å ta det ordinære residuumet og dividere den med en "
"eller annen størrelse for å estimere et standardisert begrep om residuumet."

#: ../../Ch12/Ch12_Regression_11.rst:94
msgid "The formula for doing the calculations this time is subtly different"
msgstr "Formelen for å gjøre beregningene denne gangen er litt annerledes"

#: ../../Ch12/Ch12_Regression_11.rst:96
msgid ""
"\\epsilon_{i}^* = \\frac{\\epsilon_i}{\\hat{\\sigma}_{(-i)} \\sqrt{1-h_i}}\n"
"\n"
msgstr ""
"\\epsilon_{i}^* = \\frac{\\epsilon_i}{\\hat{\\sigma}_{(-i)} \\sqrt{1-h_i}}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:98
msgid ""
"Notice that our estimate of the standard deviation here is written :math:"
"`\\hat{\\sigma}_{(-i)}`. What this corresponds to is the estimate of the "
"residual standard deviation that you *would have obtained* if you just "
"deleted the i\\ th observation from the data set. This sounds like the sort "
"of thing that would be a nightmare to calculate, since it seems to be saying "
"that you have to run *N* new regression models (even a modern computer might "
"grumble a bit at that, especially if you’ve got a large data set). "
"Fortunately, some terribly clever person has shown that this standard "
"deviation estimate is actually given by the following equation:"
msgstr ""
"Legg merke til at vårt estimat av standardavviket her er skrevet :math:"
"`\\hat{\\sigma}_{(-i)}`. Dette tilsvarer estimatet av standardavviket for "
"residuene som du *ville ha fått* hvis du bare slettet den i\\te "
"observasjonen fra datasettet. Dette høres ut som noe som ville vært et "
"mareritt å beregne, siden det ser ut til å bety at du må kjøre *N* nye "
"regresjonsmodeller (selv en moderne datamaskin vil kanskje murre litt over "
"det, spesielt hvis du har et stort datasett). Heldigvis har en eller annen "
"veldig smart person vist at dette standardavviksestimatet faktisk er gitt av "
"følgende ligning:"

#: ../../Ch12/Ch12_Regression_11.rst:109
msgid ""
"\\hat\\sigma_{(-i)} = \\hat{\\sigma} \\ \\sqrt{\\frac{N-K-1 - {\\epsilon_{i}"
"^\\prime}^2}{N-K-2}}\n"
"\n"
msgstr ""
"\\hat\\sigma_{(-i)} = \\hat{\\sigma} \\ \\sqrt{\\frac{N-K-1 - {\\epsilon_{i}^"
"\\prime}^2}{N-K-2}}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:111
msgid "Isn’t that a pip?"
msgstr "Er ikke det et pip?"

#: ../../Ch12/Ch12_Regression_11.rst:113
msgid ""
"Before moving on, I should point out that you don’t often need to obtain "
"these residuals yourself, even though they are at the heart of almost all "
"regression diagnostics. Most of the time the various options that provide "
"the diagnostics, or assumption checks, will take care of these calculations "
"for you. Even so, it’s always nice to know how to actually get hold of these "
"things yourself in case you ever need to do something non-standard."
msgstr ""
"Før jeg går videre, vil jeg påpeke at du ofte ikke trenger å finne disse "
"residuene selv, selv om de er sentrale i nesten all regresjonsdiagnostikk. "
"Som oftest vil de ulike alternativene som tilbyr diagnostikk eller sjekk av "
"antakelser, ta seg av disse beregningene for deg. Likevel er det alltid "
"greit å vite hvordan du kan få tak i disse tingene selv, i tilfelle du "
"skulle få behov for å gjøre noe som ikke er standard."

#: ../../Ch12/Ch12_Regression_11.rst:124
msgid "Three kinds of anomalous data"
msgstr "Tre typer avvikende data"

#: ../../Ch12/Ch12_Regression_11.rst:126
msgid ""
"One danger that you can run into with linear regression models is that your "
"analysis might be disproportionately sensitive to a smallish number of "
"“unusual” or “anomalous” observations. I discussed this idea previously in "
"subsection :ref:`Using box plots to detect outliers "
"<box_plots_detect_outliers>` in the context of discussing the outliers that "
"get automatically identified by the ``Box plot`` option under "
"``Exploration`` → ``Descriptives``, but this time we need to be much more "
"precise. In the context of linear regression, there are three conceptually "
"distinct ways in which an observation might be called “anomalous”. All three "
"are interesting, but they have rather different implications for your "
"analysis."
msgstr ""
"En fare du kan støte på med lineære regresjonsmodeller, er at analysen kan "
"være uforholdsmessig følsom for et lite antall «uvanlige» eller «avvikende» "
"observasjoner. Jeg har diskutert dette tidligere i underavsnittet :ref:`Bruk "
"av boksplott for å oppdage ekstremverdier <box_plots_detect_outliers>` i "
"forbindelse med diskusjonen av ekstremverdier som automatisk blir "
"identifisert av alternativet ``Box plot`` under ``Exploration`` → "
"``Descriptives``, men denne gangen må vi være mye mer presise. I forbindelse "
"med lineær regresjon er det tre konseptuelt forskjellige måter en "
"observasjon kan kalles «avvikende» på. Alle tre er interessante, men de har "
"ganske ulike implikasjoner for analysen din."

#: ../../Ch12/Ch12_Regression_11.rst:138
msgid ""
"The first kind of unusual observation is an **outlier**. The definition of "
"an outlier (in this context) is an observation that is very different from "
"what the regression model predicts. An example is shown in :numref:`fig-"
"outlier`. In practice, we operationalise this concept by saying that an "
"outlier is an observation that has a very large Studentised residual, ε\\ :"
"sub:`i`\\ :sup:`*`. Outliers are interesting: a big outlier *might* "
"correspond to junk data, e.g., the variables might have been recorded "
"incorrectly in the data set, or some other defect may be detectable. Note "
"that you shouldn’t throw an observation away just because it’s an outlier. "
"But the fact that it’s an outlier is often a cue to look more closely at "
"that case and try to find out why it’s so different."
msgstr ""
"Den første typen uvanlig observasjon er en **outlier**. Definisjonen av en "
"outlier (i denne sammenhengen) er en observasjon som er svært forskjellig "
"fra det regresjonsmodellen forutser. Et eksempel er vist i :numref:`fig-"
"outlier`. I praksis operasjonaliserer vi dette begrepet ved å si at en "
"uteligger er en observasjon som har et svært stor Studentisert residuum, "
"ε\\ :sub:`i`\\ :sup:`*`. Uteliggere er interessante: En stor uteligger *kan* "
"tilsvare søppeldata, f.eks. kan variablene ha blitt registrert feil i "
"datasettet, eller det kan være andre feil som kan påvises. Merk at du ikke "
"bør forkaste en observasjon bare fordi den er en outlier. Men det faktum at "
"det er en outlier, er ofte en god grunn til å se nærmere på det aktuelle "
"tilfellet og prøve å finne ut hvorfor det er så annerledes."

#: ../../Ch12/Ch12_Regression_11.rst:153
msgid "Outliers and their effect"
msgstr "Uteliggere og effekten av dem"

#: ../../Ch12/Ch12_Regression_11.rst:157
msgid ""
"Illustration of outliers: The dotted lines plot the regression line that "
"would have been estimated without the anomalous observation included, and "
"the corresponding residual (i.e., the Studentised residual). The solid line "
"shows the regression line with the anomalous observation included. The "
"outlier has an unusual value on the outcome (y axis location) but not the "
"predictor (x axis location), and lies a long way from the regression line."
msgstr ""
"Illustrasjon av ekstremverdier: De stiplede linjene viser regresjonslinjen "
"som ville ha blitt estimert uten den avvikende observasjonen inkludert, og "
"det tilsvarende residuumet (dvs. det studentiserte residuumet). Den "
"heltrukne linjen viser regresjonslinjen med den unormale observasjonen "
"inkludert. Avvikeren har en uvanlig verdi på utfallet (y-aksens plassering), "
"men ikke på prediktoren (x-aksens plassering), og ligger langt fra "
"regresjonslinjen."

#: ../../Ch12/Ch12_Regression_11.rst:169
msgid "High leverage points and their effect"
msgstr "Høye innflytelsespunkter og deres effekt"

#: ../../Ch12/Ch12_Regression_11.rst:173
msgid ""
"Illustration of high leverage points: The anomalous observation in this case "
"is unusual both in terms of the predictor (x axis) and the outcome (y axis), "
"but this unusualness is highly consistent with the pattern of correlations "
"that exists among the other observations. The observation falls very close "
"to the regression line and does not distort it."
msgstr ""
"Illustrasjon av høye innflytelsespunkter: Den unormale observasjonen i dette "
"tilfellet er uvanlig både når det gjelder prediktoren (x-aksen) og utfallet "
"(y-aksen), men denne uvanligheten stemmer godt overens med det "
"korrelasjonsmønsteret som finnes blant de andre observasjonene. "
"Observasjonen ligger svært nær regresjonslinjen og forvrenger den ikke."

#: ../../Ch12/Ch12_Regression_11.rst:181
msgid ""
"The second way in which an observation can be unusual is if it has high "
"**leverage**, which happens when the observation is very different from all "
"the other observations. This doesn’t necessarily have to correspond to a "
"large residual. If the observation happens to be unusual on all variables in "
"precisely the same way, it can actually lie very close to the regression "
"line. An example of this is shown in :numref:`fig-leverage`. The leverage of "
"an observation is operationalised in terms of its *hat value*, usually "
"written h\\ :sub:`i`. The formula for the hat value is rather complicated,\\ "
"[#]_ but it interpretation is not: h\\ :sub:`i` is a measure of the extent "
"to which the *i*-th observation is “in control” of where the regression line "
"ends up going."
msgstr ""
"Den andre måten en observasjon kan være uvanlig på, er hvis den har høy "
"**leverage**, noe som skjer når observasjonen er svært forskjellig fra alle "
"de andre observasjonene. Dette trenger ikke nødvendigvis å tilsvare et stort "
"residuum. Hvis observasjonen tilfeldigvis er uvanlig på nøyaktig samme måte "
"på alle variablene, kan den faktisk ligge svært nær regresjonslinjen. Et "
"eksempel på dette er vist i :numref:`fig-leverage`. En observasjons "
"innflytelse operasjonaliseres i form av dens *hat-verdi*, vanligvis skrevet "
"h\\ :sub:`i`. Formelen for hat-verdien er ganske komplisert,\\ [#]_ men "
"tolkningen er ikke det: h\\ :sub:`i` er et mål på i hvor stor grad den *i*-"
"te observasjonen har «kontroll» over hvor regresjonslinjen ender opp med å "
"gå."

#: ../../Ch12/Ch12_Regression_11.rst:192
msgid ""
"In general, if an observation lies far away from the other ones in terms of "
"the predictor variables, it will have a large hat value (as a rough guide, "
"high leverage is when the hat value is more than 2 - 3 times the average; "
"and note that the sum of the hat values is constrained to be equal to *K* + "
"1). High leverage points are also worth looking at in more detail, but "
"they’re much less likely to be a cause for concern unless they are also "
"outliers."
msgstr ""
"Generelt vil en observasjon som ligger langt unna de andre observasjonene "
"når det gjelder prediktorvariablene, ha en stor hatverdi (som en grov "
"veiledning er høy gearing når hatverdien er mer enn 2 - 3 ganger "
"gjennomsnittet; og merk at summen av hatverdiene er begrenset til å være lik "
"*K* + 1). Det er også verdt å se nærmere på punkter med høy gearing, men det "
"er mye mindre sannsynlig at de gir grunn til bekymring, med mindre de også "
"er ekstremverdier."

#: ../../Ch12/Ch12_Regression_11.rst:201
msgid "High influence points and their effect"
msgstr "Høye innflytelsespunkter og deres effekt"

#: ../../Ch12/Ch12_Regression_11.rst:205
msgid ""
"Illustration of high influence points: In this case, the anomalous "
"observation is highly unusual on the predictor variable (x axis), and falls "
"a long way from the regression line. As a consequence, the regression line "
"is highly distorted, even though (in this case) the anomalous observation is "
"entirely typical in terms of the outcome variable (y axis)."
msgstr ""
"Illustrasjon av høye innflytelsespunkter: I dette tilfellet er den avvikende "
"observasjonen svært uvanlig på prediktorvariabelen (x-aksen), og faller "
"langt fra regresjonslinjen. Regresjonslinjen blir derfor svært forvrengt, "
"selv om den avvikende observasjonen (i dette tilfellet) er helt typisk når "
"det gjelder utfallsvariabelen (y-aksen)."

#: ../../Ch12/Ch12_Regression_11.rst:213
msgid ""
"This brings us to our third measure of unusualness, the **influence** of an "
"observation. A high influence observation is an outlier that has high "
"leverage. That is, it is an observation that is very different to all the "
"other ones in some respect, and also lies a long way from the regression "
"line. This is illustrated in :numref:`fig-influence`. Notice the contrast to "
"the previous two figures. Outliers don’t move the regression line much and "
"neither do high leverage points. But something that is both an outlier and "
"has high leverage, well that has a big effect on the regression line. That’s "
"why we call these points high influence, and it’s why they’re the biggest "
"worry. We operationalise influence in terms of a measure known as **Cook’s "
"distance**."
msgstr ""
"Dette bringer oss til vårt tredje mål på usedvanlighet, nemlig "
"**innflytelsen** til en observasjon. En observasjon med høy innflytelse er "
"en avvikende observasjon som har høy innflytelse. Det vil si at det er en "
"observasjon som er svært forskjellig fra alle de andre observasjonene i en "
"eller annen henseende, og som også ligger langt fra regresjonslinjen. Dette "
"er illustrert i :numref:`fig-influence`. Legg merke til kontrasten til de to "
"foregående figurene. Uteliggere flytter ikke regresjonslinjen mye, og det "
"gjør heller ikke punkter med høy innflytelse. Men noe som både er en "
"ekstremverdi og har høy innflytelse, har en stor effekt på regresjonslinjen. "
"Det er derfor vi kaller disse punktene for høy innflytelse, og det er derfor "
"de er den største bekymringen. Vi operasjonaliserer innflytelse i form av et "
"mål som kalles **Cooks avstand**."

#: ../../Ch12/Ch12_Regression_11.rst:224
msgid ""
"D_i = \\frac{{\\epsilon_i^*}^2 }{K+1} \\times \\frac{h_i}{1-h_i}\n"
"\n"
msgstr ""
"D_i = \\frac{{\\epsilon_i^*}^2 }{K+1} \\times \\frac{h_i}{1-h_i}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:226
msgid ""
"Notice that this is a multiplication of something that measures the outlier-"
"ness of the observation (the bit on the left), and something that measures "
"the leverage of the observation (the bit on the right)."
msgstr ""
"Legg merke til at dette er en multiplikasjon av noe som måler observasjonens "
"outlier-het (biten til venstre), og noe som måler observasjonens innflytelse "
"(biten til høyre)."

#: ../../Ch12/Ch12_Regression_11.rst:230
msgid ""
"In order to have a large Cook’s distance an observation must be a fairly "
"substantial outlier *and* have high leverage. As a rough guide, Cook’s "
"distance greater than 1 is often considered large (that’s what I typically "
"use as a quick and dirty rule)."
msgstr ""
"For å ha en stor Cook-avstand må en observasjon være en ganske betydelig "
"outlier *og* ha høy innflytelse. Som en grov veiledning regnes ofte Cook-"
"avstand større enn 1 som stor (det er det jeg vanligvis bruker som en rask "
"og skitten regel)."

#: ../../Ch12/Ch12_Regression_11.rst:235
msgid ""
"In jamovi, information about Cook’s distance can be calculated by clicking "
"on the ``Cook’s Distance`` checkbox in the ``Assumption Checks`` → ``Data "
"Summary`` options. When you do this, for the multiple regression model we "
"have been using as an example in this chapter, you get the results as shown "
"in :numref:`fig-reg4`\\."
msgstr ""
"I jamovi kan informasjon om Cooks avstand beregnes ved å klikke på "
"avmerkingsboksen ``Cook's Distance`` under ``Assumption Checks`` → ``Data "
"Summary``. Når du gjør dette for den multiple regresjonsmodellen vi har "
"brukt som eksempel i dette kapittelet, får du resultatene som vist i :numref:"
"`fig-reg4`\\."

#: ../../Ch12/Ch12_Regression_11.rst:243 ../../Ch12/Ch12_Regression_11.rst:247
msgid "jamovi output showing the table for the Cook’s distance statistics"
msgstr "Utgave fra jamovi som viser tabellen for Cook's avstandsstatistikk"

#: ../../Ch12/Ch12_Regression_11.rst:251
msgid ""
"You can see that, in this example, the mean Cook’s distance value is 0.01, "
"and the range is from 0.00000262 to 0.11, so this is some way off the rule "
"of thumb figure mentioned above that a Cook’s distance greater than 1 is "
"considered large."
msgstr ""
"Du kan se at i dette eksempelet er gjennomsnittsverdien for Cook's avstand "
"0,01, og variasjonsbredden er fra 0,00000262 til 0,11, så dette er et stykke "
"unna tommelfingerregelen nevnt ovenfor om at en Cook's avstand større enn 1 "
"regnes som stor."

#: ../../Ch12/Ch12_Regression_11.rst:256
msgid ""
"An obvious question to ask next is, if you do have large values of Cook’s "
"distance what should you do? As always, there’s no hard and fast rule. "
"Probably the first thing to do is to try running the regression with the "
"outlier with the greatest Cook’s distance\\ [#]_ excluded and see what "
"happens to the model performance and to the regression coefficients. If they "
"really are substantially different, it’s time to start digging into your "
"data set and your notes that you no doubt were scribbling as your ran your "
"study. Try to figure out *why* the point is so different. If you start to "
"become convinced that this one data point is badly distorting your results "
"then you might consider excluding it, but that’s less than ideal unless you "
"have a solid explanation for why this particular case is qualitatively "
"different from the others and therefore deserves to be handled separately."
msgstr ""
"Et nærliggende spørsmål å stille seg er hva man bør gjøre hvis man har store "
"verdier av Cooks avstand. Som alltid finnes det ingen fasit. Det første du "
"bør gjøre, er sannsynligvis å prøve å kjøre regresjonen med avvikeren med "
"størst Cook's avstand\\ [#]_ ekskludert, og se hva som skjer med modellens "
"ytelse og regresjonskoeffisientene. Hvis de virkelig er vesentlig "
"forskjellige, er det på tide å begynne å grave i datasettet og notatene "
"dine, som du utvilsomt har skrevet ned mens du kjørte studien. Prøv å finne "
"ut *hvorfor* poenget er så forskjellig. Hvis du begynner å bli overbevist om "
"at dette ene datapunktet forvrenger resultatene dine, kan du vurdere å "
"ekskludere det, men det er ikke ideelt med mindre du har en solid forklaring "
"på hvorfor akkurat dette tilfellet er kvalitativt forskjellig fra de andre "
"og derfor fortjener å bli håndtert separat."

#: ../../Ch12/Ch12_Regression_11.rst:272
msgid "Checking the normality of the residuals"
msgstr "Sjekk om residuene er normalfordelt"

#: ../../Ch12/Ch12_Regression_11.rst:274
msgid ""
"Like many of the statistical tools we’ve discussed in this book, regression "
"models rely on a normality assumption. In this case, we assume that the "
"residuals are normally distributed. The first thing we can do is draw a QQ-"
"plot via the ``Assumption Checks`` → ``Q-Q plot of residuals`` option."
msgstr ""
"I likhet med mange av de statistiske verktøyene vi har diskutert i denne "
"boken, forutsetter regresjonsmodeller en normalfordeling. I dette tilfellet "
"antar vi at residuene er normalfordelte. Det første vi kan gjøre, er å tegne "
"et QQ-plott ved hjelp av å sette opsjonen ``Assumption Checks`` → ``Q-Q plot "
"of residuals``."

#: ../../Ch12/Ch12_Regression_11.rst:279
msgid ""
"The output is shown in :numref:`fig-reg5`, showing the standardised "
"residuals plotted as a function of their theoretical quantiles according to "
"the regression model."
msgstr ""
"Resultatet vises i :numref:`fig-reg5`, som viser plottet med de "
"standardiserte residuene som en funksjon av de teoretiske kvantilene i "
"henhold til regresjonsmodellen."

#: ../../Ch12/Ch12_Regression_11.rst:285
msgid "Quantiles according to the model against standardised residuals"
msgstr "Kvantiler i henhold til modellen mot standardiserte residuer"

#: ../../Ch12/Ch12_Regression_11.rst:289
msgid ""
"Plot of the theoretical quantiles according to the model, against the "
"quantiles of the standardised residuals, produced in jamovi"
msgstr ""
"Plot av de teoretiske kvantilene i henhold til modellen, mot kvantilene til "
"de standardiserte residuene, produsert i jamovi"

#: ../../Ch12/Ch12_Regression_11.rst:294
msgid ""
"Another thing we should check is the relationship between the fitted values "
"and the residuals themselves. We can get jamovi to do this using the "
"``Residuals Plots`` option, which provides a scatterplot for each predictor "
"variable, the outcome variable, and the fitted values against residuals, "
"see :numref:`fig-reg6`. In these plots we are looking for a fairly uniform "
"distribution of “dots”, with no clear bunching or patterning of the “dots”. "
"Looking at these plots, there is nothing particularly worrying as the dots "
"are fairly evenly spread across the whole plot. There may be a little bit of "
"non-uniformity in the right panel, but it is not a strong deviation and "
"probably not worth worrying about."
msgstr ""
"En annen ting vi bør sjekke, er forholdet mellom de tilpassede verdiene og "
"selve residuene. Vi kan få jamovi til å gjøre dette ved hjelp av "
"alternativet ``Residuals Plots``, som gir et spredningsdiagram for hver "
"prediktorvariabel, utfallsvariabelen og de tilpassede verdiene mot "
"residuene, se :numref:`fig-reg6`. I disse plottene ser vi etter en ganske "
"jevn fordeling av «prikker», uten noen tydelig opphopning eller mønster i "
"«prikkene». Når vi ser på disse plottene, er det ikke noe spesielt "
"bekymringsfullt, ettersom prikkene er ganske jevnt fordelt over hele "
"plottet. Det kan være en liten ujevnhet i det høyre panelet, men det er ikke "
"et stort avvik og sannsynligvis ikke verdt å bekymre seg for."

#: ../../Ch12/Ch12_Regression_11.rst:307 ../../Ch12/Ch12_Regression_11.rst:311
msgid "Residuals plots produced in jamovi"
msgstr "Restplott produsert i jamovi"

#: ../../Ch12/Ch12_Regression_11.rst:315
msgid ""
"If we were worried, then in a lot of cases the solution to this problem (and "
"many others) is to transform one or more of the variables. We discussed the "
"basics of variable transformation in the sections :doc:`../Ch06/"
"Ch06_DataHandling_3` and :doc:`../Ch06/Ch06_DataHandling_4`, but I do want "
"to make special note of one additional possibility that I didn’t explain "
"fully earlier: the Box-Cox transform."
msgstr ""
"Hvis vi skulle være bekymret, er løsningen på dette problemet (og mange "
"andre) i mange tilfeller å transformere en eller flere av variablene. Vi "
"diskuterte det grunnleggende om variabeltransformasjon i avsnittene :doc:`../"
"Ch06/Ch06_DataHandling_3` og :doc:`../Ch06/Ch06_DataHandling_4`, men jeg vil "
"gjøre spesielt oppmerksom på en ekstra mulighet som jeg ikke har forklart "
"fullt ut tidligere: Box-Cox-transformasjonen."

#: ../../Ch12/Ch12_Regression_11.rst:324
msgid "The Box-Cox function is a fairly simple one and it’s very widely used."
msgstr "Box-Cox-funksjonen er ganske enkel, og den er svært mye brukt."

#: ../../Ch12/Ch12_Regression_11.rst:326
msgid ""
"f(x,\\lambda) = \\frac{x^\\lambda - 1}{\\lambda}\n"
"\n"
msgstr ""
"f(x,\\lambda) = \\frac{x^\\lambda - 1}{\\lambda}\n"
"\n"

#: ../../Ch12/Ch12_Regression_11.rst:328
msgid ""
"for all values of λ except λ = 0. When λ = 0 we just take the natural "
"logarithm (i.e., *ln*\\(x))."
msgstr ""
"for alle verdier av λ unntatt λ = 0. Når λ = 0, tar vi bare den naturlige "
"logaritmen (dvs. *ln*\\(x))."

#: ../../Ch12/Ch12_Regression_11.rst:331
msgid ""
"You can calculate it using the ``BOXCOX`` function in the ``Compute`` "
"variables screen in jamovi."
msgstr ""
"Du kan beregne den ved hjelp av funksjonen ``BOXCOX`` i skjermbildet "
"``Compute`` for variabler i jamovi."

#: ../../Ch12/Ch12_Regression_11.rst:337
msgid "Checking for collinearity"
msgstr "Sjekker for kollinearitet"

#: ../../Ch12/Ch12_Regression_11.rst:339
msgid ""
"The last kind of regression diagnostic that I’m going to discuss in this "
"chapter is the use of **variance inflation factors** (VIFs), which are "
"useful for determining whether or not the predictors in your regression "
"model are too highly correlated with each other. There is a variance "
"inflation factor associated with each predictor *X*\\ :sub:`k` in the model."
msgstr ""
"Den siste typen regresjonsdiagnostikk som jeg skal diskutere i dette "
"kapittelet, er bruken av **variansinflasjonsfaktorer** (VIF-er), som er "
"nyttige for å avgjøre om prediktorene i regresjonsmodellen din er for høyt "
"korrelert med hverandre eller ikke. Det er en variansinflasjonsfaktor "
"knyttet til hver prediktor *X*\\ :sub:`k` i modellen."

#: ../../Ch12/Ch12_Regression_11.rst:346
msgid "The formula for the k-th VIF is:"
msgstr "Formelen for den k-te VIF er:"

#: ../../Ch12/Ch12_Regression_11.rst:348
msgid "VIF\\ :sub:`k` = 1 / (1 - *R*\\²\\ :sub:`(-k)`\\)"
msgstr "VIF\\ :sub:`k` = 1 / (1 - *R*\\²\\ :sub:`(-k)`\\)"

#: ../../Ch12/Ch12_Regression_11.rst:350
msgid ""
"where *R*\\²\\ :sub:`(-k)` refers to *R*-squared value you would get if you "
"ran a regression using *X*\\ :sub:`k` as the outcome variable, and all the "
"other *X* variables as the predictors. The idea here is that *R*\\²\\ :sub:"
"`(-k)` is a very good measure of the extent to which *X*\\ :sub:`k` is "
"correlated with all the other variables in the model."
msgstr ""
"hvor *R*\\²\\ :sub:`(-k)` refererer til *R*-kvadrat-verdien du ville fått "
"hvis du kjørte en regresjon med *X*\\ :sub:`k` som utfallsvariabel, og alle "
"de andre *X*-variablene som prediktorer. Tanken her er at *R*\\²\\ :sub:`(-"
"k)` er et veldig godt mål på i hvor stor grad *X*\\ :sub:`k` er korrelert "
"med alle de andre variablene i modellen."

#: ../../Ch12/Ch12_Regression_11.rst:356
msgid ""
"The square root of the VIF is pretty interpretable. It tells you how much "
"wider the confidence interval for the corresponding coefficient *b*\\ :sub:"
"`k` is, relative to what you would have expected if the predictors are all "
"nice and uncorrelated with one another. If you’ve only got two predictors, "
"the VIF values are always going to be the same, as we can see if we click on "
"the ``Collinearity`` checkbox in the ``Regression`` → ``Assumption Checks`` "
"options in jamovi. For both ``dani.sleep`` and ``baby.sleep`` the VIF is "
"1.65. And since the square root of 1.65 is 1.28, we see that the correlation "
"between our two predictors isn’t causing much of a problem."
msgstr ""
"Kvadratroten av VIF er ganske tolkbar. Den forteller deg hvor mye bredere "
"konfidensintervallet for den tilsvarende koeffisienten *b*\\ :sub:`k` er, i "
"forhold til hva du ville ha forventet hvis alle prediktorene var like og "
"ukorrelerte med hverandre. Hvis du bare har to prediktorer, vil VIF-verdiene "
"alltid være de samme, som vi kan se hvis vi klikker på avkrysningsboksen "
"``Collinearity`` i ``Regression`` → ``Assumption Checks`` i jamovi. For både "
"``dani.sleep`` og ``baby.sleep`` er VIF 1,65. Og siden kvadratroten av 1,65 "
"er 1,28, ser vi at korrelasjonen mellom de to prediktorene våre ikke er noe "
"stort problem."

#: ../../Ch12/Ch12_Regression_11.rst:367
msgid ""
"To give a sense of how we could end up with a model that has bigger "
"collinearity problems, suppose I were to run a much less interesting "
"regression model, in which I tried to predict the ``day`` on which the data "
"were collected, as a function of all the other variables in the data set. To "
"see why this would be a bit of a problem, let’s have a look at the "
"correlation matrix for all four variables:"
msgstr ""
"For å gi en pekepinn på hvordan vi kan ende opp med en modell som har større "
"kollinearitetsproblemer, kan vi tenke oss at jeg kjører en langt mindre "
"interessant regresjonsmodell, der jeg prøver å forutsi ``day`` dataene ble "
"samlet inn på, som en funksjon av alle de andre variablene i datasettet. For "
"å se hvorfor dette ville være litt problematisk, kan vi ta en titt på "
"korrelasjonsmatrisen for alle de fire variablene:"

#: ../../Ch12/Ch12_Regression_11.rst:382
msgid ""
"We have some fairly large correlations between some of our predictor "
"variables! When we run the regression model and look at the VIF values, we "
"see that the collinearity is causing a lot of uncertainty about the "
"coefficients. First, run the regression, as in :numref:`fig-reg7` and you "
"can see from the VIF values that, yep, that’s some mighty fine collinearity "
"there."
msgstr ""
"Vi har noen ganske store korrelasjoner mellom noen av prediktorvariablene "
"våre! Når vi kjører regresjonsmodellen og ser på VIF-verdiene, ser vi at "
"kollineariteten skaper mye usikkerhet rundt koeffisientene. Kjør først "
"regresjonen, som i :numref:`fig-reg7`, og du kan se fra VIF-verdiene at, "
"jepp, det er en veldig fin kollinearitet der."

#: ../../Ch12/Ch12_Regression_11.rst:390 ../../Ch12/Ch12_Regression_11.rst:394
msgid "Collinearity statistics for multiple regression, produced in jamovi"
msgstr "Kollinearitetsstatistikk for multippel regresjon, produsert i jamovi"

#: ../../Ch12/Ch12_Regression_11.rst:401
msgid "Or have no hope, as the case may be."
msgstr "Eller ikke har noe håp, alt ettersom."

#: ../../Ch12/Ch12_Regression_11.rst:404
msgid ""
"Again, for the linear algebra fanatics: the “hat matrix” is defined to be "
"that matrix **H** that converts the vector of observed values *y* into a "
"vector of fitted values ŷ, such that ŷ = **H**\\ *y*. The name comes from "
"the fact that this is the matrix that “puts a hat on *y*”. The hat *value* "
"of the i-th observation is the i-th diagonal element of this matrix (so "
"technically I should be writing it as h\\ :sub:`ii` rather than h\\ :sub:"
"`i`). Oh, and in case you care, here’s how it’s calculated: **H** = "
"**X**\\(**X**'**X**\\)\\ :sup:`-1` **X**'\\. Pretty, isn’t it?"
msgstr ""
"Igjen, for de lineære algebra-fanatikerne: «Hatmatrisen» er definert som den "
"matrisen **H** som konverterer vektoren av observerte verdier *y* til en "
"vektor av tilpassede verdier ŷ, slik at ŷ = **H**\\ *y*. Navnet kommer av at "
"dette er matrisen som «setter en hatt på *y*». Hat *verdien* av den i-te "
"observasjonen er det i-te diagonalelementet i denne matrisen (så teknisk "
"sett burde jeg skrive det som h\\ :sub:`ii` i stedet for h\\ :sub:`i`). Og i "
"tilfelle du bryr deg, her er hvordan den beregnes: **H** = "
"**X**\\(**X**'**X**\\)\\ :sup:`-1` **X**'\\. Vakkert, ikke sant?"

#: ../../Ch12/Ch12_Regression_11.rst:414
msgid ""
"In order to obtain the Cook’s distance for each participant, open the drop-"
"down menu ``Save`` within the ``Linear Regression`` analysis options and set "
"the check box ``Cook's distance``. A new column containing Cook’s distances "
"will be added at the end of your data set. Those values can then be used in "
"connection with a :doc:`filter <../Ch06/Ch06_DataHandling_5>` to select "
"participants."
msgstr ""
"For å få Cooks avstand for hver deltaker, åpner du rullegardinmenyen "
"``Save`` i analysealternativene ``Linear Regression`` og merker av i "
"avmerkingsboksen ``Cook's distance``. En ny kolonne med Cooks avstander vil "
"bli lagt til på slutten av datasettet. Disse verdiene kan deretter brukes i "
"forbindelse med et :doc:`filter <../Ch06/Ch06_DataHandling_5>` for å velge "
"ut deltakere."

#: ../../Ch12/Ch12_Regression_12.rst:4
msgid "Model selection"
msgstr "Valg av modell"

#: ../../Ch12/Ch12_Regression_12.rst:6
msgid ""
"One fairly major problem that remains is the problem of “model selection”. "
"That is, if we have a data set that contains several variables, which ones "
"should we include as predictors, and which ones should we not include? In "
"other words, we have a problem of **variable selection**. In general, model "
"selection is a complex business but it’s made somewhat simpler if we "
"restrict ourselves to the problem of choosing a subset of the variables that "
"ought to be included in the model. Nevertheless, I’m not going to try "
"covering even this reduced topic in a lot of detail. Instead, I’ll talk "
"about two broad principles that you need to think about, and then discuss "
"one concrete tool that jamovi provides to help you select a subset of "
"variables to include in your model. First, the two principles:"
msgstr ""
"Et ganske stort problem som gjenstår, er problemet med «modellvalg». Det vil "
"si at hvis vi har et datasett som inneholder flere variabler, hvilke skal vi "
"inkludere som prediktorer, og hvilke skal vi ikke inkludere? Vi har med "
"andre ord et problem med **variabelutvelgelse**. Generelt er "
"modellutvelgelse en kompleks affære, men det blir noe enklere hvis vi "
"begrenser oss til problemet med å velge en delmengde av de variablene som "
"bør inkluderes i modellen. Jeg skal likevel ikke forsøke å dekke selv dette "
"begrensede temaet i detalj. I stedet vil jeg snakke om to overordnede "
"prinsipper som du bør tenke på, og deretter diskutere et konkret verktøy som "
"jamovi tilbyr for å hjelpe deg med å velge ut en delmengde av variabler som "
"skal inkluderes i modellen din. Først de to prinsippene:"

#: ../../Ch12/Ch12_Regression_12.rst:19
msgid ""
"It’s nice to have an actual substantive basis for your choices. That is, in "
"a lot of situations you the researcher have good reasons to pick out a "
"smallish number of possible regression models that are of theoretical "
"interest. These models will have a sensible interpretation in the context of "
"your field. Never discount the importance of this. Statistics serves the "
"scientific process, not the other way around."
msgstr ""
"Det er fint å ha et faktisk substansielt grunnlag for valgene dine. Det vil "
"si at i mange situasjoner har du som forsker gode grunner til å velge ut et "
"lite antall mulige regresjonsmodeller som er av teoretisk interesse. Disse "
"modellene vil ha en fornuftig tolkning i konteksten av ditt fagfelt. Du må "
"aldri undervurdere betydningen av dette. Statistikk tjener den "
"vitenskapelige prosessen, ikke omvendt."

#: ../../Ch12/Ch12_Regression_12.rst:27
msgid ""
"To the extent that your choices rely on statistical inference, there is a "
"trade off between simplicity and goodness of fit. As you add more predictors "
"to the model you make it more complex. Each predictor adds a new free "
"parameter (i.e., a new regression coefficient), and each new parameter "
"increases the model’s capacity to “absorb” random variations. So the "
"goodness of fit (e.g., *R*\\²) continues to rise, sometimes trivially or by "
"chance, as you add more predictors no matter what. If you want your model to "
"be able to generalise well to new observations you need to avoid throwing in "
"too many variables."
msgstr ""
"I den grad valgene dine baserer seg på statistisk inferens, er det en "
"avveining mellom enkelhet og god tilpasning. Når du legger til flere "
"prediktorer i modellen, blir den mer kompleks. Hver prediktor tilfører en ny "
"fri parameter (dvs. en ny regresjonskoeffisient), og hver nye parameter øker "
"modellens evne til å «absorbere» tilfeldige variasjoner. Så "
"tilpasningsdyktigheten (f.eks. *R*\\²) fortsetter å øke, noen ganger "
"trivielt eller tilfeldig, etter hvert som du legger til flere prediktorer, "
"uansett hva som skjer. Hvis du vil at modellen skal kunne generaliseres godt "
"til nye observasjoner, må du unngå å legge inn for mange variabler."

#: ../../Ch12/Ch12_Regression_12.rst:37
msgid ""
"This latter principle is often referred to as **Ockham’s razor** and is "
"often summarised in terms of the following pithy saying: *do not multiply "
"entities beyond necessity*. In this context, it means don’t chuck in a bunch "
"of largely irrelevant predictors just to boost your *R*\\². Hmm. Yeah, the "
"original was better."
msgstr ""
"Dette sistnevnte prinsippet omtales ofte som **Ockhams barberkniv**, og "
"oppsummeres gjerne i følgende fyndord: * Ikke multipliser enheter utover det "
"som er nødvendig*. I denne sammenhengen betyr det at du ikke må legge inn en "
"haug med stort sett irrelevante prediktorer bare for å øke *R*\\². Ja, "
"originalen var bedre. Det virkelig var bedre."

#: ../../Ch12/Ch12_Regression_12.rst:43
msgid ""
"In any case, what we need is an actual mathematical criterion that will "
"implement the qualitative principle behind Ockham’s razor in the context of "
"selecting a regression model. As it turns out there are several "
"possibilities. The one that I’ll talk about is the **Akaike information "
"criterion** (AIC; :ref:`Akaike, 1974 <Akaike_1974>`) simply because it’s "
"available as an option in jamovi."
msgstr ""
"Uansett trenger vi et faktisk matematisk kriterium som kan implementere det "
"kvalitative prinsippet bak Ockhams barberkniv i forbindelse med valg av "
"regresjonsmodell. Det viser seg at det finnes flere muligheter. Den jeg vil "
"snakke om, er **Akaike informasjonskriterium** (AIC; :ref:`Akaike, 1974 "
"<Akaike_1974>`), rett og slett fordi det er tilgjengelig som et alternativ i "
"jamovi."

#: ../../Ch12/Ch12_Regression_12.rst:50
msgid ""
"In the context of a linear regression model (and ignoring terms that don’t "
"depend on the model in any way!), the AIC for a model that has *K* predictor "
"variables plus an intercept is"
msgstr ""
"I forbindelse med en lineær regresjonsmodell (og uten å ta hensyn til termer "
"som ikke avhenger av modellen på noen måte!), er AIC for en modell som har "
"*K* prediktorvariabler pluss et skjæringspunkt"

#: ../../Ch12/Ch12_Regression_12.rst:54
msgid ""
"\\mbox{AIC} = \\displaystyle\\frac{\\mbox{SS}_{res}}{\\hat{\\sigma}^2} + 2K\n"
"\n"
msgstr ""
"\\mbox{AIC} = \\displaystyle\\frac{\\mbox{SS}_{res}}{\\hat{\\sigma}^2} + 2K\n"
"\n"

#: ../../Ch12/Ch12_Regression_12.rst:56
msgid ""
"The smaller the AIC value, the better the model performance. If we ignore "
"the low level details it’s fairly obvious what the AIC does. On the left we "
"have a term that increases as the model predictions get worse; on the right "
"we have a term that increases as the model complexity increases. The best "
"model is the one that fits the data well (low residuals, left hand side) "
"using as few predictors as possible (low K, right hand side). In short, this "
"is a simple implementation of Ockham’s razor."
msgstr ""
"Jo mindre AIC-verdien er, desto bedre er modellens ytelse. Hvis vi ser bort "
"fra detaljene, er det ganske åpenbart hva AIC gjør. Til venstre har vi en "
"term som øker når modellens prediksjoner blir dårligere, og til høyre har vi "
"en term som øker når modellens kompleksitet øker. Den beste modellen er den "
"som passer dataene godt (lave residuer, venstre side) ved hjelp av så få "
"prediktorer som mulig (lav K, høyre side). Kort sagt er dette en enkel "
"implementering av Ockhams barberkniv."

#: ../../Ch12/Ch12_Regression_12.rst:65
msgid ""
"AIC can be added to the ``Model Fit Measures`` output Table when the ``AIC`` "
"checkbox is clicked, and a rather clunky way of assessing different models "
"is seeing if the ``AIC`` value is lower if you remove one or more of the "
"predictors in the regression model. This is the only way currently "
"implemented in jamovi, but there are alternatives in other more powerful "
"programmes, such as R. These alternative methods can automate the process of "
"selectively removing (or adding) predictor variables to find the best AIC. "
"Although these methods are not implemented in jamovi, I will mention them "
"briefly below just so you know about them."
msgstr ""
"AIC kan legges til i resultattabellen ``Model Fit Measures`` (mål for "
"modeltilpasning) når du klikker på avmerkingsboksen ``AIC``, og en ganske "
"klønete måte å vurdere ulike modeller på er å se om ``AIC``-verdien blir "
"lavere hvis du fjerner en eller flere av prediktorene i regresjonsmodellen. "
"Dette er den eneste måten som for øyeblikket er implementert i jamovi, men "
"det finnes alternativer i andre kraftigere programmer, for eksempel R. Disse "
"alternative metodene kan automatisere prosessen med å selektivt fjerne "
"(eller legge til) prediktorvariabler for å finne den beste AIC-verdien. Selv "
"om disse metodene ikke er implementert i jamovi, vil jeg nevne dem kort "
"nedenfor, bare så du vet om dem."

#: ../../Ch12/Ch12_Regression_12.rst:77
msgid "Backward elimination"
msgstr "Eliminering bakover i tid"

#: ../../Ch12/Ch12_Regression_12.rst:79
msgid ""
"In backward elimination you start with the complete regression model, "
"including all possible predictors. Then, at each “step” we try all possible "
"ways of removing one of the variables, and whichever of these is best (in "
"terms of lowest AIC value) is accepted. This becomes our new regression "
"model, and we then try all possible deletions from the new model, again "
"choosing the option with lowest AIC. This process continues until we end up "
"with a model that has a lower AIC value than any of the other possible "
"models that you could produce by deleting one of its predictors."
msgstr ""
"I baklengs eliminering starter man med den komplette regresjonsmodellen, "
"inkludert alle mulige prediktorer. Deretter prøver vi alle mulige måter å "
"fjerne en av variablene på i hvert «trinn», og den av disse som er best (i "
"form av lavest AIC-verdi), blir akseptert. Dette blir vår nye "
"regresjonsmodell, og vi prøver deretter alle mulige strykninger fra den nye "
"modellen, og velger igjen alternativet med lavest AIC. Denne prosessen "
"fortsetter helt til vi ender opp med en modell som har en lavere AIC-verdi "
"enn noen av de andre mulige modellene som du kan produsere ved å slette en "
"av prediktorene."

#: ../../Ch12/Ch12_Regression_12.rst:90
msgid "Forward selection"
msgstr "Fremovervalg"

#: ../../Ch12/Ch12_Regression_12.rst:92
msgid ""
"As an alternative, you can also try **forward selection**. This time around "
"we start with the smallest possible model as our start point, and only "
"consider the possible additions to the model. However, there’s one "
"complication. You also need to specify what the largest possible model "
"you’re willing to entertain is."
msgstr ""
"Som et alternativ kan du også prøve **forward selection**. Denne gangen "
"starter vi med den minste mulige modellen som utgangspunkt, og tar bare "
"hensyn til de mulige tilleggene til modellen. Det er imidlertid en "
"komplikasjon. Du må også spesifisere hva som er den største mulige modellen "
"du er villig til å godta."

#: ../../Ch12/Ch12_Regression_12.rst:98
msgid ""
"Although backward and forward selection can lead to the same conclusion, "
"they don’t always."
msgstr ""
"Selv om baklengs og forlengs seleksjon kan føre til samme konklusjon, gjør "
"de ikke alltid det."

#: ../../Ch12/Ch12_Regression_12.rst:102
msgid "A caveat"
msgstr "En advarsel"

#: ../../Ch12/Ch12_Regression_12.rst:104
msgid ""
"Automated variable selection methods are seductive things, especially when "
"they’re bundled up in (fairly) simple functions in powerful statistical "
"programmes. They provide an element of objectivity to your model selection, "
"and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse "
"for thoughtlessness. No longer do you have to think carefully about which "
"predictors to add to the model and what the theoretical basis for their "
"inclusion might be. Everything is solved by the magic of AIC. And if we "
"start throwing around phrases like Ockham’s razor, well it sounds like "
"everything is wrapped up in a nice neat little package that no-one can argue "
"with."
msgstr ""
"Automatiserte metoder for variabelutvelgelse er forførende, spesielt når de "
"er pakket inn i (ganske) enkle funksjoner i kraftige statistikkprogrammer. "
"De gir et element av objektivitet til modellvalget, og det er jo ganske "
"fint. Dessverre blir de noen ganger brukt som en unnskyldning for "
"tankeløshet. Du trenger ikke lenger å tenke nøye gjennom hvilke prediktorer "
"du skal legge til i modellen, og hva som er det teoretiske grunnlaget for å "
"inkludere dem. Alt løses ved hjelp av magien i AIC. Og hvis vi begynner å "
"slenge rundt oss med uttrykk som Ockhams barberkniv, høres det ut som om alt "
"er pakket inn i en fin liten pakke som ingen kan argumentere mot."

#: ../../Ch12/Ch12_Regression_12.rst:115
msgid ""
"Or, perhaps not. Firstly, there’s very little agreement on what counts as an "
"appropriate model selection criterion. When I was taught backward "
"elimination as an undergraduate, we used *F*-tests to do it, because that "
"was the default method used by the software. I’ve described using AIC, and "
"since this is an introductory text that’s the only method I’ve described, "
"but the AIC is hardly the Word of the Gods of Statistics. It’s an "
"approximation, derived under certain assumptions, and it’s guaranteed to "
"work only for large samples when those assumptions are met. Alter those "
"assumptions and you get a different criterion, like the BIC for instance "
"(also available in jamovi). Take a different approach again and you get the "
"NML criterion. Decide that you’re a Bayesian and you get model selection "
"based on posterior odds ratios. Then there are a bunch of regression "
"specific tools that I haven’t mentioned. And so on. All of these different "
"methods have strengths and weaknesses, and some are easier to calculate than "
"others (AIC is probably the easiest of the lot, which might account for its "
"popularity). Almost all of them produce the same answers when the answer is "
"“obvious” but there’s a fair amount of disagreement when the model selection "
"problem becomes hard."
msgstr ""
"Eller kanskje ikke. For det første er det svært liten enighet om hva som er "
"et passende modellvalgkriterium. Da jeg ble undervist i baklengs eliminasjon "
"som student, brukte vi *F*-tester for å gjøre det, fordi det var "
"standardmetoden som ble brukt av programvaren. Jeg har beskrevet bruk av "
"AIC, og siden dette er en introduksjonstekst, er det den eneste metoden jeg "
"har beskrevet, men AIC er neppe statistikkgudenes ord. Det er en tilnærming, "
"utledet under visse forutsetninger, og den fungerer garantert bare for store "
"utvalg når disse forutsetningene er oppfylt. Hvis du endrer disse "
"forutsetningene, får du et annet kriterium, som for eksempel BIC (også "
"tilgjengelig i jamovi). Hvis du velger en annen tilnærming igjen, får du NML-"
"kriteriet. Hvis du bestemmer deg for å være bayesianer, får du modellvalg "
"basert på posterior odds ratio. Så finnes det en rekke regresjonsspesifikke "
"verktøy som jeg ikke har nevnt. Og så videre. Alle disse ulike metodene har "
"styrker og svakheter, og noen er enklere å beregne enn andre (AIC er "
"sannsynligvis den enkleste av dem alle, noe som kan forklare dens "
"popularitet). Nesten alle gir de samme svarene når svaret er «opplagt», men "
"det er en god del uenighet når modellvalgproblemet blir vanskelig."

#: ../../Ch12/Ch12_Regression_12.rst:135
msgid ""
"What does this mean in practice? Well, you *could* go and spend several "
"years teaching yourself the theory of model selection, learning all the ins "
"and outs of it so that you could finally decide on what you personally think "
"the right thing to do is. Speaking as someone who actually did that, I "
"wouldn’t recommend it. You’ll probably come out the other side even more "
"confused than when you started. A better strategy is to show a bit of common "
"sense. If you’re staring at the results of an automated backwards or "
"forwards selection procedure, and the model that makes sense is close to "
"having the smallest AIC but is narrowly defeated by a model that doesn’t "
"make any sense, then trust your instincts. Statistical model selection is an "
"inexact tool, and as I said at the beginning, *interpretability matters*."
msgstr ""
"Hva betyr dette i praksis? Vel, du *kan* bruke flere år på å lære deg selv "
"teorien om modellvalg, lære deg alle detaljene i det, slik at du til slutt "
"kan bestemme deg for hva du personlig mener er det riktige å gjøre. Jeg vil "
"ikke anbefale det, selv om jeg faktisk har gjort det. Du vil sannsynligvis "
"komme ut på den andre siden enda mer forvirret enn da du startet. En bedre "
"strategi er å vise litt sunn fornuft. Hvis du ser på resultatene av en "
"automatisert prosedyre for å velge baklengs eller forlengs modell, og den "
"modellen som gir mening, er nær ved å ha den minste AIC-verdien, men blir "
"knepent slått av en modell som ikke gir noen mening, bør du stole på "
"instinktene dine. Statistisk modellvalg er et unøyaktig verktøy, og som jeg "
"sa innledningsvis, er *tolkbarhet viktig*."

#: ../../Ch12/Ch12_Regression_12.rst:149
msgid "Comparing two regression models"
msgstr "Sammenligning av to regresjonsmodeller"

#: ../../Ch12/Ch12_Regression_12.rst:151
msgid ""
"An alternative to using automated model selection procedures is for the "
"researcher to explicitly select two or more regression models to compare to "
"each other. You can do this in a few different ways, depending on what "
"research question you’re trying to answer. Suppose we want to know whether "
"or not the amount of sleep that my son got has any relationship to my "
"grumpiness, over and above what we might expect from the amount of sleep "
"that I got. We also want to make sure that the day on which we took the "
"measurement has no influence on the relationship. That is, we’re interested "
"in the relationship between ``baby.sleep`` and ``dani.grump``, and from that "
"perspective ``dani.sleep`` and ``day`` are nuisance variable or "
"**covariates** that we want to control for. In this situation, what we would "
"like to know is whether ``dani.grump ~ dani.sleep + day + baby.sleep`` "
"(which I’ll call Model 2, or ``M2``) is a better regression model for these "
"data than ``dani.grump ~ dani.sleep + day`` (which I’ll call Model 1, or "
"``M1``). There are two different ways we can compare these two models, one "
"based on a model selection criterion like AIC, and the other based on an "
"explicit hypothesis test. I’ll show you the AIC based approach first because "
"it’s simpler, and follows naturally from discussion in the last section. The "
"first thing I need to do is actually run the two regressions, note the AIC "
"for each one, and then select the model with the smaller AIC value as it is "
"judged to be the better model for these data. Actually, don’t do this just "
"yet. Read on because there is an easy way in jamovi to get the AIC values "
"for different models included in one table.\\ [#]_"
msgstr ""
"Et alternativ til å bruke automatiserte modellvalgprosedyrer er at forskeren "
"eksplisitt velger ut to eller flere regresjonsmodeller som skal sammenlignes "
"med hverandre. Dette kan gjøres på flere forskjellige måter, avhengig av "
"hvilket forskningsspørsmål du prøver å besvare. Anta at vi ønsker å finne ut "
"om søvnmengden til sønnen min har noen sammenheng med min grettenhet, utover "
"det vi kan forvente ut fra søvnmengden til meg. Vi vil også forsikre oss om "
"at dagen vi foretok målingen, ikke har noen innvirkning på forholdet. Det "
"vil si at vi er interessert i forholdet mellom ``baby.sleep`` og ``dani."
"grump``, og fra det perspektivet er ``dani.sleep`` og ``day`` uvedkommende "
"variabler eller **kovariater** som vi ønsker å kontrollere for. I denne "
"situasjonen vil vi gjerne vite om ``dani.grump ~ dani.sleep + day + baby."
"sleep`` (som jeg vil kalle modell 2, eller ``M2``) er en bedre "
"regresjonsmodell for disse dataene enn ``dani.grump ~ dani.sleep + day`` "
"(som jeg vil kalle modell 1, eller ``M1``). Det finnes to ulike måter vi kan "
"sammenligne disse to modellene på, den ene basert på et modellvalgkriterium "
"som AIC, og den andre basert på en eksplisitt hypotesetest. Jeg skal vise "
"deg den AIC-baserte tilnærmingen først, fordi den er enklere og følger "
"naturlig av diskusjonen i forrige avsnitt. Det første jeg må gjøre, er å "
"kjøre de to regresjonene, notere AIC for hver av dem, og deretter velge "
"modellen med den laveste AIC-verdien, ettersom den vurderes å være den beste "
"modellen for disse dataene. Men ikke gjør dette ennå. Les videre, for det "
"finnes en enkel måte i jamovi å få AIC-verdiene for ulike modeller inkludert "
"i én tabell.\\ [#]_"

#: ../../Ch12/Ch12_Regression_12.rst:177
msgid ""
"A somewhat different approach to the problem comes out of the hypothesis "
"testing framework. Suppose you have two regression models, where one of them "
"(Model 1) contains a *subset* of the predictors from the other one (Model "
"2). That is, Model 2 contains all of the predictors included in Model 1, "
"plus one or more additional predictors. When this happens we say that Model "
"1 is **nested** within Model 2, or possibly that Model 1 is a **submodel** "
"of Model 2. Regardless of the terminology, what this means is that we can "
"think of Model 1 as a null hypothesis and Model 2 as an alternative "
"hypothesis. And in fact we can construct an *F* test for this in a fairly "
"straightforward fashion."
msgstr ""
"En noe annen tilnærming til problemet kommer fra rammeverket for "
"hypotesetesting. Anta at du har to regresjonsmodeller, der den ene (modell "
"1) inneholder et *subset* av prediktorene fra den andre (modell 2). Det vil "
"si at modell 2 inneholder alle prediktorene som inngår i modell 1, pluss én "
"eller flere ekstra prediktorer. Når dette skjer, sier vi at modell 1 er "
"**innkapslet** i modell 2, eller eventuelt at modell 1 er en **submodell** "
"av modell 2. Uansett terminologi betyr dette at vi kan tenke på modell 1 som "
"en nullhypotese og modell 2 som en alternativhypotese. Og vi kan faktisk "
"lage en *F*-test for dette på en ganske enkel måte."

#: ../../Ch12/Ch12_Regression_12.rst:188
msgid ""
"We can fit both models to the data and obtain a residual sum of squares for "
"both models. I’ll denote these as SS\\ :sub:`res`\\ :sup:`(1)` and SS\\ :sub:"
"`res`\\ :sup:`(2)` respectively. The superscripting here just indicates "
"which model we’re talking about. Then our *F*-statistic is"
msgstr ""
"Vi kan tilpasse begge modellene til dataene og få en kvadratsumme for de "
"residuene i begge modeller. Jeg vil betegne disse som henholdsvis SS\\ :sub:"
"`res`\\ :sup:`(1)` og SS\\ :sub:`res`\\ :sup:`(2)`. Overskriftene her "
"indikerer bare hvilken modell vi snakker om. Da er *F*-statistikken vår"

#: ../../Ch12/Ch12_Regression_12.rst:194
msgid ""
"F = \\frac{(\\mbox{SS}_{res}^{(1)} - \\mbox{SS}_{res}^{(1)})/k}{(\\mbox{SS}"
"_{res}^{(2)})/(N-p-1)}\n"
"\n"
msgstr ""
"F = \\frac{(\\mbox{SS}_{res}^{(1)} - \\mbox{SS}_{res}^{(1)})/k}{(\\mbox"
"{SS}_{res}^{(2)})/(N-p-1)}\n"
"\n"

#: ../../Ch12/Ch12_Regression_12.rst:196
msgid ""
"where *N* is the number of observations, *p* is the number of predictors in "
"the full model (not including the intercept), and *k* is the difference in "
"the number of parameters between the two models.\\ [#]_ The degrees of "
"freedom here are *k* and *N* - *p* - 1. Note that it’s often more convenient "
"to think about the difference between those two SS values as a sum of "
"squares in its own right. That is"
msgstr ""
"der *N* er antall observasjoner, *p* er antall prediktorer i den "
"fullstendige modellen (ikke inkludert skjæringspunktet), og *k* er "
"forskjellen i antall parametere mellom de to modellene.\\ [#]_ "
"Frihetsgradene her er *k* og *N* - *p* - 1. Merk at det ofte er mer praktisk "
"å tenke på forskjellen mellom disse to SS-verdiene som en egen kvadratsum. "
"Det vil si"

#: ../../Ch12/Ch12_Regression_12.rst:204
msgid ""
"SS\\ :sub:`Δ` = SS\\ :sub:`res`\\ :sup:`(1)` - SS\\ :sub:`res`\\ :sup:`(2)`"
msgstr ""
"SS\\ :sub:`Δ` = SS\\ :sub:`res`\\ :sup:`(1)` - SS\\ :sub:`res`\\ :sup:`(2)`"

#: ../../Ch12/Ch12_Regression_12.rst:206
msgid ""
"The reason why this is helpful is that we can express SS\\ :sub:`Δ` as a "
"measure of the extent to which the two models make different predictions "
"about the the outcome variable. Specifically,"
msgstr ""
"Grunnen til at dette er nyttig, er at vi kan uttrykke SS\\ :sub:`Δ` som et "
"mål på i hvilken grad de to modellene gir ulike prediksjoner om "
"utfallsvariabelen. Nærmere bestemt,"

#: ../../Ch12/Ch12_Regression_12.rst:211
msgid ""
"SS\\ :sub:`Δ` = :math:`\\sum_{i} \\left(\\hat{y}_i^{(2)} - \\hat{y}_i^{(1)} "
"\\right)^2`"
msgstr ""
"SS\\ :sub:`Δ` = :math:`\\sum_{i} \\left(\\hat{y}_i^{(2)} - \\hat{y}_i^{(1)} "
"\\right)^2`"

#: ../../Ch12/Ch12_Regression_12.rst:213
msgid ""
"where *ŷ*\\ :sub:`i`\\ :sup:`(1)` is the fitted value for *y*\\ :sub:`i` "
"according to model M\\ :sub:`1` and *ŷ*\\ :sub:`i`\\ :sup:`(2)` is the "
"fitted value for *y*\\ :sub:`i` according to model M\\ :sub:`2`."
msgstr ""
"der *ŷ*\\ :sub:`i`\\ :sup:`(1)` er den tilpassede verdien for *y*\\ :sub:`i` "
"i henhold til modell M\\ :sub:`1`, og *ŷ*\\ :sub:`i`\\ :sup:`(2)` er den "
"tilpassede verdien for *y*\\ :sub:`i` i henhold til modell M\\ :sub:`2`."

#: ../../Ch12/Ch12_Regression_12.rst:219 ../../Ch12/Ch12_Regression_12.rst:223
msgid "Model comparison in jamovi using the ``Model Builder`` option"
msgstr ""
"Sammenligning av modeller i jamovi ved hjelp av alternativet ``Model "
"Builder``"

#: ../../Ch12/Ch12_Regression_12.rst:227
msgid ""
"Okay, so that’s the hypothesis test that we use to compare two regression "
"models to one another. Now, how do we do it in jamovi? The answer is to use "
"the ``Model Builder`` option and specify the Model 1 predictors ``dani."
"sleep`` and ``day`` in ``Block 1`` and then add the additional predictor "
"from Model 2 (``baby.sleep``) in ``Block 2``, as in :numref:`fig-reg8`. This "
"shows, in the ``Model Comparisons`` Table, that for the comparisons between "
"Model 1 and Model 2, *F*\\ (1,96) = 0.00, *p* = 0.954. Since we have *p* > "
"0.05 we retain the null hypothesis (``M1``). This approach to regression, in "
"which we add all of our covariates into a null model, then *add* the "
"variables of interest into an alternative model, and then compare the two "
"models in a hypothesis testing framework, is often referred to as "
"**hierarchical regression**."
msgstr ""
"Det er altså hypotesetesten vi bruker for å sammenligne to "
"regresjonsmodeller med hverandre. Hvordan gjør vi det i jamovi? Svaret er å "
"bruke alternativet ``Model Builder`` og spesifisere modell 1-prediktorene "
"``dani.sleep`` og ``day`` i ``Block 1``, og deretter legge til den ekstra "
"prediktoren fra modell 2 (``baby.sleep``) i ``Block 2``, som i :numref:`fig-"
"reg8`. Dette viser, i tabellen ``Model Comparisons``, at for "
"sammenligningene mellom modell 1 og modell 2, *F*\\ (1,96) = 0,00, *p* = "
"0,954. Siden vi har *p* > 0,05, beholder vi nullhypotesen (``M1``). Denne "
"tilnærmingen til regresjon, der vi legger til alle kovariatene våre i en "
"nullmodell, deretter *legger til* variablene av interesse i en alternativ "
"modell, og deretter sammenligner de to modellene i et "
"hypotesetestingsrammeverk, kalles ofte **hierarkisk regresjon**."

#: ../../Ch12/Ch12_Regression_12.rst:240
msgid ""
"We can also use this ``Model Comparison`` option to display a table that "
"shows the AIC and BIC for each model, making it easy to compare and identify "
"which model has the lowest value, as in :numref:`fig-reg8`."
msgstr ""
"Vi kan også bruke dette ``Model Comparison``-alternativet til å vise en "
"tabell som viser AIC og BIC for hver modell, noe som gjør det enkelt å "
"sammenligne og identifisere hvilken modell som har lavest verdi, som i :"
"numref:`fig-reg8`."

#: ../../Ch12/Ch12_Regression_12.rst:247
msgid ""
"While I’m on this topic I should point out that the empirical evidence "
"suggests that BIC is a better criterion than AIC. In most simulation studies "
"that I’ve seen, BIC does a much better job of selecting the correct model."
msgstr ""
"Mens jeg er inne på dette temaet, bør jeg påpeke at de empiriske bevisene "
"tyder på at BIC er et bedre kriterium enn AIC. I de fleste "
"simuleringsstudier jeg har sett, gjør BIC en mye bedre jobb med å velge "
"riktig modell."

#: ../../Ch12/Ch12_Regression_12.rst:252
msgid ""
"It’s worth noting in passing that this same *F*-statistic can be used to "
"test a much broader range of hypotheses than those that I’m mentioning here. "
"Very briefly, notice that the nested model M1 corresponds to the full model "
"M2 when we constrain some of the regression coefficients to zero. It is "
"sometimes useful to construct sub-models by placing other kinds of "
"constraints on the regression coefficients. For instance, maybe two "
"different coefficients might have to sum to zero, or something like that. "
"You can construct hypothesis tests for those kind of constraints too, but it "
"is somewhat more complicated and the sampling distribution for *F* can end "
"up being something known as the non-central *F*-distribution, which is "
"waaaaay beyond the scope of this book! All I want to do is alert you to this "
"possibility."
msgstr ""
"Det er verdt å merke seg i forbifarten at den samme *F*-statistikken kan "
"brukes til å teste et mye bredere spekter av hypoteser enn dem jeg nevner "
"her. Legg merke til at den nestede modellen M1 tilsvarer den fullstendige "
"modellen M2 når vi begrenser noen av regresjonskoeffisientene til null. Noen "
"ganger er det nyttig å lage delmodeller ved å legge andre typer "
"begrensninger på regresjonskoeffisientene. For eksempel kan det hende at to "
"forskjellige koeffisienter må summere seg til null, eller noe lignende. Du "
"kan gjennomføre hypotesetester for slike begrensninger også, men det er noe "
"mer komplisert, og utvalgsfordelingen for *F* kan ende opp med å bli noe som "
"kalles den ikke-sentrale *F*-fordelingen, noe som er langt utenfor omfanget "
"av denne boken! Jeg vil bare gjøre deg oppmerksom på denne muligheten."

#: ../../Ch12/Ch12_Regression_13.rst:4
msgid "Summary"
msgstr "Sammendrag"

#: ../../Ch12/Ch12_Regression_13.rst:6
msgid ""
"Want to know how strong the relationship is between two variables? Calculate "
"a :doc:`correlation <Ch12_Regression_01>`"
msgstr ""
"Vil du vite hvor sterk sammenhengen er mellom to variabler? Beregn en :doc:"
"`korrelasjon <Ch12_Regression_01>`"

#: ../../Ch12/Ch12_Regression_13.rst:9
msgid "Drawing :doc:`scatterplots <Ch12_Regression_02>`"
msgstr "Tegning av :doc:`spredningsplott <Ch12_Regression_02>`"

#: ../../Ch12/Ch12_Regression_13.rst:11
msgid ""
":doc:`Basic ideas in linear regression <Ch12_Regression_03>` and :doc:`how "
"regression models are estimated <Ch12_Regression_04>`"
msgstr ""
":doc:`Grunnleggende ideer i lineær regresjon <Ch12_Regression_03>` og :doc:"
"`Hvordan regresjonsmodeller estimeres <Ch12_Regression_04>`"

#: ../../Ch12/Ch12_Regression_13.rst:14
msgid ":doc:`Ch12_Regression_05`"
msgstr ":doc:`Ch12_Regression_05`"

#: ../../Ch12/Ch12_Regression_13.rst:16
msgid ""
"Measuring the :doc:`overall performance of a regression model using *R*\\² "
"<Ch12_Regression_06>`"
msgstr ""
"Måling av :doc:`den samlede ytelsen til en regresjonsmodell ved hjelp av "
"*R*\\² <Ch12_Regression_06>`"

#: ../../Ch12/Ch12_Regression_13.rst:19
msgid ":doc:`Ch12_Regression_07`"
msgstr ":doc:`Ch12_Regression_07`"

#: ../../Ch12/Ch12_Regression_13.rst:21
msgid ""
":doc:`Calculating confidence intervals for regression coefficients and "
"standardised coefficients <Ch12_Regression_09>`"
msgstr ""
":doc:`Beregning av konfidensintervaller for regresjonskoeffisienter og "
"standardiserte koeffisienter <Ch12_Regression_09>`"

#: ../../Ch12/Ch12_Regression_13.rst:24
msgid ""
"The :doc:`assumptions of regression <Ch12_Regression_10>` and :doc:`how to "
"check them <Ch12_Regression_11>`"
msgstr ""
":doc:`forutsetningene for regresjon <Ch12_Regression_10>` og :doc:`hvordan "
"de kan kontrolleres <Ch12_Regression_11>`"

#: ../../Ch12/Ch12_Regression_13.rst:27
msgid ":doc:`Selecting a regression model <Ch12_Regression_12>`"
msgstr ":doc:`Velger en regresjonsmodell <Ch12_Regression_12>`"
