#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-03-17 18:06+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Project-Id-Version: PROJECT VERSION\n"
"Report-Msgid-Bugs-To: sebastian.jentschke@uib.no\n"
"POT-Creation-Date: 2025-03-17 18:06+0100\n"
"PO-Revision-Date: 2025-03-31 16:02+0000\n"
"Last-Translator: Anonymous <noreply@weblate.org>\n"
"Language-Team: Polish <https://hosted.weblate.org/projects/lsjdocs/ch13/pl/"
">\n"
"Language: pl\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=3; plural=n==1 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 "
"|| n%100>=20) ? 1 : 2;\n"
"X-Generator: Weblate 5.11-dev\n"
"Generated-By: Babel 2.17.0\n"

#: ../../Ch13/Ch13_ANOVA.rst:4
msgid "Comparing several means (one-way ANOVA)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA.rst:22
msgid ""
"This chapter introduces one of the most widely used tools in psychological "
"statistics, known as “the analysis of variance”, but usually referred to as "
"ANOVA. The basic technique was developed by Sir Ronald Fisher in the early "
"20th century and it is to him that we owe the rather unfortunate "
"terminology. The term ANOVA is a little misleading, in two respects. "
"Firstly, although the name of the technique refers to variances, ANOVA is "
"concerned with investigating differences in means. Secondly, there are "
"several different things out there that are all referred to as ANOVAs, some "
"of which have only a very tenuous connection to one another. Later on in the "
"book we’ll encounter a range of different ANOVA methods that apply in quite "
"different situations, but for the purposes of this chapter we’ll only "
"consider the simplest form of ANOVA, in which we have several different "
"groups of observations, and we’re interested in finding out whether those "
"groups differ in terms of some outcome variable of interest. This is the "
"question that is addressed by a **one-way ANOVA**."
msgstr ""

#: ../../Ch13/Ch13_ANOVA.rst:39
msgid ""
"The structure of this chapter is as follows: in section :doc:`Ch13_ANOVA_01` "
"I’ll introduce a fictitious data set that we’ll use as a running example "
"throughout the chapter. After introducing the data, I’ll describe the "
"mechanics of how a one-way ANOVA actually works (:doc:`Ch13_ANOVA_02`) and "
"then focus on :doc:`how you can run one in jamovi <Ch13_ANOVA_03>`. These "
"two sections are the core of the chapter. The remainder of the chapter "
"discusses a range of important topics that inevitably arise when running an "
"ANOVA, namely how to calculate :doc:`effect sizes <Ch13_ANOVA_04>`, and :doc:"
"`post-hoc tests and corrections for multiple comparisons <Ch13_ANOVA_05>`. "
"Afterwards, we will talk about the :doc:`assumptions the ANOVA relies upon "
"<Ch13_ANOVA_06>`, how to check those assumptions and some of the things you "
"can do if the assumptions are violated. Then we’ll cover :doc:`repeated "
"measures ANOVA <Ch13_ANOVA_07>` and it's non-parametric equivalent, the :doc:"
"`Friedman test <Ch13_ANOVA_08>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA.rst:54
msgid ""
"At the end of the chapter we’ll talk a little about the :doc:`relationship "
"between ANOVA and other statistical tools <Ch13_ANOVA_09>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_01.rst:4
msgid "An illustrative data set"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_01.rst:6
msgid ""
"Suppose you’ve become involved in a clinical trial in which you are testing "
"a new antidepressant drug called *Joyzepam*. In order to construct a fair "
"test of the drug’s effectiveness, the study involves three separate drugs to "
"be administered. One is a placebo, and the other is an existing "
"antidepressant / anti-anxiety drug called *Anxifree*. A collection of 18 "
"participants with moderate to severe depression are recruited for your "
"initial testing. Because the drugs are sometimes administered in conjunction "
"with psychological therapy, your study includes 9 people undergoing "
"cognitive behavioural therapy (CBT) and 9 who are not. Participants are "
"randomly assigned (doubly blinded, of course) a treatment, such that there "
"are 3 CBT people and 3 no-therapy people assigned to each of the 3 drugs. A "
"psychologist assesses the mood of each person after a 3 month run with each "
"drug, and the overall *improvement* in each person’s mood is assessed on a "
"scale ranging from -5 to +5. With that as the study design, let’s now load |"
"clinicaltrial|_ data set. It contains the three variables ``drug`` |"
"nominal|, ``therapy`` |nominal| and ``mood.gain`` |continuous|."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_01.rst:55 ../../Ch13/Ch13_ANOVA_08.rst:48
msgid "nominal"
msgstr "nominal"

#: ../../Ch13/Ch13_ANOVA_01.rst:52
msgid "continuous"
msgstr "continuous"

#: ../../Ch13/Ch13_ANOVA_01.rst:24
msgid ""
"For the purposes of this chapter, what we’re really interested in is the "
"effect of ``drug`` on ``mood.gain``. The first thing to do is calculate some "
"descriptive statistics and draw some graphs. In :doc:`../Ch04/"
"Ch04_Descriptives` we showed you how to do this, and some of the descriptive "
"statistics we can calculate in jamovi are shown in :numref:`fig-anova1`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_01.rst:33 ../../Ch13/Ch13_ANOVA_01.rst:37
msgid "Descriptives for ``mood.gain``, and box plots by ``drug`` administered"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_01.rst:41
msgid ""
"As the plot makes clear, there is a larger improvement in mood for "
"participants in the ``joyzepam`` group than for either the ``anxifree`` "
"group or the ``placebo`` group. The ``anxifree`` group shows a larger mood "
"gain than the ``placebo`` group, but the difference isn’t as large. The "
"question that we want to answer is are these difference “real”, or are they "
"just due to chance?"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:4
msgid "How ANOVA works"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:6
msgid ""
"In order to answer the question posed by our |clinicaltrial|_ data we’re "
"going to run a one-way ANOVA. I’m going to start by showing you how to do it "
"the hard way, building the statistical tool from the ground up and showing "
"you how you could do it if you didn’t have access to any of the cool built-"
"in ANOVA functions in jamovi. And I hope you’ll read it carefully, try to do "
"it the long way once or twice to make sure you really understand how ANOVA "
"works, and then once you’ve grasped the concept never *ever* do it this way "
"again."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:15
msgid ""
"The experimental design that I described in the previous section strongly "
"suggests that we’re interested in comparing the average mood change for the "
"three different drugs. In that sense, we’re talking about an analysis "
"similar to the *t*-test (chapter :doc:`../Ch11/Ch11_tTest`) but involving "
"more than two groups. If we let µ\\ :sub:`P` denote the population mean for "
"the mood change induced by the placebo, and let µ\\ :sub:`A` and µ\\ :sub:"
"`J` denote the corresponding means for our two drugs, Anxifree and Joyzepam, "
"then the (somewhat pessimistic) null hypothesis that we want to test is that "
"all three population means are identical. That is, *neither* of the two "
"drugs is any more effective than a placebo. We can write out this null "
"hypothesis as:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:26
msgid ""
"H\\ :sub:`0`: it is true that µ\\ :sub:`P` = µ\\ :sub:`A` = µ\\ :sub:`J`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:28
msgid ""
"As a consequence, our alternative hypothesis is that at least one of the "
"three different treatments is different from the others. It’s a bit tricky "
"to write this mathematically, because (as we’ll discuss) there are quite a "
"few different ways in which the null hypothesis can be false. So for now "
"we’ll just write the alternative hypothesis like this:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:34
msgid ""
"H\\ :sub:`1`: it is NOT true that µ\\ :sub:`P` = µ\\ :sub:`A` = µ\\ :sub:`J`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:36
msgid ""
"This null hypothesis is a lot trickier to test than any of the ones we’ve "
"seen previously. How shall we do it? A sensible guess would be to “do an "
"ANOVA”, since that’s the title of the chapter, but it’s not particularly "
"clear why an “analysis of *variances*” will help us learn anything useful "
"about the *means*. In fact, this is one of the biggest conceptual "
"difficulties that people have when first encountering ANOVA. To see how this "
"works, I find it most helpful to start by talking about variances. In fact, "
"what I’m going to do is start by playing some mathematical games with the "
"formula that describes the variance. That is, we’ll start out by playing "
"around with variances and it will turn out that this gives us a useful tool "
"for investigating means."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:49
msgid "Two formulas for the variance of *Y*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:51
msgid ""
"First, let’s start by introducing some notation. We’ll use *G* to refer to "
"the total number of groups. For our data set there are three drugs, so there "
"are *G* = 3 groups. Next, we’ll use *N* to refer to the total sample size; "
"there are a total of *N* = 18 people in our data set. Similarly, let’s use |"
"N_k| to denote the number of people in the *k*-th group. In our |"
"clinicaltrial|_ data, the sample size is |N_k| = 6` for all three groups.\\ "
"[#]_ Finally, we’ll use *Y* to denote the outcome variable. In our case, *Y* "
"refers to mood change. Specifically, we’ll use |Y_ik| to refer to the mood "
"change experienced by the *i*-th member of the *k*-th group. Similarly, "
"we’ll use |Yb| to be the average mood change, taken across all 18 people in "
"the experiment, and |Yb_k| to refer to the average mood change experienced "
"by the 6 people in group *k*."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:66
msgid ""
"Now that we’ve got our notation sorted out we can start writing down "
"formulas. To start with, let’s recall the :ref:`formula for the variance "
"<variance_formula>` that we used way back in those kinder days when we were "
"just doing descriptive statistics. The sample variance of *Y* is defined as "
"follows:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:72
msgid ""
"\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} "
"- \\bar{Y} \\right)^2\n"
"\n"
msgstr ""
"\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} "
"- \\bar{Y} \\right)^2\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:74
msgid ""
"This formula looks pretty much identical to the :ref:`formula for the "
"variance <variance_formula>`. The only difference is that this time around "
"I’ve got two summations here: I’m summing over groups (i.e., values for *k*) "
"and over the people within the groups (i.e., values for *:`i*). This is "
"purely a cosmetic detail. If I’d instead used the notation |Y_p| to refer to "
"the value of the outcome variable for person *p* in the sample, then I’d "
"only have a single summation. The only reason that we have a double "
"summation here is that I’ve classified people into groups, and then assigned "
"numbers to people within groups."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:84
msgid ""
"A concrete example might be useful here. Let’s consider this table, in which "
"we have a total of *N* = 5 people sorted into *G* = 2 groups. Arbitrarily, "
"let’s say that the “cool” people are group 1 and the “uncool” people are "
"group 2. It turns out that we have three cool people (*N*\\ :sub:`1` = 3) "
"and two uncool people (*N*\\ :sub:`2` = 2)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91
msgid "name"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91
msgid "person"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91 ../../Ch13/Ch13_ANOVA_02.rst:398
#: ../../Ch13/Ch13_ANOVA_02.rst:424 ../../Ch13/Ch13_ANOVA_02.rst:445
#: ../../Ch13/Ch13_ANOVA_02.rst:491 ../../Ch13/Ch13_ANOVA_02.rst:521
msgid "group"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91
msgid "group num."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91
msgid "index in group"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:91
msgid "grumpiness"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:93
msgid "*p*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:93 ../../Ch13/Ch13_ANOVA_02.rst:400
#: ../../Ch13/Ch13_ANOVA_02.rst:426 ../../Ch13/Ch13_ANOVA_02.rst:447
#: ../../Ch13/Ch13_ANOVA_02.rst:493 ../../Ch13/Ch13_ANOVA_02.rst:523
msgid "*k*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:93
msgid "*i*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:93
msgid "|Y_ik| or |Y_p|"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:95
msgid "Ann"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:95 ../../Ch13/Ch13_ANOVA_02.rst:97
#: ../../Ch13/Ch13_ANOVA_02.rst:99 ../../Ch13/Ch13_ANOVA_02.rst:101
#: ../../Ch13/Ch13_ANOVA_05.rst:43 ../../Ch13/Ch13_ANOVA_05.rst:206
#: ../../Ch13/Ch13_ANOVA_06.rst:333 ../../Ch13/Ch13_ANOVA_07.rst:66
msgid "1"
msgstr "1"

#: ../../Ch13/Ch13_ANOVA_02.rst:95 ../../Ch13/Ch13_ANOVA_02.rst:97
#: ../../Ch13/Ch13_ANOVA_02.rst:99
msgid "cool"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:95
msgid "20"
msgstr "20"

#: ../../Ch13/Ch13_ANOVA_02.rst:97
msgid "Ben"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:97 ../../Ch13/Ch13_ANOVA_02.rst:101
#: ../../Ch13/Ch13_ANOVA_02.rst:103 ../../Ch13/Ch13_ANOVA_02.rst:597
#: ../../Ch13/Ch13_ANOVA_05.rst:45 ../../Ch13/Ch13_ANOVA_05.rst:204
#: ../../Ch13/Ch13_ANOVA_06.rst:333 ../../Ch13/Ch13_ANOVA_07.rst:68
#: ../../Ch13/Ch13_ANOVA_07.rst:74
msgid "2"
msgstr "2"

#: ../../Ch13/Ch13_ANOVA_02.rst:97
msgid "55"
msgstr "55"

#: ../../Ch13/Ch13_ANOVA_02.rst:99
msgid "Cat"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:99 ../../Ch13/Ch13_ANOVA_05.rst:47
#: ../../Ch13/Ch13_ANOVA_05.rst:202 ../../Ch13/Ch13_ANOVA_07.rst:70
msgid "3"
msgstr "3"

#: ../../Ch13/Ch13_ANOVA_02.rst:99
msgid "21"
msgstr "21"

#: ../../Ch13/Ch13_ANOVA_02.rst:101
msgid "Tim"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:101 ../../Ch13/Ch13_ANOVA_05.rst:49
#: ../../Ch13/Ch13_ANOVA_05.rst:200 ../../Ch13/Ch13_ANOVA_07.rst:72
#: ../../Ch13/Ch13_ANOVA_07.rst:76
msgid "4"
msgstr "4"

#: ../../Ch13/Ch13_ANOVA_02.rst:101 ../../Ch13/Ch13_ANOVA_02.rst:103
msgid "uncool"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:101
msgid "91"
msgstr "91"

#: ../../Ch13/Ch13_ANOVA_02.rst:103
msgid "Egg"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:103 ../../Ch13/Ch13_ANOVA_05.rst:51
#: ../../Ch13/Ch13_ANOVA_05.rst:198 ../../Ch13/Ch13_ANOVA_07.rst:70
#: ../../Ch13/Ch13_ANOVA_07.rst:72 ../../Ch13/Ch13_ANOVA_07.rst:74
msgid "5"
msgstr "5"

#: ../../Ch13/Ch13_ANOVA_02.rst:103
msgid "22"
msgstr "22"

#: ../../Ch13/Ch13_ANOVA_02.rst:106
msgid ""
"Notice that I’ve constructed two different labelling schemes here. We have a "
"“person” variable *p* so it would be perfectly sensible to refer to |Y_p| as "
"the grumpiness of the *p*-th person in the sample. For instance, the table "
"shows that Tim is the fourth so we’d say *p* = 4. So, when talking about the "
"grumpiness *Y* of this “Tim” person, whoever he might be, we could refer to "
"his grumpiness by saying that |Y_p| = 91, for person *p* = 4 that is. "
"However, that’s not the only way we could refer to Tim. As an alternative we "
"could note that Tim belongs to the “uncool” group (*k* = 2), and is in fact "
"the first person listed in the “uncool” group (*i* = 1). So it’s equally "
"valid to refer to Tim’s grumpiness by saying that |Y_ik| = 91, where *k* = 2 "
"and *i* = 1."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:119
msgid ""
"In other words, each person *p* corresponds to a unique *ik* combination, "
"and so the formula that I gave above is actually identical to our original "
"formula for the variance, which would be"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:123
msgid ""
"\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{p=1}^N  \\left(Y_{p} - \\bar{Y} "
"\\right)^2\n"
"\n"
msgstr ""
"\\mbox{Var}(Y) = \\frac{1}{N} \\sum_{p=1}^N  \\left(Y_{p} - \\bar{Y} "
"\\right)^2\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:125
msgid ""
"In both formulas, all we’re doing is summing over all of the observations in "
"the sample. Most of the time we would just use the simpler |Y_p| notation; "
"the equation using |Y_p| is clearly the simpler of the two. However, when "
"doing an ANOVA it’s important to keep track of which participants belong in "
"which groups, and we need to use the |Y_ik| notation to do this."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:133
msgid "From variances to sums of squares"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:135
msgid ""
"Okay, now that we’ve got a good grasp on how the variance is calculated, "
"let’s define something called the **total sum of squares**, which is denoted "
"|SS_t|\\. This is very simple. Instead of averaging the squared deviations, "
"which is what we do when calculating the variance, we just add them up."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:141
msgid ""
"So the formula for the total sum of squares is almost identical to the "
"formula for the variance"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:144
msgid ""
"\\mbox{SS}_{tot} = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} - \\bar{Y} "
"\\right)^2\n"
"\n"
msgstr ""
"\\mbox{SS}_{tot} = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left(Y_{ik} - \\bar{Y} "
"\\right)^2\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:146
msgid ""
"When we talk about analysing variances in the context of ANOVA, what we’re "
"really doing is working with the total sums of squares rather than the "
"actual variance. One very nice thing about the total sum of squares is that "
"we can break it up into two different kinds of variation."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:151
msgid ""
"First, we can talk about the **within-group sum of squares**, in which we "
"look to see how different each individual person is from their own group mean"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:155
msgid ""
"\\mbox{SS}_w = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( Y_{ik} - \\bar{Y}_k "
"\\right)^2\n"
"\n"
msgstr ""
"\\mbox{SS}_w = \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( Y_{ik} - \\bar{Y}_k "
"\\right)^2\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:157
msgid ""
"where |Yb_k| is a group mean. In our example, |Yb_k| would be the average "
"mood change experienced by those people given the *k*-th drug. So, instead "
"of comparing individuals to the average of all people in the experiment, "
"we’re only comparing them to those people in the the same group. As a "
"consequence, you’d expect the value of |SS_w| to be smaller than the total "
"sum of squares, because it’s completely ignoring any group differences, i."
"e., whether the drugs will have different effects on people’s moods."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:165
msgid ""
"Next, we can define a third notion of variation which captures *only* the "
"differences between groups. We do this by looking at the differences between "
"the group means |Yb_k| and grand mean |Yb|."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:169
msgid ""
"In order to quantify the extent of this variation, what we do is calculate "
"the **between-group sum of squares**"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:172
msgid ""
"\\begin{aligned}\n"
"\\mbox{SS}_{b} &=& \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( \\bar{Y}_k - "
"\\bar{Y} \\right)^2 \\\\\n"
"              &=& \\sum_{k=1}^G N_k \\left( \\bar{Y}_k - \\bar{Y} "
"\\right)^2\\end{aligned}"
msgstr ""
"\\begin{aligned}\n"
"\\mbox{SS}_{b} &=& \\sum_{k=1}^G \\sum_{i=1}^{N_k} \\left( \\bar{Y}_k - "
"\\bar{Y} \\right)^2 \\\\\n"
"              &=& \\sum_{k=1}^G N_k \\left( \\bar{Y}_k - \\bar{Y} "
"\\right)^2\\end{aligned}"

#: ../../Ch13/Ch13_ANOVA_02.rst:178
msgid ""
"It’s not too difficult to show that the total variation among people in the "
"experiment |SS_t| is actually the sum of the differences between the groups |"
"SS_b| and the variation inside the groups S\\ :sub:`w`\\. That is,"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:183
msgid "|SS_w| + |SS_b| = |SS_t|"
msgstr "|SS_w| + |SS_b| = |SS_t|"

#: ../../Ch13/Ch13_ANOVA_02.rst:185
msgid "Yay."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:189
msgid "illustration of between and within groups variation"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:193
msgid ""
"Graphical illustration of “between groups” variation (left panel) and "
"“within groups” variation (right panel). In the left panel, the arrows show "
"the differences in the group means. In the right panel, the arrows highlight "
"the variability within each group."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:200
msgid ""
"Okay, so what have we found out? We’ve discovered that the total variability "
"associated with the outcome variable (|SS_t|\\) can be mathematically carved "
"up into the sum of “the variation due to the differences in the sample means "
"for the different groups” (|SS_b|\\) plus “all the rest of the variation” (|"
"SS_w|\\).\\ [#]_"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:207
msgid ""
"How does that help me find out whether the groups have different population "
"means? Um. Wait. Hold on a second. Now that I think about it, this is "
"*exactly* what we were looking for. If the null hypothesis is true then "
"you’d expect all the sample means to be pretty similar to each other, right? "
"And that would imply that you’d expect |SS_b| to be really small, or at "
"least you’d expect it to be a lot smaller than “the variation associated "
"with everything else”, |SS_w|\\. Hmm. I detect a hypothesis test coming on."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:216
msgid "From sums of squares to the *F*-test"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:218
msgid ""
"As we saw in the last section, the *qualitative* idea behind ANOVA is to "
"compare the two sums of squares values |SS_b| and |SS_w| to each other. If "
"the between-group variation |SS_b| is large relative to the within-group "
"variation |SS_w| then we have reason to suspect that the population means "
"for the different groups aren’t identical to each other. In order to convert "
"this into a workable hypothesis test, there’s a little bit of “fiddling "
"around” needed. What I’ll do is first show you *what* we do to calculate our "
"test statistic, the **F-ratio**, and then try to give you a feel for *why* "
"we do it this way."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:229
msgid ""
"In order to convert our SS values into an *F*-ratio the first thing we need "
"to calculate is the **degrees of freedom** associated with the |SS_b| and |"
"SS_w| values. As usual, the degrees of freedom corresponds to the number of "
"unique “data points” that contribute to a particular calculation, minus the "
"number of “constraints” that they need to satisfy. For the within-groups "
"variability what we’re calculating is the variation of the individual "
"observations (*N* data points) around the group means (*G* constraints). In "
"contrast, for the between groups variability we’re interested in the "
"variation of the group means (*G* data points) around the grand mean (1 "
"constraint). Therefore, the degrees of freedom here are:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:242 ../../Ch13/Ch13_ANOVA_02.rst:289
msgid "|df_b| = *G* - 1"
msgstr "|df_b| = *G* - 1"

#: ../../Ch13/Ch13_ANOVA_02.rst:243 ../../Ch13/Ch13_ANOVA_02.rst:291
msgid "|df_w| = *N* - *G*"
msgstr "|df_w| = *N* - *G*"

#: ../../Ch13/Ch13_ANOVA_02.rst:245
msgid ""
"Okay, that seems simple enough. What we do next is convert our summed "
"squares value into a “mean squares” value, which we do by dividing by the "
"degrees of freedom:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:249 ../../Ch13/Ch13_ANOVA_02.rst:289
msgid "|MS_b| = |SS_b| / |df_b|"
msgstr "|MS_b| = |SS_b| / |df_b|"

#: ../../Ch13/Ch13_ANOVA_02.rst:250 ../../Ch13/Ch13_ANOVA_02.rst:291
msgid "|MS_w| = |SS_w| / |df_w|"
msgstr "|MS_w| = |SS_w| / |df_w|"

#: ../../Ch13/Ch13_ANOVA_02.rst:252
msgid ""
"Finally, we calculate the *F*-ratio by dividing the between-groups MS by the "
"within-groups MS:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:255 ../../Ch13/Ch13_ANOVA_02.rst:289
msgid "F = |MS_b| / |MS_w|"
msgstr "F = |MS_b| / |MS_w|"

#: ../../Ch13/Ch13_ANOVA_02.rst:257
msgid ""
"At a very general level, the intuition behind the *F*-statistic is "
"straightforward. Bigger values of *F* means that the between-groups "
"variation is large relative to the within-groups variation. As a "
"consequence, the larger the value of *F* the more evidence we have against "
"the null hypothesis. But how large does *F* have to be in order to actually "
"*reject* H\\ :sub:`0`? In order to understand this, you need a slightly "
"deeper understanding of what ANOVA is and what the mean squares values "
"actually are."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:266
msgid ""
"The next section discusses that in a bit of detail, but for readers that "
"aren’t interested in the details of what the test is actually measuring I’ll "
"cut to the chase. In order to complete our hypothesis test we need to know "
"the sampling distribution for *F* if the null hypothesis is true. Not "
"surprisingly, the sampling distribution for the *F*-statistic under the null "
"hypothesis is an *F*-distribution. If you recall our discussion of the *F*-"
"distribution in chapter :doc:`../Ch07/Ch07_Probability`, the *F*-"
"distribution has two parameters, corresponding to the two degrees of freedom "
"involved. The first one *df*\\ :sub:`1` is the between groups degrees of "
"freedom |df_b|, and the second one *df*\\ :sub:`2` is the within groups "
"degrees of freedom |df_w|\\."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:277
msgid ""
"A summary of all the key quantities involved in a one-way ANOVA, including "
"the formulas showing how they are calculated, is shown in :numref:`tab-"
"anovatable`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:280
msgid ""
"All of the key quantities involved in an ANOVA organised into a “standard” "
"ANOVA table. The formulas for all quantities (except the *p*-value which has "
"a very ugly formula and would be nightmarishly hard to calculate without a "
"computer) are shown."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:287 ../../Ch13/Ch13_ANOVA_02.rst:595
msgid "*df*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:287 ../../Ch13/Ch13_ANOVA_02.rst:595
msgid "sum of squares"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:287 ../../Ch13/Ch13_ANOVA_02.rst:595
msgid "mean squares"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:287 ../../Ch13/Ch13_ANOVA_02.rst:595
msgid "*F*-statistic"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:287 ../../Ch13/Ch13_ANOVA_02.rst:595
msgid "*p*-value"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:289 ../../Ch13/Ch13_ANOVA_02.rst:597
msgid "**between groups**"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:289
msgid "|SS_b| = |f_SS_b|"
msgstr "|SS_b| = |f_SS_b|"

#: ../../Ch13/Ch13_ANOVA_02.rst:289
msgid "[complicated]"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:291 ../../Ch13/Ch13_ANOVA_02.rst:599
msgid "**within groups**"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:291
msgid "|SS_w| = |f_SS_w|"
msgstr "|SS_w| = |f_SS_w|"

#: ../../Ch13/Ch13_ANOVA_02.rst:297
msgid "The model for the data and the meaning of *F*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:299
msgid ""
"At a fundamental level ANOVA is a competition between two different "
"statistical models, H\\ :sub:`0` and H\\ :sub:`1`. When I described the null "
"and alternative hypotheses at the start of the section, I was a little "
"imprecise about what these models actually are. I’ll remedy that now, though "
"you probably won’t like me for doing so. If you recall, our null hypothesis "
"was that all of the group means are identical to one another. If so, then a "
"natural way to think about the outcome variable |Y_ik| is to describe "
"individual scores in terms of a single population mean µ, plus the deviation "
"from that population mean. This deviation is usually denoted ϵ\\ :sub:`ik` "
"and is traditionally called the *error* or **residual** associated with that "
"observation. Be careful though. Just like we saw with the word "
"“significant”, the word “error” has a technical meaning in statistics that "
"isn’t quite the same as its everyday English definition. In everyday "
"language, “error” implies a mistake of some kind, but in statistics it "
"doesn’t (or at least, not necessarily). With that in mind, the word "
"“residual” is a better term than the word “error”. In statistics both words "
"mean “leftover variability”, that is “stuff” that the model can’t explain."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:319
msgid ""
"In any case, here’s what the null hypothesis looks like when we write it as "
"a statistical model"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:322
msgid "|Y_ik| = µ + ϵ\\ :sub:`ik`"
msgstr "|Y_ik| = µ + ϵ\\ :sub:`ik`"

#: ../../Ch13/Ch13_ANOVA_02.rst:324
msgid ""
"where we make the *assumption* (discussed later) that the residual values "
"ϵ\\ :sub:`ik` are normally distributed, with mean 0 and a standard deviation "
"σ that is the same for all groups. To use the notation that we introduced in "
"chapter :doc:`../Ch07/Ch07_Probability` we would write this assumption like "
"this:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:330 ../../Ch13/Ch13_ANOVA_06.rst:29
msgid "ϵ\\ :sub:`ik` ~ Normal(0, σ²)"
msgstr "ϵ\\ :sub:`ik` ~ Normal(0, σ²)"

#: ../../Ch13/Ch13_ANOVA_02.rst:332
msgid ""
"What about the alternative hypothesis, H\\ :sub:`1`? The only difference "
"between the null hypothesis and the alternative hypothesis is that we allow "
"each group to have a different population mean. So, if we let µ\\ :sub:`k` "
"denote the population mean for the *k*-th group in our experiment, then the "
"statistical model corresponding to H\\ :sub:`1` is"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:339
msgid "|Y_ik| = µ\\ :sub:`k` + ϵ\\ :sub:`ik`"
msgstr "|Y_ik| = µ\\ :sub:`k` + ϵ\\ :sub:`ik`"

#: ../../Ch13/Ch13_ANOVA_02.rst:341
msgid ""
"where, once again, we assume that the error terms are normally distributed "
"with mean 0 and standard deviation σ. That is, the alternative hypothesis "
"also assumes that ϵ ~ Normal(0, σ²)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:346
msgid ""
"Okay, now that we’ve described the statistical models underpinning H\\ :sub:"
"`0` and H\\ :sub:`1` in more detail, it’s now pretty straightforward to say "
"what the mean square values are measuring, and what this means for the "
"interpretation of *F*. I won’t bore you with the proof of this but it turns "
"out that the within-groups mean square, |MS_w|, can be viewed as an "
"estimator (in the technical sense, chapter :doc:`../Ch08/Ch08_Estimation`) "
"of the error variance σ². The between-groups mean square |MS_b| is also an "
"estimator, but what it estimates is the error variance *plus* a quantity "
"that depends on the true differences among the group means. If we call this "
"quantity *Q*, then we can see that the *F*-statistic is basically:\\ [#]_"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:357
msgid ""
"F = \\frac{\\hat{Q} + \\hat\\sigma^2}{\\hat\\sigma^2}\n"
"\n"
msgstr ""
"F = \\frac{\\hat{Q} + \\hat\\sigma^2}{\\hat\\sigma^2}\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:359
msgid ""
"where the true value *Q* = 0 if the null hypothesis is true, and *Q* > 0 if "
"the alternative hypothesis is true (:ref:`Hays, 1994 <Hays_1994>`, Ch. 10). "
"Therefore, at a bare minimum *the F-value must be larger than 1* to have any "
"chance of rejecting the null hypothesis. Note that this *doesn’t* mean that "
"it’s impossible to get an *F*-value less than 1. What it means is that if "
"the null hypothesis is true the sampling distribution of the *F*-ratio has a "
"mean of 1,\\ [#]_ and so we need to see *F*-values larger than 1 in order to "
"safely reject the null."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:368
msgid ""
"To be a bit more precise about the sampling distribution, notice that if the "
"null hypothesis is true, both |MS_b| and |MS_w| are estimators of the "
"variance of the residuals ϵ\\ :sub:`ik`. If those residuals are normally "
"distributed, then you might suspect that the estimate of the variance of "
"ϵ\\ :sub:`ik` is χ²-distributed, because (as discussed in :doc:`../Ch07/"
"Ch07_Probability_6`) that’s what a χ²-distribution *is*: it’s what you get "
"when you square a bunch of normally-distributed things and add them up. And "
"since the *F*-distribution is (again, by definition) what you get when you "
"take the ratio between two things that are χ² distributed, we have our "
"sampling distribution. Obviously, I’m glossing over a whole lot of stuff "
"when I say this, but in broad terms, this really is where our sampling "
"distribution comes from."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:383
msgid "A worked example"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:385
msgid ""
"The previous discussion was fairly abstract and a little on the technical "
"side, so I think that at this point it might be useful to see a worked "
"example. For that, let’s go back to the |clinicaltrial|_ data set that was "
"introduced earlier in the chapter. The descriptive statistics that we "
"calculated at the beginning tell us our group means: An average mood gain of "
"0.45 for the placebo, 0.72 for Anxifree, and 1.48 for Joyzepam. With that in "
"mind, let’s party like it’s 1899\\ [#]_ and start doing some pencil and "
"paper calculations. I’ll only do this for the first 5 observations because "
"it’s not bloody 1899 and I’m very lazy. Let’s start by calculating |SS_w|, "
"the within-group sums of squares. First, let’s draw up a nice table to help "
"us with our calculations:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:398 ../../Ch13/Ch13_ANOVA_02.rst:424
#: ../../Ch13/Ch13_ANOVA_02.rst:445
msgid "outcome"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:400 ../../Ch13/Ch13_ANOVA_02.rst:426
#: ../../Ch13/Ch13_ANOVA_02.rst:447
msgid "|Y_ik|"
msgstr "|Y_ik|"

#: ../../Ch13/Ch13_ANOVA_02.rst:402 ../../Ch13/Ch13_ANOVA_02.rst:404
#: ../../Ch13/Ch13_ANOVA_02.rst:406 ../../Ch13/Ch13_ANOVA_02.rst:428
#: ../../Ch13/Ch13_ANOVA_02.rst:430 ../../Ch13/Ch13_ANOVA_02.rst:432
#: ../../Ch13/Ch13_ANOVA_02.rst:449 ../../Ch13/Ch13_ANOVA_02.rst:451
#: ../../Ch13/Ch13_ANOVA_02.rst:453 ../../Ch13/Ch13_ANOVA_02.rst:495
#: ../../Ch13/Ch13_ANOVA_02.rst:525
msgid "placebo"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:402 ../../Ch13/Ch13_ANOVA_02.rst:428
#: ../../Ch13/Ch13_ANOVA_02.rst:449 ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.5"
msgstr "0.5"

#: ../../Ch13/Ch13_ANOVA_02.rst:404 ../../Ch13/Ch13_ANOVA_02.rst:430
#: ../../Ch13/Ch13_ANOVA_02.rst:451 ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.3"
msgstr "0.3"

#: ../../Ch13/Ch13_ANOVA_02.rst:406 ../../Ch13/Ch13_ANOVA_02.rst:432
#: ../../Ch13/Ch13_ANOVA_02.rst:453 ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.1"
msgstr "0.1"

#: ../../Ch13/Ch13_ANOVA_02.rst:408 ../../Ch13/Ch13_ANOVA_02.rst:410
#: ../../Ch13/Ch13_ANOVA_02.rst:434 ../../Ch13/Ch13_ANOVA_02.rst:436
#: ../../Ch13/Ch13_ANOVA_02.rst:455 ../../Ch13/Ch13_ANOVA_02.rst:457
#: ../../Ch13/Ch13_ANOVA_02.rst:497 ../../Ch13/Ch13_ANOVA_02.rst:527
msgid "anxifree"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:408 ../../Ch13/Ch13_ANOVA_02.rst:434
#: ../../Ch13/Ch13_ANOVA_02.rst:455 ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.6"
msgstr "0.6"

#: ../../Ch13/Ch13_ANOVA_02.rst:410 ../../Ch13/Ch13_ANOVA_02.rst:436
#: ../../Ch13/Ch13_ANOVA_02.rst:457 ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.4"
msgstr "0.4"

#: ../../Ch13/Ch13_ANOVA_02.rst:413
msgid ""
"At this stage, the only thing I’ve included in the table is the raw data "
"itself. That is, the grouping variable (i.e., ``drug``) and outcome variable "
"(i.e. ``mood.gain``) for each person. Note that the outcome variable here "
"corresponds to the |Y_ik| value in our equation previously. The next step in "
"the calculation is to write down, for each person in the study, the "
"corresponding group mean, |Yb_k|. This is slightly repetitive but not "
"particularly difficult since we already calculated those group means when "
"doing our descriptive statistics:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:424 ../../Ch13/Ch13_ANOVA_02.rst:445
#: ../../Ch13/Ch13_ANOVA_02.rst:491
msgid "group mean"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:426 ../../Ch13/Ch13_ANOVA_02.rst:447
#: ../../Ch13/Ch13_ANOVA_02.rst:493
msgid "|Yb_k|"
msgstr "|Yb_k|"

#: ../../Ch13/Ch13_ANOVA_02.rst:428 ../../Ch13/Ch13_ANOVA_02.rst:430
#: ../../Ch13/Ch13_ANOVA_02.rst:432
msgid "**0.45**"
msgstr "**0.45**"

#: ../../Ch13/Ch13_ANOVA_02.rst:434 ../../Ch13/Ch13_ANOVA_02.rst:436
msgid "**0.72**"
msgstr "**0.72**"

#: ../../Ch13/Ch13_ANOVA_02.rst:439
msgid ""
"Now that we’ve written those down, we need to calculate, again for every "
"person, the deviation from the corresponding group mean. That is, we want to "
"subtract |Y_ik| - |Yb_k|. After we’ve done that, we need to square "
"everything. When we do that, here’s what we get:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:445
msgid "dev. from group mean"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:445
msgid "squared deviation"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:447
msgid "(|Y_ik| - |Yb_k|)"
msgstr "(|Y_ik| - |Yb_k|)"

#: ../../Ch13/Ch13_ANOVA_02.rst:447
msgid "(|Y_ik| - |Yb_k|\\)²"
msgstr "(|Y_ik| - |Yb_k|\\)²"

#: ../../Ch13/Ch13_ANOVA_02.rst:449 ../../Ch13/Ch13_ANOVA_02.rst:451
#: ../../Ch13/Ch13_ANOVA_02.rst:453 ../../Ch13/Ch13_ANOVA_02.rst:495
msgid "0.45"
msgstr "0.45"

#: ../../Ch13/Ch13_ANOVA_02.rst:449
msgid "**0.05**"
msgstr "**0.05**"

#: ../../Ch13/Ch13_ANOVA_02.rst:449
msgid "**0.0025**"
msgstr "**0.0025**"

#: ../../Ch13/Ch13_ANOVA_02.rst:451
msgid "**-0.15**"
msgstr "**-0.15**"

#: ../../Ch13/Ch13_ANOVA_02.rst:451
msgid "**0.0225**"
msgstr "**0.0225**"

#: ../../Ch13/Ch13_ANOVA_02.rst:453
msgid "**-0.35**"
msgstr "**-0.35**"

#: ../../Ch13/Ch13_ANOVA_02.rst:453
msgid "**0.1225**"
msgstr "**0.1225**"

#: ../../Ch13/Ch13_ANOVA_02.rst:455 ../../Ch13/Ch13_ANOVA_02.rst:457
#: ../../Ch13/Ch13_ANOVA_02.rst:497
msgid "0.72"
msgstr "0.72"

#: ../../Ch13/Ch13_ANOVA_02.rst:455
msgid "**-0.12**"
msgstr "**-0.12**"

#: ../../Ch13/Ch13_ANOVA_02.rst:455
msgid "**0.0136**"
msgstr "**0.0136**"

#: ../../Ch13/Ch13_ANOVA_02.rst:457
msgid "**-0.32**"
msgstr "**-0.32**"

#: ../../Ch13/Ch13_ANOVA_02.rst:457
msgid "**0.1003**"
msgstr "**0.1003**"

#: ../../Ch13/Ch13_ANOVA_02.rst:460
msgid ""
"The last step is equally straightforward. In order to calculate the within-"
"group sum of squares we just add up the squared deviations across all "
"observations:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:464
msgid "|SS_w| = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 = 0.2614"
msgstr "|SS_w| = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 = 0.2614"

#: ../../Ch13/Ch13_ANOVA_02.rst:466
msgid ""
"Of course, if we actually wanted to get the *right* answer we’d need to do "
"this for all 18 observations in the data set, not just the first five. We "
"could continue with the pencil and paper calculations if we wanted to, but "
"it’s pretty tedious. Alternatively, it’s not too hard to do this in jamovi."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:472
msgid ""
"Go to an empty column (at the end of the data set) and double click on the "
"column header, choose ``New computed variable`` and enter ``sq_res_wth`` in "
"the first line and the formula ``(mood.gain - VMEAN(mood.gain, group_by = "
"drug)) ^ 2`` in the line starting with ``=`` (next to the *f*\\ :sub:`x`). "
"``mood.gain`` represents |Y_ik|, ``VMEAN(mood.gain, group_by = drug)`` the "
"group mean |Yb_k|. This difference (third column in the table above) is then "
"squared and it is therefore not much surprise to see that the values are "
"(apart from rounding errors) identical to those in the last column of the "
"table above."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:482
msgid ""
"Okay. Now that we’ve calculated the within groups variation, |SS_w|, it’s "
"time to turn our attention to the between-group sum of squares, |SS_b|. The "
"calculations for this case are very similar. The main difference is that "
"instead of calculating the differences between an observation |Y_ik| and a "
"group mean |Yb_k| for all of the observations, we calculate the differences "
"between the group means |Yb_k| and the grand mean |Yb| (in this case 0.88) "
"for all of the groups."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:491
msgid "grand mean"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:491
msgid "deviation"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:491 ../../Ch13/Ch13_ANOVA_02.rst:521
msgid "squared deviations"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:493
msgid "|Yb|"
msgstr "|Yb|"

#: ../../Ch13/Ch13_ANOVA_02.rst:493
msgid "|Yb_k| - |YB|"
msgstr "|Yb_k| - |YB|"

#: ../../Ch13/Ch13_ANOVA_02.rst:493 ../../Ch13/Ch13_ANOVA_02.rst:523
msgid "(|Yb_k| - |Yb|)²"
msgstr "(|Yb_k| - |Yb|)²"

#: ../../Ch13/Ch13_ANOVA_02.rst:495 ../../Ch13/Ch13_ANOVA_02.rst:497
#: ../../Ch13/Ch13_ANOVA_02.rst:499
msgid "0.88"
msgstr "0.88"

#: ../../Ch13/Ch13_ANOVA_02.rst:495
msgid "-0.43"
msgstr "-0.43"

#: ../../Ch13/Ch13_ANOVA_02.rst:495 ../../Ch13/Ch13_ANOVA_02.rst:525
msgid "0.19"
msgstr "0.19"

#: ../../Ch13/Ch13_ANOVA_02.rst:497
msgid "-0.16"
msgstr "-0.16"

#: ../../Ch13/Ch13_ANOVA_02.rst:497 ../../Ch13/Ch13_ANOVA_02.rst:527
msgid "0.03"
msgstr "0.03"

#: ../../Ch13/Ch13_ANOVA_02.rst:499 ../../Ch13/Ch13_ANOVA_02.rst:529
msgid "joyzepam"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:499
msgid "1.48"
msgstr "1.48"

#: ../../Ch13/Ch13_ANOVA_02.rst:499
msgid "0.60"
msgstr "0.60"

#: ../../Ch13/Ch13_ANOVA_02.rst:499 ../../Ch13/Ch13_ANOVA_02.rst:529
msgid "0.36"
msgstr "0.36"

#: ../../Ch13/Ch13_ANOVA_02.rst:502
msgid ""
"We create another computed variable with the name ``sq_res_btw`` and "
"``(VMEAN(mood.gain, group_by = drug) - VMEAN(mood.gain) - ) ^ 2`` as "
"formula. The term ``VMEAN(mood.gain, group_by = drug)`` represents the group "
"mean |Yb_k|, and ``VMEAN(mood.gain)`` the grand mean |Yb|. Again, we find "
"that the values for that variable are the same as in the last column of the "
"table above: the first three rows represent ``placebo``, followed by three "
"lines with ``anxifree`` and three lines with ``joyzepam``; the next nine "
"lines are a repetition of the first nine ones."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:511
msgid ""
"However, for the between group calculations we need to multiply each of "
"these squared deviations by |N_k|, the number of observations in the group. "
"We do this because every *observation* in the group (all |N_k| of them) is "
"associated with a between group difference. So if there are six people in "
"the placebo group and the placebo group mean differs from the grand mean by "
"0.19, then the *total* between group variation associated with these six "
"people is 6 · 0.19 = 1.14. So we have to extend our little table of "
"calculations:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:521 ../../Ch13/Ch13_ANOVA_02.rst:523
#: ../../Ch13/Ch13_ANOVA_02.rst:525 ../../Ch13/Ch13_ANOVA_02.rst:527
#: ../../Ch13/Ch13_ANOVA_02.rst:529
msgid "…"
msgstr "…"

#: ../../Ch13/Ch13_ANOVA_02.rst:521
msgid "sample size"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:521
msgid "weighted squared deviat."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:523
msgid "|N_k|"
msgstr "|N_k|"

#: ../../Ch13/Ch13_ANOVA_02.rst:523
msgid "|N_k| · (|Yb_k| - |Yb|)²"
msgstr "|N_k| · (|Yb_k| - |Yb|)²"

#: ../../Ch13/Ch13_ANOVA_02.rst:525 ../../Ch13/Ch13_ANOVA_02.rst:527
#: ../../Ch13/Ch13_ANOVA_02.rst:529 ../../Ch13/Ch13_ANOVA_05.rst:53
#: ../../Ch13/Ch13_ANOVA_07.rst:66 ../../Ch13/Ch13_ANOVA_07.rst:68
#: ../../Ch13/Ch13_ANOVA_07.rst:74 ../../Ch13/Ch13_ANOVA_07.rst:76
msgid "6"
msgstr "6"

#: ../../Ch13/Ch13_ANOVA_02.rst:525
msgid "1.14"
msgstr "1.14"

#: ../../Ch13/Ch13_ANOVA_02.rst:527
msgid "0.18"
msgstr "0.18"

#: ../../Ch13/Ch13_ANOVA_02.rst:529
msgid "2.16"
msgstr "2.16"

#: ../../Ch13/Ch13_ANOVA_02.rst:532
msgid ""
"And so now our between group sum of squares is obtained by summing these "
"“weighted squared deviations” over all three groups in the study:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:535
msgid "|SS_b| = 1.14 + 0.18 + 2.16 = 3.48"
msgstr "|SS_b| = 1.14 + 0.18 + 2.16 = 3.48"

#: ../../Ch13/Ch13_ANOVA_02.rst:537
msgid ""
"As you can see, the between group calculations are a lot shorter (when "
"calculated b hand)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:540
msgid ""
"In jamovi, we can calculate these sums, i.e., the values for |SS_b| and |"
"SS_w|, by clicking ``Descriptives`` →  ``Descriptive Statistics``, then "
"moving ``sq_res_wth`` and ``sq_res_btw`` to the ``Variables`` box, and "
"finally selecting ``Sum`` from the ``Statistics`` drop-down menu. The sum of "
"``sq_res_wth`` (|SS_w|) has a value of **1.392**, ``sq_res_wth`` (|SS_b|) a "
"value of **3.453** (just rounding errors away from the 3.48 we calculated "
"above)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:548
msgid ""
"Now that we’ve calculated our sums of squares values, |SS_b| and |SS_w|, the "
"rest of the ANOVA is pretty painless. The next step is to calculate the "
"degrees of freedom. Since we have *G* = 3 groups and *N* = 18 observations "
"in total our degrees of freedom can be calculated by simple subtraction:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:553
msgid "|df_b| = *G* - 1 = 2 |df_w| = *N* - *G* = 15"
msgstr "|df_b| = *G* - 1 = 2 |df_w| = *N* - *G* = 15"

#: ../../Ch13/Ch13_ANOVA_02.rst:556
msgid ""
"Next, since we’ve now calculated the values for the sums of squares and the "
"degrees of freedom, for both the within-groups variability and the between-"
"groups variability, we can obtain the mean square values by dividing one by "
"the other:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:561
msgid ""
"\\begin{array}{lclclcl}\n"
"\\mbox{MS}_b &=& \\displaystyle\\frac{\\mbox{SS}_b }{  \\mbox{df}_b } &=& "
"\\displaystyle\\frac{3.453}{ 2} &=& 1.727 \\\\\n"
"\\mbox{MS}_w &=& \\displaystyle\\frac{\\mbox{SS}_w }{  \\mbox{df}_w } &=& "
"\\displaystyle\\frac{1.392}{15} &=& 0.093\n"
"\\end{array}"
msgstr ""
"\\begin{array}{lclclcl}\n"
"\\mbox{MS}_b &=& \\displaystyle\\frac{\\mbox{SS}_b }{  \\mbox{df}_b } &=& "
"\\displaystyle\\frac{3.453}{ 2} &=& 1.727 \\\\\n"
"\\mbox{MS}_w &=& \\displaystyle\\frac{\\mbox{SS}_w }{  \\mbox{df}_w } &=& "
"\\displaystyle\\frac{1.392}{15} &=& 0.093\n"
"\\end{array}"

#: ../../Ch13/Ch13_ANOVA_02.rst:568
msgid ""
"We’re almost done. The mean square values can be used to calculate the *F*-"
"value, which is the test statistic that we’re interested in. We do this by "
"dividing the between-groups MS value by the within-groups MS value.\\ [#]_"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:573
msgid ""
"F = \\frac{\\mbox{MS}_b }{\\mbox{MS}_w} = \\frac{1.727}{0.093} = 18.611\n"
"\n"
msgstr ""
"F = \\frac{\\mbox{MS}_b }{\\mbox{MS}_w} = \\frac{1.727}{0.093} = 18.611\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_02.rst:575
msgid ""
"Woohooo! This is terribly exciting, yes? Now that we have our test "
"statistic, the last step is to find out whether the test itself gives us a "
"significant result. As discussed in chapter :doc:`../Ch09/"
"Ch09_HypothesisTesting` back in the “old days” what we’d do is open up a "
"statistics textbook or flick to the back section which would actually have a "
"huge lookup table and we would find the threshold *F*-value corresponding to "
"a particular value of α (the null hypothesis rejection region), e.g. 0.05, "
"0.01 or 0.001, for 2 and 15 degrees of freedom. Doing it this way would give "
"us a threshold *F*-value for an α of 0.001 of 11.34. As this is less than "
"our calculated *F*-value we say that *p* < 0.001. But those were the old "
"days, and nowadays fancy stats software calculates the exact *p*-value for "
"you. In fact, the exact *p*-value is 0.000086. So, unless we’re being "
"*extremely* conservative about our Type I error rate, we’re pretty much "
"guaranteed to reject the null hypothesis."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:589
msgid ""
"At this point, we’re basically done. Having completed our calculations, it’s "
"traditional to organise all these numbers into an ANOVA table like the one "
"in :numref:`tab-anovatable`. For our |clinicaltrial|_ data, the ANOVA table "
"would look like this:\\ [#]_"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:597
msgid "3.453"
msgstr "3.453"

#: ../../Ch13/Ch13_ANOVA_02.rst:597
msgid "1.727"
msgstr "1.727"

#: ../../Ch13/Ch13_ANOVA_02.rst:597
msgid "18.611"
msgstr "18.611"

#: ../../Ch13/Ch13_ANOVA_02.rst:597
msgid "0.000086"
msgstr "0.000086"

#: ../../Ch13/Ch13_ANOVA_02.rst:599
msgid "15"
msgstr "15"

#: ../../Ch13/Ch13_ANOVA_02.rst:599
msgid "1.392"
msgstr "1.392"

#: ../../Ch13/Ch13_ANOVA_02.rst:599
msgid "0.093"
msgstr "0.093"

#: ../../Ch13/Ch13_ANOVA_02.rst:602
msgid ""
"These days, you’ll probably never have much reason to want to construct one "
"of these tables yourself, but you will find that almost all statistical "
"software (jamovi included) tends to organise the output of an ANOVA into a "
"table like this, so it’s a good idea to get used to reading them. However, "
"although the software will output a full ANOVA table, there’s almost never a "
"good reason to include the whole table in your write up. A pretty standard "
"way of reporting this result would be to write something like this:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:611
msgid ""
"One-way ANOVA showed a significant effect of drug on mood gain: *F*\\(2,15) "
"= 18.61, *p* < 0.001."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:614
msgid "Sigh. So much work for one short sentence."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:619
msgid ""
"When all groups have the same number of observations, the experimental "
"design is said to be “balanced”. Balance isn’t such a big deal for one-way "
"ANOVA, which is the topic of this chapter. It becomes more important when "
"you start doing more complicated ANOVAs."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:625
msgid ""
"|SS_w| is also referred to in an independent ANOVA as the error variance, or "
"SS\\ :sub:`error`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:629
msgid ""
"If you read ahead to chapter :doc:`../Ch14/Ch14_ANOVA2` and look at how the "
"“treatment effect” at level *k* of a factor is defined in terms of the α\\ :"
"sub:`k` values (see section :doc:`../Ch14/Ch14_ANOVA2_02`), it turns out "
"that *Q* refers to a weighted mean of the squared treatment effects, :math:"
"`Q = (\\sum_{k=1}^G N_k \\alpha_k^2)/(G-1)`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:636
msgid ""
"Or, if we want to be sticklers for accuracy, :math:`1 + \\frac{2}{df_2 - 2}`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:640
msgid ""
"Or, to be precise, party like “it’s 1899 and we’ve got no friends and "
"nothing better to do with our time than do some calculations that wouldn’t "
"have made any sense in 1899 because ANOVA didn’t exist until about the "
"1920s”."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:646
msgid ""
"We could as well do this with creating yet another computed variable, named "
"``F`` using the formula ``(VSUM(sq_res_btw) / 2) / (VSUM(sq_res_wth) / 15)`` "
"which gives us 18.611 as value. If you could not reprodcuce the calculation "
"steps above, you can download and open the |clinicaltrial_anova|_ data set."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_02.rst:653
msgid ""
"In order to see the *p*-value with a high number of decimal places, click on "
"the settings menu (``⋮``, top-right corner) and set the ``p-value format`` "
"to ``16 dp``."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:4
msgid "Running an ANOVA in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:6
msgid ""
"I’m pretty sure I know what you’re thinking after reading the last section, "
"*especially* if you followed my advice and did all of that by pencil and "
"paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations "
"yourself *sucks*. There’s quite a lot of calculations that we needed to do "
"along the way, and it would be tedious to have to do this over and over "
"again every time you wanted to do an ANOVA."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:14
msgid "Using jamovi to specify your ANOVA"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:16
msgid ""
"To make life easier for you, jamovi can do ANOVA… hurrah! Go to the "
"``ANOVA`` - ``ANOVA`` analysis, and move the ``mood.gain`` variable across "
"so it is in the ``Dependent Variable`` box, and then move the ``drug`` "
"variable across so it is in the ``Fixed Factors`` box. This should give the "
"results as shown in :numref:`fig-anova2`.\\ [#]_ Note I have also checked "
"the η² checkbox, pronounced “eta” squared, under the ``Effect Size`` option "
"and this is also shown on the results table. We will come back to effect "
"sizes a bit later."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:27
msgid "``ANOVA`` results table for ``mood.gain`` by ``drug`` administered"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:31
msgid ""
"jamovi ``ANOVA`` results table for ``mood.gain`` by ``drug`` administered"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:35
msgid ""
"The jamovi results table shows you the sums of squares values, the degrees "
"of freedom, and a couple of other quantities that we’re not really "
"interested in right now. Notice, however, that jamovi doesn’t use the names "
"“between-group” and “within-group”. Instead, it tries to assign more "
"meaningful names. In our particular example, the *between groups* variance "
"corresponds to the effect that the ``drug`` has on the outcome variable, and "
"the *within groups* variance corresponds to the “leftover” variability so it "
"calls that the *residuals*. If we compare these numbers to the numbers that "
"I calculated by hand in section :ref:`A worked example <worked_example>`, "
"you can see that they’re more or less the same, apart from rounding errors. "
"The between groups sums of squares is SS\\ :sub:`b` = 3.45, the within "
"groups sums of squares is SS\\ :sub:`w` = 1.39, and the degrees of freedom "
"are 2 and 15 respectively. We also get the *F*-value and the *p*-value and, "
"again, these are more or less the same, give or take rounding errors, to the "
"numbers that we calculated ourselves when doing it the long and tedious way."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_03.rst:54
msgid ""
"The jamovi results are more accurate than the ones in the text above, due to "
"rounding errors."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_04.rst:4
msgid "Effect size"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_04.rst:6
msgid ""
"There’s a few different ways you could measure the effect size in an ANOVA, "
"but the most commonly used measures are η² (**eta squared**) and partial η². "
"For a one-way analysis of variance they’re identical to each other, so for "
"the moment I’ll just explain η². The definition of η² is actually really "
"simple"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_04.rst:13
msgid "η² = SS\\ :sub:`b` / SS\\ :sub:`tot`"
msgstr "η² = SS\\ :sub:`b` / SS\\ :sub:`tot`"

#: ../../Ch13/Ch13_ANOVA_04.rst:15
msgid ""
"That’s all it is. So when I look at the ANOVA table in :numref:`fig-anova2`, "
"I see that SS\\ :sub:`b`   = 3.45 and SS\\ :sub:`tot` = 3.45 + 1.39 = 4.84. "
"Thus we get an η² value of"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_04.rst:20
msgid "η² = 3.45 / 4.84 = 0.71"
msgstr "η² = 3.45 / 4.84 = 0.71"

#: ../../Ch13/Ch13_ANOVA_04.rst:22
msgid ""
"The interpretation of η² is equally straightforward. It refers to the "
"proportion of the variability in the outcome variable (``mood.gain``) that "
"can be explained in terms of the predictor (``drug``). A value of η² = 0 "
"means that there is no relationship at all between the two, whereas a value "
"of η = 1 means that the relationship is perfect. Better yet, the η² value is "
"very closely related to *R*\\², as discussed previously in subsection :doc:"
"`The *R*\\² (R-squared) value <../Ch12/Ch12_Regression_06>`, and has an "
"equivalent interpretation."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_04.rst:31
msgid ""
"Although many statistics text books suggest η² as the default effect size "
"measure in ANOVA, there’s an interesting `blog post <https://daniellakens."
"blogspot.com.au/2015/06/why-you-should-use-omega-squared.html>`__ by Daniel "
"Lakens suggesting that eta-squared is perhaps not the best measure of effect "
"size in real world data analysis, because it can be a biased estimator. "
"Usefully, there is also an option in jamovi to specify omega-squared (ω²), "
"which is less biased, alongside eta-squared."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:4
msgid "Multiple comparisons and post-hoc tests"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:6
msgid ""
"Any time you run an ANOVA with more than two groups and you end up with a "
"significant effect, the first thing you’ll probably want to ask is which "
"groups are actually different from one another. In our drugs example, our "
"null hypothesis was that all three drugs (placebo, Anxifree and Joyzepam) "
"have the exact same effect on mood. But if you think about it, the null "
"hypothesis is actually claiming *three* different things all at once here. "
"Specifically, it claims that:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:14
msgid ""
"Your competitor’s drug (Anxifree) is no better than a placebo (i.e., µ\\ :"
"sub:`A` = µ\\ :sub:`P`)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:17
msgid ""
"Your drug (Joyzepam) is no better than a placebo (i.e., µ\\ :sub:`J` = µ\\ :"
"sub:`P`)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:20
msgid ""
"Anxifree and Joyzepam are equally effective (i.e., µ\\ :sub:`J` = µ\\ :sub:"
"`A`)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:23
msgid ""
"If any one of those three claims is false, then the null hypothesis is also "
"false. So, now that we’ve rejected our null hypothesis, we’re thinking that "
"*at least* one of those things isn’t true. But which ones? All three of "
"these propositions are of interest. Since you certainly want to know if your "
"new drug Joyzepam is better than a placebo, it would be nice to know how "
"well it stacks up against an existing commercial alternative (i.e., "
"Anxifree). It would even be useful to check the performance of Anxifree "
"against the placebo. Even if Anxifree has already been extensively tested "
"against placebos by other researchers, it can still be very useful to check "
"that your study is producing similar results to earlier work."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:35
msgid ""
"When we characterise the null hypothesis in terms of these three distinct "
"propositions, it becomes clear that there are eight possible “states of the "
"world” that we need to distinguish between:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:40
msgid "possibility:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:40
msgid "is µ\\ :sub:`P` = µ\\ :sub:`A`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:40
msgid "is µ\\ :sub:`P` = µ\\ :sub:`J`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:40
msgid "is µ\\ :sub:`A` = µ\\ :sub:`J`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:40
msgid "which hypothesis?"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:43 ../../Ch13/Ch13_ANOVA_05.rst:45
#: ../../Ch13/Ch13_ANOVA_05.rst:47 ../../Ch13/Ch13_ANOVA_05.rst:49
#: ../../Ch13/Ch13_ANOVA_05.rst:51 ../../Ch13/Ch13_ANOVA_05.rst:53
#: ../../Ch13/Ch13_ANOVA_05.rst:55
msgid "✓"
msgstr "✓"

#: ../../Ch13/Ch13_ANOVA_05.rst:43
msgid "null"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:45 ../../Ch13/Ch13_ANOVA_05.rst:47
#: ../../Ch13/Ch13_ANOVA_05.rst:49 ../../Ch13/Ch13_ANOVA_05.rst:51
#: ../../Ch13/Ch13_ANOVA_05.rst:53 ../../Ch13/Ch13_ANOVA_05.rst:55
#: ../../Ch13/Ch13_ANOVA_05.rst:57
msgid "alternative"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:55 ../../Ch13/Ch13_ANOVA_07.rst:66
#: ../../Ch13/Ch13_ANOVA_07.rst:68 ../../Ch13/Ch13_ANOVA_07.rst:76
msgid "7"
msgstr "7"

#: ../../Ch13/Ch13_ANOVA_05.rst:57 ../../Ch13/Ch13_ANOVA_07.rst:66
#: ../../Ch13/Ch13_ANOVA_07.rst:68 ../../Ch13/Ch13_ANOVA_07.rst:76
msgid "8"
msgstr "8"

#: ../../Ch13/Ch13_ANOVA_05.rst:60
msgid ""
"By rejecting the null hypothesis, we’ve decided that we *don’t* believe that "
"#1 is the true state of the world. The next question to ask is, which of the "
"other seven possibilities *do* we think is right? When faced with this "
"situation, its usually helps to look at the data. For instance, if we look "
"at the plots in :numref:`fig-anova1`, it’s tempting to conclude that "
"Joyzepam is better than the placebo and better than Anxifree, but there’s no "
"real difference between Anxifree and the placebo. However, if we want to get "
"a clearer answer about this, it might help to run some tests."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:71
msgid "Running “pairwise” *t*-tests"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:73
msgid ""
"How might we go about solving our problem? Given that we’ve got three "
"separate pairs of means (placebo versus Anxifree, placebo versus Joyzepam, "
"and Anxifree versus Joyzepam) to compare, what we could do is run three "
"separate *t*-tests and see what happens. This is easy to do in jamovi. Go to "
"the ``ANOVA`` → ``Post Hoc Tests`` options, move the ``drug`` variable "
"across into the active box on the right, and then click on the ``No "
"correction`` checkbox. This will produce a neat table showing all the "
"pairwise *t*-test comparisons amongst the three levels of the ``drug`` "
"variable, as in :numref:`fig-anova3`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:85 ../../Ch13/Ch13_ANOVA_05.rst:89
msgid "Uncorrected pairwise *t*-tests as post-hoc comparisons in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:94
msgid "Corrections for multiple testing"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:96
msgid ""
"In the previous section I hinted that there’s a problem with just running "
"lots and lots of *t*-tests. The concern is that, when running these "
"analyses, what we’re doing is going on a “fishing expedition”. We’re running "
"lots and lots of tests without much theoretical guidance in the hope that "
"some of them come up significant. This kind of theory-free search for group "
"differences is referred to as **post-hoc analysis** (“post-hoc” being Latin "
"for “after this”).\\ [#]_"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:104
msgid ""
"It’s okay to run post-hoc analyses, but a lot of care is required. For "
"instance, the analysis that I ran in the previous section should be avoided, "
"as each *individual* *t*-test is designed to have a 5\\% Type I error rate "
"(i.e., α = 0.05) and I ran three of these tests. Imagine what would have "
"happened if my ANOVA involved 10 different groups, and I had decided to run "
"45 “post-hoc” *t*-tests to try to find out which ones were significantly "
"different from each other, you’d expect 2 or 3 of them to come up "
"significant *by chance alone*. As we saw in chapter :doc:`../Ch09/"
"Ch09_HypothesisTesting`, the central organising principle behind null "
"hypothesis testing is that we seek to control our Type I error rate, but now "
"that I’m running lots of *t*-tests at once in order to determine the source "
"of my ANOVA results, my actual Type I error rate across this whole *family* "
"of tests has gotten completely out of control."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:117
msgid ""
"The usual solution to this problem is to introduce an adjustment to the *p*-"
"value, which aims to control the total error rate across the family of tests "
"(:ref:`Shaffer, 1995 <Shaffer_1995>`). An adjustment of this form, which is "
"usually (but not always) applied because one is doing post-hoc analysis, is "
"often referred to as a **correction for multiple comparisons**, though it is "
"sometimes referred to as “simultaneous inference”. In any case, there are "
"quite a few different ways of doing this adjustment. I’ll discuss a few of "
"them in this section and in section :doc:`../Ch14/Ch14_ANOVA2_09`, but you "
"should be aware that there are many other methods out there (:ref:`Hsu, 1996 "
"<Hsu_1996>`)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:129
msgid "Bonferroni corrections"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:131
msgid ""
"The simplest of these adjustments is called the **Bonferroni correction** (:"
"ref:`Dunn, 1961 <Dunn_1961>`), and it’s very very simple indeed. Suppose "
"that my post-hoc analysis consists of *m* separate tests, and I want to "
"ensure that the total probability of making *any* Type I errors at all is at "
"most α.\\ [#]_ If so, then the Bonferroni correction just says “multiply all "
"your raw *p*-values by *m*”. If we let *p* denote the original *p*-value, "
"and let *p*'\\ :sub:`j` be the corrected value, then the Bonferroni "
"correction tells that:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:140
msgid "*p*'\\ :sub:`j` = *m* × *p*"
msgstr "*p*'\\ :sub:`j` = *m* × *p*"

#: ../../Ch13/Ch13_ANOVA_05.rst:142
msgid ""
"And therefore, if you’re using the Bonferroni correction, you would reject "
"the null hypothesis if *p*'\\ :sub:`j` < α. The logic behind this correction "
"is very straightforward. We’re doing *m* different tests, so if we arrange "
"it so that each test has a Type I error rate of at most α / *m*, then the "
"*total* Type I error rate across these tests cannot be larger than α. That’s "
"pretty simple, so much so that in the original paper, the author writes,"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:150
msgid ""
"The method given here is so simple and so general that I am sure it must "
"have been used before this. I do not find it, however, so can only conclude "
"that perhaps its very simplicity has kept statisticians from realizing that "
"it is a very good method in some situations (:ref:`Dunn, 1961 <Dunn_1961>`, "
"pp. 52-53)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:156
msgid ""
"To use the Bonferroni correction in jamovi, just click on the ``Bonferroni`` "
"checkbox in the ``Correction`` options, and you will see another column "
"added to the ``ANOVA`` results table showing the adjusted *p*-values for the "
"Bonferroni correction (:numref:`fig-anova3`). If we compare these three *p*-"
"values to those for the uncorrected, pairwise *t*-tests, it is clear that "
"the only thing that jamovi has done is multiply them by 3."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:165
msgid "Holm corrections"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:167
msgid ""
"Although the Bonferroni correction is the simplest adjustment out there, "
"it’s not usually the best one to use. One method that is often used instead "
"is the **Holm correction** (:ref:`Holm, 1979 <Holm_1979>`). The idea behind "
"the Holm correction is to pretend that you’re doing the tests sequentially, "
"starting with the smallest (raw) *p*-value and moving onto the largest one. "
"For the *j*-th largest of the *p*-values, the adjustment is *either*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:174
msgid "*p*'\\ :sub:`j` = j × *p*\\ :sub:`j`"
msgstr "*p*'\\ :sub:`j` = j × *p*\\ :sub:`j`"

#: ../../Ch13/Ch13_ANOVA_05.rst:176
msgid ""
"(i.e., the biggest *p*-value remains unchanged, the second biggest *p*-value "
"is doubled, the third biggest *p*-value is tripled, and so on), *or*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:180
msgid "*p*'\\ :sub:`j` = *p*'\\ :sub:`j + 1`"
msgstr "*p*'\\ :sub:`j` = *p*'\\ :sub:`j + 1`"

#: ../../Ch13/Ch13_ANOVA_05.rst:182
msgid ""
"whichever one is larger. This might sound a little confusing, so let’s go "
"through it a little more slowly. Here’s what the Holm correction does. "
"First, you sort all of your *p*-values in order, from smallest to largest. "
"For the smallest *p*-value all you do is multiply it by *m*, and you’re "
"done. However, for all the other ones it’s a two-stage process. For "
"instance, when you move to the second smallest *p*-value, you first multiply "
"it by *m* - 1. If this produces a number that is bigger than the adjusted "
"*p*-value that you got last time, then you keep it. But if it’s smaller than "
"the last one, then you copy the last *p*-value. To illustrate how this "
"works, consider the table below, which shows the calculations of a Holm "
"correction for a collection of five *p*-values:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:196
msgid "raw *p* rank"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:196
msgid "*j*"
msgstr "*j*"

#: ../../Ch13/Ch13_ANOVA_05.rst:196
msgid "*p* × *j*"
msgstr "*p* × *j*"

#: ../../Ch13/Ch13_ANOVA_05.rst:196
msgid "Holm *p*"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:198
msgid ".001"
msgstr ".001"

#: ../../Ch13/Ch13_ANOVA_05.rst:198
msgid "0.005"
msgstr "0.005"

#: ../../Ch13/Ch13_ANOVA_05.rst:200
msgid ".005"
msgstr ".005"

#: ../../Ch13/Ch13_ANOVA_05.rst:200
msgid "0.020"
msgstr "0.020"

#: ../../Ch13/Ch13_ANOVA_05.rst:202
msgid ".019"
msgstr ".019"

#: ../../Ch13/Ch13_ANOVA_05.rst:202 ../../Ch13/Ch13_ANOVA_05.rst:204
msgid "0.057"
msgstr "0.057"

#: ../../Ch13/Ch13_ANOVA_05.rst:204
msgid ".022"
msgstr ".022"

#: ../../Ch13/Ch13_ANOVA_05.rst:204
msgid "0.044"
msgstr "0.044"

#: ../../Ch13/Ch13_ANOVA_05.rst:206
msgid ".103"
msgstr ".103"

#: ../../Ch13/Ch13_ANOVA_05.rst:206
msgid "0.103"
msgstr "0.103"

#: ../../Ch13/Ch13_ANOVA_05.rst:209
msgid "Hopefully that makes things clear."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:211
msgid ""
"Although it’s a little harder to calculate, the Holm correction has some "
"very nice properties. It’s more powerful than Bonferroni (i.e., it has a "
"lower Type II error rate) but, counter-intuitive as it might seem, it has "
"the *same* Type I error rate. As a consequence, in practice there’s never "
"any reason to use the simpler Bonferroni correction since it is always "
"outperformed by the slightly more elaborate Holm correction. Because of "
"this, the Holm correction should be your *go to* multiple comparison "
"correction. :numref:`fig-anova3` also shows the Holm corrected *p*-values "
"and, as you can see, the biggest *p*-value (corresponding to the comparison "
"between Anxifree and the placebo) is unaltered. At a value of 0.15, it is "
"exactly the same as the value we got originally when we applied no "
"correction at all. In contrast, the smallest *p*-value (Joyzepam versus "
"placebo) has been multiplied by three."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:226
msgid "Writing up the post-hoc test"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:228
msgid ""
"Finally, having run the post-hoc analysis to determine which groups are "
"significantly different to one another, you might write up the result like "
"this:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:232
msgid ""
"Post-hoc tests (using the Holm correction to adjust *p*) indicated that "
"Joyzepam produced a significantly larger mood change than both Anxifree (*p* "
"= 0.001) and the placebo (*p* = 9.0 · 10\\ :sup:`-5`). We found no evidence "
"that Anxifree performed better than the placebo (*p* = 0.15)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:238
msgid ""
"Or, if you don’t like the idea of reporting exact *p*-values, then you’d "
"change those numbers to *p* < 0.001`, *p* < 0.01 and *p* > 0.05 "
"respectively. Either way, the key thing is that you indicate that you used "
"Holm’s correction to adjust the *p*-values. And of course, I’m assuming that "
"elsewhere in the write up you’ve included the relevant descriptive "
"statistics (i.e., the group means and standard deviations), since these *p*-"
"values on their own aren’t terribly informative."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:250
msgid ""
"If you *do* have some theoretical basis for wanting to investigate some "
"comparisons but not others, it’s a different story. In those circumstances "
"you’re not really running “post-hoc” analyses at all, you’re making “planned "
"comparisons”. I do talk about this situation later in the book in section :"
"doc:`../Ch14/Ch14_ANOVA2_10`), but for now I want to keep things simple."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_05.rst:257
msgid ""
"It’s worth noting in passing that not all adjustment methods try to do this. "
"What I’ve described here is an approach for controlling “family wise Type I "
"error rate”. However, there are other post-hoc tests that seek to control "
"the “false discovery rate”, which is a somewhat different thing."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:4
msgid "Assumptions of the one-way ANOVA"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:6
msgid ""
"Like any statistical test, analysis of variance relies on some assumptions "
"about the data, specifically the residuals. There are three key assumptions "
"that you need to be aware of: *normality*, *homogeneity of variance* and "
"*independence*."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:11
msgid ""
"If you remember back to subsection :ref:`The model for the data and the "
"meaning of *F* <meaning_of_F>` which I hope you at least skimmed even if you "
"didn’t read the whole thing, I described the statistical models underpinning "
"ANOVA in this way:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:16
msgid "H\\ :sub:`0`: Y\\ :sub:`ik` = µ           + ϵ\\ :sub:`ik`"
msgstr "H\\ :sub:`0`: Y\\ :sub:`ik` = µ           + ϵ\\ :sub:`ik`"

#: ../../Ch13/Ch13_ANOVA_06.rst:17
msgid "H\\ :sub:`1`: Y\\ :sub:`ik` = µ\\ :sub:`k` + ϵ\\ :sub:`ik`"
msgstr "H\\ :sub:`1`: Y\\ :sub:`ik` = µ\\ :sub:`k` + ϵ\\ :sub:`ik`"

#: ../../Ch13/Ch13_ANOVA_06.rst:19
msgid ""
"In these equations µ refers to a single grand population mean which is the "
"same for all groups, and µ\\ :sub:`k` is the population mean for the *k*-th "
"group. Up to this point we’ve been mostly interested in whether our data are "
"best described in terms of a single grand mean (the null hypothesis) or in "
"terms of different group-specific means (the alternative hypothesis). This "
"makes sense, of course, as that’s actually the important research question! "
"However, all of our testing procedures have, implicitly, relied on a "
"specific assumption about the residuals, ϵ\\ :sub:`ik`, namely that"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:31
msgid ""
"None of the maths works properly without this bit. Or, to be precise, you "
"can still do all the calculations and you’ll end up with an *F*-statistic, "
"but you have no guarantee that this *F*-statistic actually measures what you "
"think it’s measuring, and so any conclusions that you might draw on the "
"basis of the *F* test might be wrong."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:38
msgid ""
"So, how do we check whether the assumption about the residuals is accurate? "
"Well, as I indicated above, there are three distinct claims buried in this "
"one statement, and we’ll consider them separately."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:42
msgid ""
"**Homogeneity of variance**. Notice that we’ve only got the one value for "
"the population standard deviation (i.e., σ), rather than allowing each group "
"to have it’s own value (i.e., σ\\ :sub:`k`). This is referred to as the "
"homogeneity of variance (sometimes called homoscedasticity) assumption. "
"ANOVA assumes that the population standard deviation is the same for all "
"groups. We’ll talk about this extensively in subsection :ref:`Checking the "
"homogeneity of variance assumption <homogeneity_of_variance_anova>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:50
msgid ""
"**Normality**. The residuals are assumed to be normally distributed. As we "
"saw in subsection :doc:`../Ch11/Ch11_tTest_08`, we can assess this by "
"looking at QQ-plots (or running a Shapiro-Wilk test). I’ll talk about this "
"more in an ANOVA context in subsection :ref:`Checking the normality "
"assumption <normality_anova>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:56
msgid ""
"**Independence**. The independence assumption is a little trickier. What it "
"basically means is that, knowing one residual tells you nothing about any "
"other residual. All of the ϵ\\ :sub:`ik` values are assumed to have been "
"generated without any “regard for” or “relationship to” any of the other "
"ones. There’s not an obvious or simple way to test for this, but there are "
"some situations that are clear violations of this. For instance, if you have "
"a repeated-measures design, where each participant in your study appears in "
"more than one condition, then independence doesn’t hold. There’s a special "
"relationship between some observations, namely those that correspond to the "
"same person! When that happens, you need to use something like repeated "
"measures ANOVA (see section :doc:`Ch13_ANOVA_07`)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:72
msgid "Checking the homogeneity of variance assumption"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:82
msgid ""
"There’s more than one way to skin a cat, as the saying goes, and more than "
"one way to test the homogeneity of variance assumption, too (though for some "
"reason no-one made a saying out of that). The most commonly used test for "
"this that I’ve seen in the literature is the **Levene test** (:ref:`Levene, "
"1960 <Levene_1960>`), and the closely related **Brown-Forsythe test** (:ref:"
"`Brown & Forsythe, 1974 <Brown_1974>`)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:89
msgid ""
"Regardless of whether you’re doing the standard Levene test or the Brown-"
"Forsythe test, the test statistic, which is sometimes denoted *F* but also "
"sometimes written as *W*, is calculated in exactly the same way that the *F*-"
"statistic for the regular ANOVA is calculated, just using a Z\\ :sub:`ik` "
"rather than Y\\ :sub:`ik`. With that in mind, we can go on to look at how to "
"run the test in jamovi."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:97
msgid ""
"The Levene test is shockingly simple. Suppose we have our outcome variable "
"Y\\ :sub:`ik`. All we do is define a new variable, which I’ll call Z\\ :sub:"
"`ik`, corresponding to the absolute deviation from the group mean"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:102
msgid "Z\\ :sub:`ik` = Y\\ :sub:`ik` - Ȳ\\ :sub:`k`"
msgstr "Z\\ :sub:`ik` = Y\\ :sub:`ik` - Ȳ\\ :sub:`k`"

#: ../../Ch13/Ch13_ANOVA_06.rst:104
msgid ""
"Okay, what good does this do us? Well, let’s take a moment to think about "
"what Z\\ :sub:`ik` actually is and what we’re trying to test. The value of "
"Z\\ :sub:`ik` is a measure of how the *i*-th observation in the *k*-th group "
"deviates from its group mean. And our null hypothesis is that all groups "
"have the same variance, i.e., the same overall deviations from the group "
"means! So the null hypothesis in a Levene test is that the population means "
"of Z are identical for all groups. Hmm. So what we need now is a statistical "
"test of the null hypothesis that all group means are identical. Where have "
"we seen that before? Oh right, that’s what ANOVA is, and so all that the "
"Levene test does is run an ANOVA on the new variable Z\\ :sub:`ik`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:116
msgid ""
"What about the Brown-Forsythe test? Does that do anything particularly "
"different? Nope. The only change from the Levene test is that it constructs "
"the transformed variable *Z* in a slightly different way, using deviations "
"from the group *medians* rather than deviations from the group *means*. That "
"is, for the Brown-Forsythe test"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:122
msgid "Z\\ :sub:`ik` = Y\\ :sub:`ik` - median\\ :sub:`k(Y)`"
msgstr "Z\\ :sub:`ik` = Y\\ :sub:`ik` - median\\ :sub:`k(Y)`"

#: ../../Ch13/Ch13_ANOVA_06.rst:124
msgid "where median\\ :sub:`k(Y)` is the median for group *k*."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:127
msgid "Running the Levene-test in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:129
msgid ""
"Okay, so how do we run the Levene test? Simple really - under the ``ANOVA`` "
"→ ``Assumption Checks`` option, just click on the ``Homogeneity tests`` "
"checkbox. If we look at the output, shown in :numref:`fig-anova4`, we see "
"that the test is non-significant (*F*\\{2,15} = 1.45, *p* = 0.266), so it "
"looks like the homogeneity of variance assumption is fine. However, looks "
"can be deceptive! If your sample size is pretty big, then the Levene test "
"could show up a significant effect (i.e., *p* < 0.05) even when the "
"homogeneity of variance assumption is not violated to an extent which "
"troubles the robustness of ANOVA. This was the point George Box was making "
"in the quote above. Similarly, if your sample size is quite small, then the "
"homogeneity of variance assumption might not be satisfied and yet a Levene "
"test could be non-significant (i.e. *p* > 0.05). What this means is that, "
"alongside any statistical test of the assumption being met, you should "
"always plot the standard deviation around the means for each group / "
"category in the analysis… just to see if they look fairly similar (i.e. "
"homogeneity of variance) or not."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:148 ../../Ch13/Ch13_ANOVA_06.rst:152
msgid "``Levene test`` output for ``One-Way ANOVA`` in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:157
msgid "Removing the homogeneity of variance assumption"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:159
msgid ""
"In our example, the homogeneity of variance assumption turned out to be a "
"pretty safe one: the Levene test came back non-significant (notwithstanding "
"that we should also look at the plot of standard deviations), so we probably "
"don’t need to worry. However, in real life we aren’t always that lucky. How "
"do we save our ANOVA when the homogeneity of variance assumption is "
"violated? If you recall from our discussion of *t*-tests, we’ve seen this "
"problem before. The Student *t*-test assumes equal variances, so the "
"solution was to use the Welch *t*-test, which does not. In fact, :ref:`Welch "
"(1951) <Welch_1951>` also showed how we can solve this problem for ANOVA too "
"(the **Welch One-way test**). It’s implemented in jamovi using the ``One-Way "
"ANOVA`` analysis. This is a specific analysis approach just for one-way "
"ANOVA, and to run the Welch one-way ANOVA for our example, we would re-run "
"the analysis as previously, but this time use the jamovi ``ANOVA`` → ``One "
"Way ANOVA`` analysis command, and check the option ``Don't assume equal "
"(Welch’s)`` (see :numref:`fig-anova4a`)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:176 ../../Ch13/Ch13_ANOVA_06.rst:180
msgid "Welch’s test as part of the One-Way ANOVA analysis in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:184
msgid ""
"To understand what’s happening here, let’s compare these numbers with those "
"obtained earlier in section :doc:`Ch13_ANOVA_03`, namely: *F*\\(2,15) = "
"18.611, *p* = 0.00009. As shown in :numref:`fig-anova4a`, these values are "
"also displayed in the ``One-Way ANOVA`` table (in the row starting with "
"``Fisher's``) if the option ``Assume equal (Fisher's)`` was chosen."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:190
msgid ""
"Okay, so originally our ANOVA gave us the result *F*\\(2,15) = 18.6, whereas "
"the Welch one-way test gave us *F*\\(2,9.49) = 26.32. In other words, the "
"Welch test has reduced the within-groups degrees of freedom from 15 to 9.49, "
"and the *F*-value has increased from 18.6 to 26.32."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:199
msgid "Checking the normality assumption"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:201
msgid ""
"Testing the normality assumption is relatively straightforward. We covered "
"most of what you need to know in section :doc:`../Ch11/Ch11_tTest_08`. The "
"only thing we really need to do is draw a QQ plot and, in addition if it is "
"available, run the Shapiro-Wilk test. The QQ plot is shown in :numref:`fig-"
"anova5` and it looks pretty normal to me. If the Shapiro-Wilk test is not "
"significant (i.e. *p* > 0.05) then this indicates that the assumption of "
"normality is not violated. However, as with Levene’s test, if the sample "
"size is large then a significant Shapiro-Wilk test may in fact be a false "
"positive, where the assumption of normality is not violated in any "
"substantive problematic sense for the analysis. And, similarly, a very small "
"sample can produce false negatives. That’s why a visual inspection of the QQ "
"plot is important."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:214
msgid ""
"Alongside inspecting the QQ plot for any deviations from normality, the "
"Shapiro-Wilk test for our data does show a non-significant effect, with *p* "
"= 0.6053 (see :numref:`fig-anova4a`). This therefore supports the QQ plot "
"assessment; both checks find no indication that normality is violated."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:222 ../../Ch13/Ch13_ANOVA_06.rst:226
msgid "QQ-plot produced from jamovi One-Way ANOVA options"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:231
msgid "Removing the normality assumption"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:233
msgid ""
"Now that we’ve seen how to check for normality, we are led naturally to ask "
"what we can do to address violations of normality. In the context of a One-"
"way ANOVA, the easiest solution is probably to switch to a non-parametric "
"test (i.e., one that doesn’t rely on any particular assumption about the "
"kind of distribution involved). We’ve seen non-parametric tests before, in "
"section :doc:`../Ch11/Ch11_tTest_09`. When you only have two groups, the "
"Mann-Whitney or the Wilcoxon test provides the non-parametric alternative "
"that you need. When you’ve got three or more groups, you can use the "
"**Kruskal-Wallis rank sum test** (:ref:`Kruskal & Wallis, 1952 "
"<Kruskal_1952>`). So that’s the test we’ll talk about next."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:244
msgid ""
"The Kruskal-Wallis test is surprisingly similar to ANOVA, in some ways. In "
"ANOVA we started with Y\\ :sub:`ik`, the value of the outcome variable for "
"the *i*-th person in the *k*-th group. For the Kruskal-Wallis test what "
"we’ll do is rank order all of these Y\\ :sub:`ik` values and conduct our "
"analysis on the ranked data."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:250
msgid ""
"So let’s let R\\ :sub:`ik` refer to the ranking given to the *i*-th member "
"of the *k*-th group. Now, let’s calculate R̄\\ :sub:`k`, the average rank "
"given to observations in the *k*-th group:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:254
msgid ""
"\\bar{R}_k = \\frac{1}{N_K} \\sum_{i} R_{ik}\n"
"\n"
msgstr ""
"\\bar{R}_k = \\frac{1}{N_K} \\sum_{i} R_{ik}\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:256
msgid "and let’s also calculate R̄, the grand mean rank"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:258
msgid ""
"\\bar{R} = \\frac{1}{N} \\sum_{i} \\sum_{k} R_{ik}\n"
"\n"
msgstr ""
"\\bar{R} = \\frac{1}{N} \\sum_{i} \\sum_{k} R_{ik}\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:260
msgid ""
"Now that we’ve done this, we can calculate the squared deviations from the "
"grand mean rank R̄. When we do this for the individual scores, i.e., if we "
"calculate (R\\ :sub:`ik` – R̄)², what we have is a “nonparametric” measure of "
"how far the *ik*-th observation deviates from the grand mean rank. When we "
"calculate the squared deviation of the group means from the grand means, i."
"e., if we calculate (R̄\\ :sub:`k` – R̄)², then what we have is a "
"nonparametric measure of how much the *group* deviates from the grand mean "
"rank. With this in mind, we’ll follow the same logic that we did with ANOVA "
"and define our *ranked* sums of squares measures, much like we did earlier. "
"First, we have our “total ranked sums of squares”"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:271
msgid ""
"\\mbox{RSS}_{tot} = \\sum_k \\sum_i ( R_{ik} - \\bar{R} )^2\n"
"\n"
msgstr ""
"\\mbox{RSS}_{tot} = \\sum_k \\sum_i ( R_{ik} - \\bar{R} )^2\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:273
msgid ""
"and we can define the “between groups ranked sums of squares” like this:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:275
msgid ""
"\\begin{array}{rcl}\n"
"\\mbox{RSS}_{b} &=& \\sum_k \\sum_i ( \\bar{R}_k  - \\bar{R} )^2 \\\\\n"
"               &=& \\sum_k N_k ( \\bar{R}_k  - \\bar{R} )^2\n"
"\\end{array}"
msgstr ""
"\\begin{array}{rcl}\n"
"\\mbox{RSS}_{b} &=& \\sum_k \\sum_i ( \\bar{R}_k  - \\bar{R} )^2 \\\\\n"
"               &=& \\sum_k N_k ( \\bar{R}_k  - \\bar{R} )^2\n"
"\\end{array}"

#: ../../Ch13/Ch13_ANOVA_06.rst:282
msgid ""
"So, if the null hypothesis is true and there are no true group differences "
"at all, you’d expect the between group rank sums RSS\\ :sub:`b` to be very "
"small, much smaller than the total rank sums RSS\\ :sub:`tot`. Qualitatively "
"this is very much the same as what we found when we went about constructing "
"the ANOVA *F*-statistic, but for technical reasons the Kruskal-Wallis test "
"statistic, usually denoted *K*, is constructed in a slightly different way,"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:291
msgid ""
"K = (N - 1) \\times \\frac{\\mbox{RSS}_b}{\\mbox{RSS}_{tot}}\n"
"\n"
msgstr ""
"K = (N - 1) \\times \\frac{\\mbox{RSS}_b}{\\mbox{RSS}_{tot}}\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:293
msgid ""
"and if the null hypothesis is true, then the sampling distribution of *K* is "
"*approximately* χ² with *G* - 1 degrees of freedom (where *G* is the number "
"of groups). The larger the value of *K*, the less consistent the data are "
"with the null hypothesis, so this is a one-sided test. We reject H\\ :sub:"
"`0` when *K* is sufficiently large."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:299
msgid ""
"The description in the previous section illustrates the logic behind the "
"Kruskal-Wallis test. At a conceptual level, this is the right way to think "
"about how the test works. However, from a purely mathematical perspective "
"it’s needlessly complicated. I won’t show you the derivation, but you can "
"use a bit of algebraic jiggery-pokery [#]_ to show that the equation for *K* "
"can be rewritten as"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:306
msgid ""
"K = \\frac{12}{N(N-1)} \\sum_k N_k {\\bar{R}_k}^2 - 3(N+1)\n"
"\n"
msgstr ""
"K = \\frac{12}{N(N-1)} \\sum_k N_k {\\bar{R}_k}^2 - 3(N+1)\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:308
msgid ""
"It’s this last equation that you sometimes see given for *K*. This is way "
"easier to calculate than the version I described in the previous section, "
"but it’s just that it’s totally meaningless to actual humans. It’s probably "
"best to think of *K* the way I described it earlier, as an analogue of ANOVA "
"based on ranks. But keep in mind that the test statistic that gets "
"calculated ends up with a rather different look to it than the one we used "
"for our original ANOVA."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:316
msgid ""
"But wait, there’s more! Dear lord, why is there always *more*? The story "
"I’ve told so far is only actually true when there are no ties in the raw "
"data. That is, if there are no two observations that have exactly the same "
"value. If there *are* ties, then we have to introduce a correction factor to "
"these calculations. At this point I’m assuming that even the most diligent "
"reader has stopped caring (or at least formed the opinion that the tie-"
"correction factor is something that doesn’t require their immediate "
"attention). So I’ll very quickly tell you how it’s calculated, and omit the "
"tedious details about *why* it’s done this way. Suppose we construct a "
"frequency table for the raw data, and let f\\ :sub:`j` be the number of "
"observations that have the *j*-th unique value. This might sound a bit "
"abstract, so here’s a concrete example from the frequency table of ``mood."
"gain`` from the |clinicaltrial|_ data set:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.2"
msgstr "0.2"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.8"
msgstr "0.8"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "0.9"
msgstr "0.9"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.1"
msgstr "1.1"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.2"
msgstr "1.2"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.3"
msgstr "1.3"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.4"
msgstr "1.4"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.7"
msgstr "1.7"

#: ../../Ch13/Ch13_ANOVA_06.rst:331
msgid "1.8"
msgstr "1.8"

#: ../../Ch13/Ch13_ANOVA_06.rst:336
msgid ""
"Looking at this table, notice that the third entry in the frequency table "
"has a value of 2. Since this corresponds to a ``mood.gain`` of 0.3, this "
"table is telling us that two people’s mood increased by 0.3. More to the "
"point, in the mathematical notation I introduced above, this is telling us "
"that f\\ :sub:`3` = 2. Yay. So, now that we know this, the tie correction "
"factor (TCF) is:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:343
msgid ""
"\\mbox{TCF} = 1 - \\frac{\\sum_j {f_j}^3 - f_j}{N^3 - N}\n"
"\n"
msgstr ""
"\\mbox{TCF} = 1 - \\frac{\\sum_j {f_j}^3 - f_j}{N^3 - N}\n"
"\n"

#: ../../Ch13/Ch13_ANOVA_06.rst:345
msgid ""
"The tie-corrected value of the Kruskal-Wallis statistic is obtained by "
"dividing the value of *K* by this quantity. It is this tie-corrected version "
"that jamovi calculates. And at long last, we’re actually finished with the "
"theory of the Kruskal-Wallis test. I’m sure you’re all terribly relieved "
"that I’ve cured you of the existential anxiety that naturally arises when "
"you realise that you *don’t* know how to calculate the tie-correction factor "
"for the Kruskal-Wallis test. Right?"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:355
msgid "How to run the Kruskal-Wallis test in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:357
msgid ""
"Despite the horror that we’ve gone through in trying to understand what the "
"Kruskal-Wallis test actually does, it turns out that running the test is "
"pretty painless, since jamovi has an analysis as part of the ``ANOVA`` "
"analysis set called ``Non-Parametric`` - ``One-Way ANOVA (Kruskall-"
"Wallis)``. Most of the time you’ll have data like the |clinicaltrial|_ data "
"set, in which you have your outcome variable ``mood.gain`` and a grouping "
"variable ``drug``. If so, you can just go ahead and run the analysis in "
"jamovi. What this gives us is a Kruskal-Wallis χ² = 12.076, *df* = 2, *p*-"
"value = 0.00239, as in :numref:`fig-anova6`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:369
msgid "non-parametric ``One-Way ANOVA (Kruskal-Wallis)`` in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:373
msgid "Non-parametric ``One-Way ANOVA (Kruskal-Wallis)`` in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_06.rst:380
msgid "A technical term."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:4
msgid "Repeated measures one-way ANOVA"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:6
msgid ""
"The one-way repeated measures ANOVA test is a statistical method of testing "
"for significant differences between three or more groups where the same "
"participants are used in each group (or each participant is closely matched "
"with participants in other experimental groups). For this reason, there "
"should always be an equal number of scores (data points) in each "
"experimental group. This type of design and analysis can also be called a "
"“related ANOVA” or a “within-subjects ANOVA”."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:14
msgid ""
"The logic behind a repeated measures ANOVA is very similar to that of an "
"independent ANOVA (sometimes called a “between-subjects” ANOVA). You’ll "
"remember that earlier we showed that in a between-subjects ANOVA total "
"variability is partitioned into between-groups variability (SS\\ :sub:`b`) "
"and within-groups variability (SS\\ :sub:`w`), and after each is divided by "
"the respective degrees of freedom to give MS\\ :sub:`b` and MS\\ :sub:`w` "
"(see :numref:`tab-anovatable`) the *F*-ratio is calculated as:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:23
msgid "*F* = MS\\ :sub:`b` / MS\\ :sub:`w`"
msgstr "*F* = MS\\ :sub:`b` / MS\\ :sub:`w`"

#: ../../Ch13/Ch13_ANOVA_07.rst:25
msgid ""
"In a repeated measures ANOVA, the *F*-ratio is calculated in a similar way, "
"but whereas in an independent ANOVA the within-group variability (SS\\ :sub:"
"`w`) is used as the basis for the MS\\ :sub:`w` denominator, in a repeated "
"measures ANOVA the SS\\ :sub:`w` is partioned into two parts. As we are "
"using the same subjects in each group, we can remove the variability due to "
"the individual differences between subjects (referred to as SS\\ :sub:"
"`subjects`) from the within-groups variability. We won’t go into too much "
"technical detail about how this is done, but essentially each subject "
"becomes a level of a factor called subjects. The variability in this within-"
"subjects factor is then calculated in the same way as any between-subjects "
"factor. And then we can subtract SS\\ :sub:`subjects` from SS\\ :sub:`w` to "
"provide a smaller SS\\ :sub:`error` term:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:40
msgid "Independent ANOVA:       SS\\ :sub:`error` = SS\\ :sub:`w`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:41
msgid ""
"Repeated Measures ANOVA: SS\\ :sub:`error` = SS\\ :sub:`w - SS\\ :sub:"
"`subjects`"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:43
msgid ""
"This change in SS\\ :sub:`error` term often leads to a more powerful "
"statistical test, but this does depend on whether the reduction in the SS\\ :"
"sub:`error` more than compensates for the reduction in degrees of freedom "
"for the error term: the degrees of freedom go from (*n* - *k*)\\ [#]_ to "
"(*n* - 1)(*k* - 1) remembering that there are more subjects in the "
"independent ANOVA design."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:51
msgid "Repeated measures ANOVA in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:53
msgid ""
"First, we need some data. :ref:`Geschwind (1972) <Geschwind_1972>` has "
"suggested that the exact nature of a patient’s language deficit following a "
"stroke can be used to diagnose the specific region of the brain that has "
"been damaged. A researcher is concerned with identifying the specific "
"communication difficulties experienced by six patients suffering from "
"Broca’s Aphasia (a language deficit commonly experienced following a stroke)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:60
msgid "Number of attempts successfully completed on three experimental tasks."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:64
msgid "Participant"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:64
msgid "Speech"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:64
msgid "Conceptual"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:64
msgid "Syntax"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:70
msgid "9"
msgstr "9"

#: ../../Ch13/Ch13_ANOVA_07.rst:79
msgid ""
"The patients were required to complete three word recognition tasks. On the "
"first (speech production) task, patients were required to repeat single "
"words read out aloud by the researcher. On the second (conceptual) task, "
"designed to test word comprehension, patients were required to match a "
"series of pictures with their correct name. On the third (syntax) task, "
"designed to test knowledge of correct word order, patients were asked to "
"reorder syntactically incorrect sentences. Each patient completed all three "
"tasks. The order in which patients attempted the tasks was counterbalanced "
"between participants. Each task consisted of a series of 10 attempts. The "
"number of attempts successfully completed by each patient are shown in :"
"numref:`tab-RManova`. Enter these data into jamovi ready for analysis (or "
"take a short-cut and load the |broca|_ data set)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:93
msgid ""
"To perform a one-way related ANOVA in jamovi, open the one-way repeated "
"measures ANOVA dialogue box, as in :numref:`fig-RManova1`, via ``ANOVA - "
"Repeated Measures ANOVA``. Then:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:97
msgid ""
"Enter a name for the ``Repeated Measures Factors`` (orginally: ``RM Factor …"
"``). This should be a label that you choose to describe the conditions "
"repeated by all participants. For example, to describe the speech, "
"conceptual and syntax tasks completed by all participants a suitable label "
"would be ``Task``. Note that this new factor name represents the independent "
"variable in the analysis."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:104
msgid ""
"Add a third level in the ``Repeated Measures Factors`` variable box, as "
"there are three levels representing the three tasks: ``Speech``, "
"``Conceptual`` and ``Syntax``. Change the labels of the levels accordingly."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:108
msgid ""
"Then move each of the levels variables across to the ``Repeated Measures "
"Cells`` text box."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:111
msgid ""
"Finally, under the ``Assumption Checks`` option, tick the ``Sphericity "
"checks`` check box."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:116 ../../Ch13/Ch13_ANOVA_07.rst:120
msgid "Repeated measures ANOVA dialogue box in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:124
msgid ""
"jamovi output for a one-way ``Repeated Measures ANOVA`` is produced as shown "
"in the :numref:`fig-RManova2` to :numref:`fig-RManova5`. The first output we "
"should look at is ``Mauchly’s Test of Sphericity``, which tests the "
"hypothesis that the variances of the differences between the conditions are "
"equal (meaning that the spread of difference scores between the study "
"conditions is approximately the same). In :numref:`fig-RManova2`, Mauchly’s "
"test significance level is *p* = 0.720. If Mauchly’s test is non-significant "
"(i.e. *p* > 0.05, as is the case in this analysis) then it is reasonable to "
"conclude that the variances of the differences are not significantly "
"different (i.e. they are roughly equal and sphericity can be assumed.)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:138 ../../Ch13/Ch13_ANOVA_07.rst:142
msgid "One-way repeated measures ANOVA output: Mauchly’s Test of Sphericity"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:146
msgid ""
"If, on the other hand, Mauchly’s test had been significant (*p* < 0.05) then "
"we would conclude that there are significant differences between the "
"variance of the differences, and the requirement of sphericity has not been "
"met. In this case, we should apply a correction to the *F*-value obtained in "
"the one-way related ANOVA analysis:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:153
msgid ""
"If the ``Greenhouse-Geisser`` value in the ``Tests of Sphericity`` table is "
"> 0.75 then you should use the Huynh-Feldt correction."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:156
msgid ""
"But if the ``Greenhouse-Geisser`` value is < 0.75, then you should use the "
"Greenhouse-Geisser correction."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:159
msgid ""
"Both these corrected *F*-values can be specified in the ``Sphericity "
"Corrections`` check boxes under the ``Assumption Checks`` options, and the "
"corrected *F*-values are then shown in the results table, as in :numref:`fig-"
"RManova3`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:166
msgid "Repeated measures ANOVA output: Tests of Within-Subjects Effects"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:170
msgid ""
"One-way repeated measures ANOVA output: Tests of Within-Subjects Effects"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:175
msgid ""
"In our analysis, we saw that the significance of Mauchly’s Test of "
"Sphericity was *p* = 0.720 (i.e. *p* > 0.05). So, this means we can assume "
"that the requirement of sphericity has been met so no correction to the *F*-"
"value is needed. Therefore, we can use the ``None`` Sphericity Correction "
"output values for the repeated measure ``Task``: *F* = 6.93, *df1* = 2, "
"*df2* = 10, *p* = 0.013, and we can conclude that the number of tests "
"successfully completed on each language task did vary significantly "
"depending on whether the task was speech, comprehension or syntax based "
"(*F*\\(2,10) = 6.93, *p* = 0.013)."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:187 ../../Ch13/Ch13_ANOVA_07.rst:191
msgid "Post-hoc tests in repeated measures ANOVA in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:195
msgid ""
"Post-hoc tests can also be specified in jamovi for repeated measures ANOVA "
"in the same way as for independent ANOVA. The results are shown in :numref:"
"`fig-RManova4`. These indicate that there is a significant difference "
"between ``Speech`` and ``Syntax``, but not between other levels."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:201
msgid ""
"Descriptive statistics (marginal means) can be reviewed to help interpret "
"the results, produced in the jamovi output as in :numref:`fig-RManova5`. "
"Comparison of the mean number of trials successfully completed by "
"participants shows that Broca’s Aphasics perform reasonably well on speech "
"production (mean = 7.17) and language comprehension (mean = 6.17) tasks. "
"However, their performance was considerably worse on the syntax task (mean = "
"4.33), with a significant difference in post-hoc tests between ``Speech`` "
"and ``Syntax`` task performance."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:212 ../../Ch13/Ch13_ANOVA_07.rst:216
msgid "One-way repeated measures ANOVA output: Descriptive Statistics"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_07.rst:223
msgid "(n - k): (number of subjects - number of groups)"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:4
msgid "The Friedman non-parametric repeated measures ANOVA test"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:6
msgid ""
"The Friedman test is a non-parametric version of a repeated measures ANOVA "
"and can be used instead of the Kruskall-Wallis test when testing for "
"differences between three or more groups |nominal| where the same "
"participants are in each group, or each participant is closely matched with "
"participants in other conditions. If the dependent variable is ordinal |"
"ordinal|, or if the assumption of normality is not met, then the Friedman "
"test can be used."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:51
msgid "ordinal"
msgstr "ordinal"

#: ../../Ch13/Ch13_ANOVA_08.rst:15 ../../Ch13/Ch13_ANOVA_08.rst:19
msgid "``Repeated Measures ANOVA (Non-parametric)`` dialogue box in jamovi"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:23
msgid ""
"As with the Kruskall-Wallis test, the underlying mathematics is complicated, "
"and won’t be presented here. For the purpose of this book, it is sufficient "
"to note that jamovi calculates the tie-corrected version of the Friedman "
"test, and in :numref:`fig-RManova6` there is an example using the Broca’s "
"Aphasia data we have already looked at."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:29
msgid ""
"It’s pretty straightforward to run a Friedman test in jamovi. Just select "
"``Analyses`` → ``ANOVA`` → ``Repeated Measures ANOVA (Non-parametric)``, as "
"in :numref:`fig-RManova6`. Then highlight and transfer the names of the "
"repeated measures variables you wish to compare (``Speech``, ``Conceptual``, "
"``Syntax``) into the ``Measures:`` text box. To produce descriptive "
"statistics (means and medians) for the three repeated measures variables, "
"click on the ``Descriptives`` button."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_08.rst:37
msgid ""
"The jamovi results show descriptive statistics, χ²-value, degrees of "
"freedom, and the *p*-value (:numref:`fig-RManova6`). Since the *p*-value is "
"less than the level conventionally used to determine significance (*p* < "
"0.05), we can conclude that Broca’s Aphasics perform reasonably well on "
"speech production (median = 7.5) and language comprehension (median = 6.5) "
"tasks. However, their performance was considerably worse on the syntax task "
"(median = 4.5), with a significant difference in post-hoc tests between "
"Speech and Syntax task performance."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_09.rst:4
msgid "On the relationship between ANOVA and the Student *t*-test"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_09.rst:6
msgid ""
"There’s one last thing I want to point out before finishing. It’s something "
"that a lot of people find kind of surprising, but it’s worth knowing about. "
"An ANOVA with two groups is identical to the Student *t*-test. No, really. "
"It’s not just that they are similar, but they are actually equivalent in "
"every meaningful way. I won’t try to prove that this is always true, but I "
"will show you a single concrete demonstration. Suppose that, instead of "
"running an ANOVA on our ``mood.gain ~ drug`` model, let’s instead do it "
"using ``therapy`` as the predictor. If we run this ANOVA we get an *F*-"
"statistic of F(1,16) = 1.71, and a *p*-value = 0.210. Since we only have two "
"groups, I didn’t actually need to resort to an ANOVA, I could have just "
"decided to run a Student *t*-test. So let’s see what happens when I do that: "
"I get a *t*-statistic of t(16) = -1.3068 and a *p*-value = 0.21. Curiously, "
"the *p*-values are identical. Once again we obtain a value of *p* = 0.210. "
"But what about the test statistic? Having run a *t*-test instead of an "
"ANOVA, we get a somewhat different answer, namely t(16) = -1.3068. However, "
"there is a fairly straightforward relationship here. If we square the *t*-"
"statistic then we get the *F*-statistic from before: -1.3068² = 1.7077."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:4
msgid "Summary"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:6
msgid ""
"There’s a fair bit covered in this chapter, but there’s still a lot missing. "
"Most obviously, I haven’t discussed how to run an ANOVA when you are "
"interested in more than one grouping variable, but that will be discussed in "
"a lot of detail in chapter :doc:`../Ch14/Ch14_ANOVA2`. In terms of what we "
"have discussed, the key topics were:"
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:12
msgid ""
"The basic logic behind :doc:`how ANOVA works <Ch13_ANOVA_02>` and :doc:`how "
"to run one in jamovi <Ch13_ANOVA_03>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:15
msgid "How to compute an :doc:`effect size <Ch13_ANOVA_04>` for an ANOVA."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:17
msgid ""
":doc:`Post-hoc analysis and corrections for multiple testing "
"<Ch13_ANOVA_05>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:19
msgid ""
"The :doc:`assumptions made by the ANOVA <Ch13_ANOVA_06>`: How to check the "
"homogeneity of variance assumption and what to do if it is violated; as well "
"as how to check the normality assumption and what to do if it is violated."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:23
msgid ""
":doc:`Repeated measures ANOVA <Ch13_ANOVA_07>` and its non-parametric "
"equivalent, the :doc:`Friedman test <Ch13_ANOVA_08>`."
msgstr ""

#: ../../Ch13/Ch13_ANOVA_10.rst:26
msgid ""
"As with all of the chapters in this book, there are quite a few different "
"sources that I’ve relied upon, but the one stand-out text that I’ve been "
"most heavily influenced by is :ref:`Sahai and Ageel (2000) <Sahai_2000>`. "
"It’s not a good book for beginners, but it’s an excellent book for more "
"advanced readers who are interested in understanding the mathematics behind "
"ANOVA."
msgstr ""
