# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020-2022, Danielle J. Navarro & David R. Foxcroft. This work is licensed under a Creative Commons Attribution-Non Commercial 4.0 International License.
# This file is distributed under the same license as the Learning statistics with jamovi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-23 12:23+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../Ch07/Ch07_Probability.rst:4
msgid "Introduction to probability"
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:0
msgid "*[God] has afforded us only the twilight … of Probability.*"
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:26
msgid "John Locke"
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:25
msgid ""
"Up to this point in the book we’ve discussed some of the key ideas in "
"experimental design, and we’ve talked a little about how you can summarise a "
"data set. To a lot of people this is all there is to statistics: collecting "
"all the numbers, calculating averages, drawing pictures, and putting them "
"all in a report somewhere. Kind of like stamp collecting but with numbers. "
"However, statistics covers much more than that. In fact, descriptive "
"statistics is one of the smallest parts of statistics and one of the least "
"powerful. The bigger and more useful part of statistics is that it provides "
"information that lets you make inferences about data."
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:36
msgid ""
"Once you start thinking about statistics in these terms, that statistics is "
"there to help us draw inferences from data, you start seeing examples of it "
"everywhere. For instance, here’s a tiny extract from a newspaper article in "
"the Sydney Morning Herald (30 Oct 2010):"
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:41
msgid ""
"“I have a tough job,” the Premier said in response to a poll which found her "
"government is now the most unpopular Labor administration in polling "
"history, with a primary vote of just 23 per cent."
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:45
msgid ""
"This kind of remark is entirely unremarkable in the papers or in everyday "
"life, but let’s have a think about what it entails. A polling company has "
"conducted a survey, usually a pretty big one because they can afford it. I’m "
"too lazy to track down the original survey so let’s just imagine that they "
"called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those "
"claimed that they intended to vote for the Australian Labor Party (ALP). For "
"the 2010 Federal election the Australian Electoral Commission reported "
"4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 "
"voters (about 99.98% of voters) remain unknown to us. Even assuming that no-"
"one lied to the polling company the only thing we can say with 100% "
"confidence is that the true ALP primary vote is somewhere between "
"230/4610795 (about \\0.005%) and 4610025/4610795 (about 99.83%). So, on what "
"basis is it legitimate for the polling company, the newspaper, and the "
"readership to conclude that the ALP primary vote is only about 23%?"
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:61
msgid ""
"The answer to the question is pretty obvious. If I call 1000 people at "
"random, and 230 of them say they intend to vote for the ALP, then it seems "
"very unlikely that these are the *only* 230 people out of the entire voting "
"public who actually intend to vote ALP. In other words, we assume that the "
"data collected by the polling company is pretty representative of the "
"population at large. But how representative? Would we be surprised to "
"discover that the true ALP primary vote is actually 24%? 29%? 37%? At this "
"point everyday intuition starts to break down a bit. No-one would be "
"surprised by 24%, and everybody would be surprised by 37%, but it’s a bit "
"hard to say whether 29% is plausible. We need some more powerful tools than "
"just looking at the numbers and guessing."
msgstr ""

#: ../../Ch07/Ch07_Probability.rst:73
msgid ""
"**Inferential statistics** provides the tools that we need to answer these "
"sorts of questions, and since these kinds of questions lie at the heart of "
"the scientific enterprise, they take up the lions share of every "
"introductory course on statistics and research methods. However, the theory "
"of statistical inference is built on top of **probability theory**. And it "
"is to probability theory that we must now turn. This discussion of "
"probability theory is basically background detail. There’s not a lot of "
"statistics per se in this chapter, and you don’t need to understand this "
"material in as much depth as the other chapters in this part of the book. "
"Nevertheless, because probability theory does underpin so much of "
"statistics, it’s worth covering some of the basics."
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:4
msgid "How are probability and statistics different?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:6
msgid ""
"Before we start talking about probability theory, it’s helpful to spend a "
"moment thinking about the relationship between probability and statistics. "
"The two disciplines are closely related but they’re not identical. "
"Probability theory is “the doctrine of chances”. It’s a branch of "
"mathematics that tells you how often different kinds of events will happen. "
"For example, all of these questions are things you can answer using "
"probability theory:"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:14
msgid "What are the chances of a fair coin coming up heads 10 times in a row?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:17
msgid ""
"If I roll a six sided dice twice, how likely is it that I’ll roll two sixes?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:20
msgid ""
"How likely is it that five cards drawn from a perfectly shuffled deck will "
"all be hearts?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:23
msgid "What are the chances that I’ll win the lottery?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:25
msgid ""
"Notice that all of these questions have something in common. In each case "
"the “truth of the world” is known and my question relates to the “what kind "
"of events” will happen. In the first question I *know* that the coin is fair "
"so there’s a 50% chance that any individual coin flip will come up heads. In "
"the second question I *know* that the chance of rolling a 6 on a single die "
"is 1 in 6. In the third question I *know* that the deck is shuffled "
"properly. And in the fourth question I *know* that the lottery follows "
"specific rules. You get the idea. The critical point is that probabilistic "
"questions start with a known **model** of the world, and we use that model "
"to do some calculations. The underlying model can be quite simple. For "
"instance, in the coin flipping example we can write down the model like this:"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:38
msgid "*P*\\ (heads) = *0.5*"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:40
msgid ""
"which you can read as “the probability of heads is 0.5”. As we’ll see later, "
"in the same way that percentages are numbers that range from 0% to 100%, "
"probabilities are just numbers that range from 0 to 1. When using this "
"probability model to answer the first question I don’t actually know exactly "
"what’s going to happen. Maybe I’ll get 10 heads, like the question says. But "
"maybe I’ll get three heads. That’s the key thing. In probability theory the "
"*model* is known but the *data* are not."
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:49
msgid ""
"So that’s probability. What about statistics? Statistical questions work the "
"other way around. In statistics we do not know the truth about the world. "
"All we have is the data and it is from the data that we want to *learn* the "
"truth about the world. Statistical questions tend to look more like these:"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:55
msgid ""
"If my friend flips a coin 10 times and gets 10 heads are they playing a "
"trick on me?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:58
msgid ""
"If five cards off the top of the deck are all hearts how likely is it that "
"the deck was shuffled?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:61
msgid ""
"If the lottery commissioner’s spouse wins the lottery how likely is it that "
"the lottery was rigged?"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:64
msgid ""
"This time around the only thing we have are data. What I *know* is that I "
"saw my friend flip the coin 10 times and it came up heads every time. And "
"what I want to **infer** is whether or not I should conclude that what I "
"just saw was actually a fair coin being flipped 10 times in a row, or "
"whether I should suspect that my friend is playing a trick on me. The data I "
"have look like this:"
msgstr ""

#: ../../Ch07/Ch07_Probability_1.rst:75
msgid ""
"and what I’m trying to do is work out which “model of the world” I should "
"put my trust in. If the coin is fair then the model I should adopt is one "
"that says that the probability of heads is 0.5, that is *P*\\ (heads) = "
"*0.5*. If the coin is not fair then I should conclude that the probability "
"of heads is *not* 0.5, which we would write as *P*\\ (heads) ≠ *0.5*. In "
"other words, the statistical inference problem is to figure out which of "
"these probability models is right. Clearly, the statistical question isn’t "
"the same as the probability question, but they’re deeply connected to one "
"another. Because of this, a good introduction to statistical theory will "
"start with a discussion of what probability is and how it works."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:4
msgid "What does probability mean?"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:6
msgid ""
"Let’s start with the first of these questions. What is “probability”? It "
"might seem surprising to you but while statisticians and mathematicians "
"(mostly) agree on what the *rules* of probability are, there’s much less of "
"a consensus on what the word really *means*. It seems weird because we’re "
"all very comfortable using words like “chance”, “likely”, “possible” and "
"“probable”, and it doesn’t seem like it should be a very difficult question "
"to answer. But if you’ve ever had that experience in real life you might "
"walk away from the conversation feeling like you didn’t quite get it right, "
"and that (like many everyday concepts) it turns out that you don’t *really* "
"know what it’s all about."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:17
msgid ""
"So I’ll have a go at it. Let’s suppose I want to bet on a soccer game "
"between two teams of robots, *Arduino Arsenal* and *C Milan*. After thinking "
"about it, I decide that there is an 80% probability of *Arduino Arsenal* "
"winning. What do I mean by that? Here are three possibilities:"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:22
msgid ""
"They’re robot teams so I can make them play over and over again, and if I "
"did that *Arduino Arsenal* would win 8 out of every 10 games on average."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:26
msgid ""
"For any given game, I would agree that betting on this game is only “fair” "
"if a $1 bet on *C Milan* gives a $5 payoff (i.e. I get my $1 back plus a $4 "
"reward for being correct), as would a $4 bet on *Arduino Arsenal* (i.e., my "
"$4 bet plus a $1 reward)."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:31
msgid ""
"My subjective “belief” or “confidence” in an *Arduino Arsenal* victory is "
"four times as strong as my belief in a *C Milan* victory."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:34
msgid ""
"Each of these seems sensible. However, they’re not identical and not every "
"statistician would endorse all of them. The reason is that there are "
"different statistical ideologies (yes, really!) and depending on which one "
"you subscribe to, you might say that some of those statements are "
"meaningless or irrelevant. In this section I give a brief introduction the "
"two main approaches that exist in the literature. These are by no means the "
"only approaches, but they’re the two big ones."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:43
msgid "The frequentist view"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:45
msgid ""
"The first of the two major approaches to probability, and the more dominant "
"one in statistics, is referred to as the **frequentist view** and it defines "
"probability as a **long-run frequency**. Suppose we were to try flipping a "
"fair coin over and over again. By definition this is a coin that has "
"*P*\\(H) = *0.5*. What might we observe? One possibility is that the first "
"20 flips might look like this:"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:56
msgid ""
"In this case 11 of these 20 coin flips (55%) came up heads. Now suppose that "
"I’d been keeping a running tally of the number of heads (which I’ll call "
"*N*\\ :sub:`H`\\ ) that I’ve seen, across the first *N* flips, and calculate "
"the proportion of heads *N*\\ :sub:`H` / *N* every time. Here’s what I’d get "
"(I did literally flip coins to produce this!):"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:71
msgid "number of flips"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "1"
msgstr "1"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "2"
msgstr "2"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "3"
msgstr "3"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "4"
msgstr "4"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "5"
msgstr "5"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "6"
msgstr "6"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "7"
msgstr "7"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "8"
msgstr "8"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "9"
msgstr "9"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "10"
msgstr "10"

#: ../../Ch07/Ch07_Probability_2.rst:65 ../../Ch07/Ch07_Probability_2.rst:73
msgid "**number of heads**"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:65
msgid "0"
msgstr "0"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "**proportion**"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.00"
msgstr "0.00"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.50"
msgstr "0.50"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.67"
msgstr "0.67"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.75"
msgstr "0.75"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.80"
msgstr "0.80"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.57"
msgstr "0.57"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.63"
msgstr "0.63"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.70"
msgstr "0.70"

#: ../../Ch07/Ch07_Probability_2.rst:71 ../../Ch07/Ch07_Probability_2.rst:73
msgid "11"
msgstr "11"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "12"
msgstr "12"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "13"
msgstr "13"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "14"
msgstr "14"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "15"
msgstr "15"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "16"
msgstr "16"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "17"
msgstr "17"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "18"
msgstr "18"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "19"
msgstr "19"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "20"
msgstr "20"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.73"
msgstr "0.73"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.69"
msgstr "0.69"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.71"
msgstr "0.71"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.59"
msgstr "0.59"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.56"
msgstr "0.56"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.53"
msgstr "0.53"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.55"
msgstr "0.55"

#: ../../Ch07/Ch07_Probability_2.rst:78
msgid ""
"Notice that at the start of the sequence the *proportion* of heads "
"fluctuates wildly, starting at 0.00 and rising as high as 0.80. Later on, "
"one gets the impression that it dampens out a bit, with more and more of the "
"values actually being pretty close to the “right” answer of 0.50. This is "
"the frequentist definition of probability in a nutshell. Flip a fair coin "
"over and over again, and as *N* grows large (approaches infinity, denoted "
"*N* → ∞) the proportion of heads will converge to 50%. There are some subtle "
"technicalities that the mathematicians care about, but qualitatively "
"speaking that’s how the frequentists define probability. Unfortunately, I "
"don’t have an infinite number of coins or the infinite patience required to "
"flip a coin an infinite number of times. However, I do have a computer and "
"computers excel at mindless repetitive tasks. So I asked my computer to "
"simulate flipping a coin 1000 times and then drew a picture of what happens "
"to the proportion *N*\\ :sub:`H` / *N* as *N* increases. Actually, I did it "
"four times just to make sure it wasn’t a fluke. The results are shown in :"
"numref:`fig-frequentistProb`. As you can see, the *proportion of observed "
"heads* eventually stops fluctuating and settles down. When it does, the "
"number at which it finally settles is the true probability of heads."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:104
msgid "Illustration of how frequentist probability works"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:104
msgid ""
"Illustration of how frequentist probability works: If you flip a fair coin "
"over and over again the proportion of heads that you’ve seen eventually "
"settles down and converges to the true probability of 0.5. Each panel shows "
"four different simulated experiments. In each case we pretend we flipped a "
"coin 1000 times and kept track of the proportion of flips that were heads as "
"we went along. Although none of these sequences actually ended up with an "
"exact value of 0.5, if we’d extended the experiment for an infinite number "
"of coin flips they would have."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:115
msgid ""
"The frequentist definition of probability has some desirable "
"characteristics. First, it is objective. The probability of an event is "
"*necessarily* grounded in the world. The only way that probability "
"statements can make sense is if they refer to (a sequence of) events that "
"occur in the physical universe.\\ [#]_ Secondly, it is unambiguous. Any two "
"people watching the same sequence of events unfold, trying to calculate the "
"probability of an event, must inevitably come up with the same answer."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:124
msgid ""
"However, it also has undesirable characteristics. First, infinite sequences "
"don’t exist in the physical world. Suppose you picked up a coin from your "
"pocket and started to flip it. Every time it lands it impacts on the ground. "
"Each impact wears the coin down a bit. Eventually the coin will be "
"destroyed. So, one might ask whether it really makes sense to pretend that "
"an “infinite” sequence of coin flips is even a meaningful concept, or an "
"objective one. We can’t say that an “infinite sequence” of events is a real "
"thing in the physical universe, because the physical universe doesn’t allow "
"infinite anything. More seriously, the frequentist definition has a narrow "
"scope. There are lots of things out there that human beings are happy to "
"assign probability to in everyday language, but cannot (even in theory) be "
"mapped onto a hypothetical sequence of events. For instance, if a "
"meteorologist comes on TV and says “the probability of rain in Adelaide on 2 "
"November 2048 is 60%” we humans are happy to accept this. But it’s not clear "
"how to define this in frequentist terms. There’s only one city of Adelaide, "
"and only one 2 November 2048. There’s no infinite sequence of events here, "
"just a one-off thing. Frequentist probability genuinely *forbids* us from "
"making probability statements about a single event. From the frequentist "
"perspective it will either rain tomorrow or it will not. There is no "
"“probability” that attaches to a single non-repeatable event. Now, it should "
"be said that there are some very clever tricks that frequentists can use to "
"get around this. One possibility is that what the meteorologist means is "
"something like “There is a category of days for which I predict a 60% chance "
"of rain, and if we look only across those days for which I make this "
"prediction, then on 60% of those days it will actually rain”. It’s very "
"weird and counterintuitive to think of it this way, but you do see "
"frequentists do this sometimes. And it *will* come up later in this book "
"(see :doc:`Estimating a confidence interval <../Ch08/Ch08_Estimation_5>`)."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:156
msgid "The Bayesian view"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:158
msgid ""
"The **Bayesian view** of probability is often called the subjectivist view, "
"and although it has been a minority view among statisticians it has been "
"steadily gaining traction for the last several decades. There are many "
"flavours of Bayesianism, making it hard to say exactly what “the” Bayesian "
"view is. The most common way of thinking about subjective probability is to "
"define the probability of an event as the **degree of belief** that an "
"intelligent and rational agent assigns to that truth of that event. From "
"that perspective, probabilities don’t exist in the world but rather in the "
"thoughts and assumptions of people and other intelligent beings."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:169
msgid ""
"However, in order for this approach to work we need some way of "
"operationalising “degree of belief”. One way that you can do this is to "
"formalise it in terms of “rational gambling”, though there are many other "
"ways. Suppose that I believe that there’s a 60% probability of rain "
"tomorrow. If someone offers me a bet that if it rains tomorrow then I win "
"$5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is "
"a pretty good bet. On the other hand, if I think that the probability of "
"rain is only 40% then it’s a bad bet to take. So we can operationalise the "
"notion of a “subjective probability” in terms of what bets I’m willing to "
"accept."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:180
msgid ""
"What are the advantages and disadvantages to the Bayesian approach? The main "
"advantage is that it allows you to assign probabilities to any event you "
"want to. You don’t need to be limited to those events that are repeatable. "
"The main disadvantage (to many people) is that we can’t be purely objective. "
"Specifying a probability requires us to specify an entity that has the "
"relevant degree of belief. This entity might be a human, an alien, a robot, "
"or even a statistician. But there has to be an intelligent agent out there "
"that believes in things. To many people this is uncomfortable, it seems to "
"make probability arbitrary. Whilst the Bayesian approach requires that the "
"agent in question be rational (i.e., obey the rules of probability), it does "
"allow everyone to have their own beliefs. I can believe the coin is fair and "
"you don’t have to, even though we’re both rational. The frequentist view "
"doesn’t allow any two observers to attribute different probabilities to the "
"same event. When that happens then at least one of them must be wrong. The "
"Bayesian view does not prevent this from occurring. Two observers with "
"different background knowledge can legitimately hold different beliefs about "
"the same event. In short, where the frequentist view is sometimes considered "
"to be too narrow (forbids lots of things that that we want to assign "
"probabilities to), the Bayesian view is sometimes thought to be too broad "
"(allows too many differences between observers)."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:203
msgid "What’s the difference? And who is right?"
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:205
msgid ""
"Now that you’ve seen each of these two views independently it’s useful to "
"make sure you can compare the two. Go back to the hypothetical robot soccer "
"game at the start of the section. What do you think a frequentist and a "
"Bayesian would say about these three statements? Which statement would a "
"frequentist say is the correct definition of probability? Which one would a "
"Bayesian opt for? Would some of these statements be meaningless to a "
"frequentist or a Bayesian? If you’ve understood the two perspectives you "
"should have some sense of how to answer those questions."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:215
msgid ""
"Okay, assuming you understand the difference then you might be wondering "
"which of them is *right*? Honestly, I don’t know that there is a right "
"answer. As far as I can tell there’s nothing mathematically incorrect about "
"the way frequentists think about sequences of events, and there’s nothing "
"mathematically incorrect about the way that Bayesians define the beliefs of "
"a rational agent. In fact, when you dig down into the details Bayesians and "
"frequentists actually agree about a lot of things. Many frequentist methods "
"lead to decisions that Bayesians agree a rational agent would make. Many "
"Bayesian methods have very good frequentist properties."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:226
msgid ""
"For the most part, I’m a pragmatist so I’ll use any statistical method that "
"I trust. As it turns out, that makes me prefer Bayesian methods for reasons "
"I’ll explain towards the end of the book. But I’m not fundamentally opposed "
"to frequentist methods. Not everyone is quite so relaxed. For instance, "
"consider Sir Ronald Fisher, one of the towering figures of 20th century "
"statistics and a vehement opponent to all things Bayesian, whose paper on "
"the mathematical foundations of statistics referred to Bayesian probability "
"as “an impenetrable jungle [that] arrests progress towards precision of "
"statistical concepts” (`Fisher, 1922b <../Other/References."
"html#fisher-1922b>`__\\ ). Or the psychologist Paul Meehl, who suggests that "
"relying on frequentist methods could turn you into “a potent but sterile "
"intellectual rake who leaves in his merry path a long train of ravished "
"maidens but no viable scientific offspring” (`Meehl, 1967 <../Other/"
"References.html#cronbach-1951>`__\\ ; p. 114). The history of statistics, as "
"you might gather, is not devoid of entertainment."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:242
msgid ""
"In any case, whilst I personally prefer the Bayesian view, the majority of "
"statistical analyses are based on the frequentist approach. My reasoning is "
"pragmatic. The goal of this book is to cover roughly the same territory as a "
"typical undergraduate stats class in psychology, and if you want to "
"understand the statistical tools used by most psychologists you’ll need a "
"good grasp of frequentist methods. I promise you that this isn’t wasted "
"effort. Even if you end up wanting to switch to the Bayesian perspective, "
"you really should read through at least one book on the “orthodox” "
"frequentist view. Besides, I won’t completely ignore the Bayesian "
"perspective. Every now and then I’ll add some commentary from a Bayesian "
"point of view, and I’ll revisit the topic in more depth in chapter :doc:"
"`Bayesian statistics <../Ch16/Ch16_Bayes>`."
msgstr ""

#: ../../Ch07/Ch07_Probability_2.rst:258
msgid ""
"This doesn’t mean that frequentists can’t make hypothetical statements, of "
"course. It’s just that if you want to make a statement about probability "
"then it must be possible to redescribe that statement in terms of a sequence "
"of potentially observable events, together with the relative frequencies of "
"different outcomes that appear within that sequence."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:4
msgid "Basic probability theory"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:6
msgid ""
"Ideological arguments between Bayesians and frequentists notwithstanding, it "
"turns out that people mostly agree on the rules that probabilities should "
"obey. There are lots of different ways of arriving at these rules. The most "
"commonly used approach is based on the work of Andrey Kolmogorov, one of the "
"great Soviet mathematicians of the 20th century. I won’t go into a lot of "
"detail, but I’ll try to give you a bit of a sense of how it works. And in "
"order to do so I’m going to have to talk about my trousers."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:16
msgid "Introducing probability distributions"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:18
msgid ""
"One of the disturbing truths about my life is that I only own 5 pairs of "
"trousers. Three pairs of jeans, the bottom half of a suit, and a pair of "
"tracksuit pants. Even sadder, I’ve given them names: I call them *X*\\ :sub:"
"`1`\\ , *X*\\ :sub:`2`\\ , *X*\\ :sub:`3`\\ , *X*\\ :sub:`4`  and *X*\\ :sub:"
"`5`\\ . I really have, that’s why they call me Mister Imaginative. Now, on "
"any given day, I pick out exactly one of pair of trousers to wear. Not even "
"I’m so stupid as to try to wear two pairs of trousers, and thanks to years "
"of training I never go outside without wearing trousers anymore. If I were "
"to describe this situation using the language of probability theory, I would "
"refer to each pair of trousers (i.e., each *X*) as an **elementary event**. "
"The key characteristic of elementary events is that every time we make an "
"observation (e.g., every time I put on a pair of trousers) then the outcome "
"will be one and only one of these events. Like I said, these days I always "
"wear exactly one pair of trousers so my trousers satisfy this constraint. "
"Similarly, the set of all possible events is called a **sample space**. "
"Granted, some people would call it a “wardrobe”, but that’s because they’re "
"refusing to think about my trousers in probabilistic terms. Sad."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:37
msgid ""
"Okay, now that we have a sample space (a wardrobe), which is built from lots "
"of possible elementary events (trousers), what we want to do is assign a "
"**probability** of one of these elementary events. For an event *X*, the "
"probability of that event *P*\\ (X) is a number that lies between 0 and 1. "
"The bigger the value of *P*\\ (X), the more likely the event is to occur. "
"So, for example, if *P*\\ (X) = 0 it means the event *X* is impossible (i."
"e., I never wear those trousers). On the other hand, if *P*\\ (X) = 1 it "
"means that event *X* is certain to occur (i.e., I always wear those "
"trousers). For probability values in the middle it means that I sometimes "
"wear those trousers. For instance, if *P*\\ (X) = 0.5 it means that I wear "
"those trousers half of the time."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:50
msgid ""
"At this point, we’re almost done. The last thing we need to recognise is "
"that “something always happens”. Every time I put on trousers, I really do "
"end up wearing trousers (crazy, right?). What this somewhat trite statement "
"means, in probabilistic terms, is that the probabilities of the elementary "
"events need to add up to 1. This is known as the **law of total "
"probability**, not that any of us really care. More importantly, if these "
"requirements are satisfied then what we have is a **probability "
"distribution**. For example, this is an example of a probability "
"distribution:"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Which trousers?"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Label"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Probability"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "Blue jeans"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "*X*\\ :sub:`1`"
msgstr "*X*\\ :sub:`1`"

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "*P*\\ (X\\ :sub:`1`\\ ) = 0.5"
msgstr "*P*\\ (X\\ :sub:`1`\\ ) = 0.5"

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "Grey jeans"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "*X*\\ :sub:`2`"
msgstr "*X*\\ :sub:`2`"

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "*P*\\ (X\\ :sub:`2`\\ ) = 0.3"
msgstr "*P*\\ (X\\ :sub:`2`\\ ) = 0.3"

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "Black jeans"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "*X*\\ :sub:`3`"
msgstr "*X*\\ :sub:`3`"

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "*P*\\ (X\\ :sub:`3`\\ ) = 0.1"
msgstr "*P*\\ (X\\ :sub:`3`\\ ) = 0.1"

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "Black suit"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "*X*\\ :sub:`4`"
msgstr "*X*\\ :sub:`4`"

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "*P*\\ (X\\ :sub:`4`\\ ) = 0"
msgstr "*P*\\ (X\\ :sub:`4`\\ ) = 0"

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "Blue tracksuit"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "*X*\\ :sub:`5`"
msgstr "*X*\\ :sub:`5`"

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "*P*\\ (X\\ :sub:`5`\\ ) = 0.1"
msgstr "*P*\\ (X\\ :sub:`5`\\ ) = 0.1"

#: ../../Ch07/Ch07_Probability_3.rst:74
msgid ""
"Each of the events has a probability that lies between 0 and 1, and if we "
"add up the probability of all events they sum to 1. Awesome. We can even "
"draw a nice bar graph (see :doc:`Bar graphs <../Ch05/Ch05_Graphics_3>`) to "
"visualise this distribution, as shown in :numref:`fig-pantsDistribution`. "
"And, at this point, we’ve all achieved something. You’ve learned what a "
"probability distribution is, and I’ve finally managed to find a way to "
"create a graph that focuses entirely on my trousers. Everyone wins!"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:88
msgid "“Trousers” probability distribution"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:88
msgid ""
"Visual depiction of the “trousers” probability distribution. There are five "
"“elementary events”, corresponding to the five pairs of trousers that I own. "
"Each event has some probability of occurring: this probability is a number "
"between 0 to 1. The sum of these probabilities is 1."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:95
msgid ""
"The only other thing that I need to point out is that probability theory "
"allows you to talk about **non elementary events** as well as elementary "
"ones. The easiest way to illustrate the concept is with an example. In the "
"trousers example it’s perfectly legitimate to refer to the probability that "
"I wear jeans. In this scenario, the “Dani wears jeans” event is said to have "
"happened as long as the elementary event that actually did occur is one of "
"the appropriate ones. In this case “blue jeans”, “black jeans” or “grey "
"jeans”. In mathematical terms we defined the “jeans” event *E* to correspond "
"to the set of elementary events (X\\ :sub:`1`\\ , X\\ :sub:`2`\\ , X\\ :sub:"
"`3`\\ )`. If any of these elementary events occurs then *E* is also said to "
"have occurred. Having decided to write down the definition of the *E* this "
"way, it’s pretty straightforward to state what the probability *P*\\ (E) is: "
"we just add everything up. In this particular case"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:110
msgid ""
"*P*\\ (E) = *P*\\ (X\\ :sub:`1`\\ ) + *P*\\ (X\\ :sub:`2`\\ ) + *P*\\ (X\\ :"
"sub:`3`\\ )"
msgstr ""
"*P*\\ (E) = *P*\\ (X\\ :sub:`1`\\ ) + *P*\\ (X\\ :sub:`2`\\ ) + *P*\\ (X\\ :"
"sub:`3`\\ )"

#: ../../Ch07/Ch07_Probability_3.rst:112
msgid ""
"and, since the probabilities of blue, grey and black jeans respectively are "
"0.5, 0.3 and 0.1, the probability that I wear jeans is equal to 0.9."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:115
msgid ""
"At this point you might be thinking that this is all terribly obvious and "
"simple and you’d be right. All we’ve really done is wrap some basic "
"mathematics around a few common sense intuitions. However, from these simple "
"beginnings it’s possible to construct some extremely powerful mathematical "
"tools. I’m definitely not going to go into the details in this book, but "
"what I will do is list, in :numref:`tab-probrules`, some of the other rules "
"that probabilities satisfy. These rules can be derived from the simple "
"assumptions that I’ve outlined above, but since we don’t actually use these "
"rules for anything in this book I won’t do so here."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:125
msgid ""
"Some basic rules that probabilities must satisfy. You don’t really need to "
"know these rules in order to understand the analyses that we’ll talk about "
"later in the book, but they are important if you want to understand "
"probability theory a bit more deeply."
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "English"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "Notation"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "Formula"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "not *A*"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "*P*\\ (¬ A)"
msgstr "*P*\\ (¬ A)"

#: ../../Ch07/Ch07_Probability_3.rst:136 ../../Ch07/Ch07_Probability_3.rst:138
#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "="
msgstr "="

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "1 - *P*\\ (A)"
msgstr "1 - *P*\\ (A)"

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*A* or *B*"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*P*\\ (A ∪ B)"
msgstr "*P*\\ (A ∪ B)"

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*P*\\ (A) + *P*\\ (B) - *P*\\ (A ∩ B)"
msgstr "*P*\\ (A) + *P*\\ (B) - *P*\\ (A ∩ B)"

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*A* and *B*"
msgstr ""

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*P*\\ (A ∩ B)"
msgstr "*P*\\ (A ∩ B)"

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*P*\\ (A|B) *P*\\ (B)"
msgstr "*P*\\ (A|B) *P*\\ (B)"

#: ../../Ch07/Ch07_Probability_4.rst:4
msgid "The binomial distribution"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:6
msgid ""
"As you might imagine, probability distributions vary enormously and there’s "
"an enormous range of distributions out there. However, they aren’t all "
"equally important. In fact, the vast majority of the content in this book "
"relies on one of five distributions: the binomial distribution, the normal "
"distribution, the *t*-distribution, the *χ²*-distribution (chi-square) and "
"the *F*-distribution. Given this, what I’ll do over the next few sections is "
"provide a brief introduction to all five of these, paying special attention "
"to the binomial and the normal. I’ll start with the binomial distribution "
"since it’s the simplest of the five."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:17
msgid "Introducing the binomial"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:19
msgid ""
"The theory of probability originated in the attempt to describe how games of "
"chance work, so it seems fitting that our discussion of the **binomial "
"distribution** should involve a discussion of rolling dice and flipping "
"coins. Let’s imagine a simple “experiment”. In my hot little hand I’m "
"holding 20 identical six-sided dice. On one face of each die there’s a "
"picture of a skull, the other five faces are all blank. If I proceed to roll "
"all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming "
"that the dice are fair, we know that the chance of any one die coming up "
"skulls is 1 in 6. To say this another way, the skull probability for a "
"single die is approximately 0.167. This is enough information to answer our "
"question, so let’s have a look at how it’s done."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:32
msgid ""
"Formulas for the binomial and normal distributions. We don’t really use "
"these formulas for anything in this book, but they’re pretty important for "
"more advanced work, so I thought it might be best to put them here in a "
"table, where they can’t get in the way of the text. In the equation for the "
"binomial, *X!* is the factorial function (i.e., multiply all whole numbers "
"from 1 to *X*), and for the normal distribution “exp” refers to the "
"exponential function, which we discussed in chapter :doc:`Pragmatic matters "
"<../Ch06/Ch06_DataHandling>`. If these equations don’t make a lot of sense "
"to you, don’t worry too much about them."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:44
msgid "Binomial"
msgstr "Binomial"

#: ../../Ch07/Ch07_Probability_4.rst:44
msgid "Normal"
msgstr "Normal"

#: ../../Ch07/Ch07_Probability_4.rst:46
msgid "|binomial|"
msgstr "|binomial|"

#: ../../Ch07/Ch07_Probability_4.rst:46
msgid "|normal|"
msgstr "|normal|"

#: ../../Ch07/Ch07_Probability_4.rst:49
msgid ""
"As usual, we’ll want to introduce some names and some notation. We’ll let "
"*N* denote the number of dice rolls in our experiment, which is often "
"referred to as the **size parameter** of our binomial distribution. We’ll "
"also use *θ* to refer to the the probability that a single die comes up "
"skulls, a quantity that is usually called the **success probability** of the "
"binomial.\\ [#]_ Finally, we’ll use *X* to refer to the results of our "
"experiment, namely the number of skulls I get when I roll the dice. Since "
"the actual value of *X* is due to chance we refer to it as a **random "
"variable**. In any case, now that we have all this terminology and notation "
"we can use it to state the problem a little more precisely. The quantity "
"that we want to calculate is the probability that *X* = 4 given that we know "
"that *θ* = 0.167 and *N* = 20. The general “form” of the thing I’m "
"interested in calculating could be written as"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:65
msgid "*P*\\ (X | θ, N)"
msgstr "*P*\\ (X | θ, N)"

#: ../../Ch07/Ch07_Probability_4.rst:67
msgid ""
"and we’re interested in the special case where *X* = 4, *θ* = 0.167 and *N* "
"= 20. There’s only one more piece of notation I want to refer to before "
"moving on to discuss the solution to the problem. If I want to say that *X* "
"is generated randomly from a binomial distribution with parameters *θ* and "
"*N*, the notation I would use is as follows:"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:74
msgid "*X* ~ Binomial(θ, N)"
msgstr "*X* ~ Binomial(θ, N)"

#: ../../Ch07/Ch07_Probability_4.rst:76
msgid ""
"Yeah, yeah. I know what you’re thinking: notation, notation, notation. "
"Really, who cares? Very few readers of this book are here for the notation, "
"so I should probably move on and talk about how to use the binomial "
"distribution. I’ve included the formula for the binomial distribution in :"
"numref:`tab-distformulas`, since some readers may want to play with it "
"themselves, but since most people probably don’t care that much and because "
"we don’t need the formula in this book, I won’t talk about it in any detail. "
"Instead, I just want to show you what the binomial distribution looks like."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:92
msgid "Binomial distribution for N = 20 and θ = 1/6"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:92
msgid ""
"Binomial distribution with size parameter of N = 20 and an underlying "
"success probability of θ = 1/6. Each vertical bar depicts the probability of "
"one specific outcome (i.e., one possible value of X). Because this is a "
"probability distribution, each of the probabilities must be a number between "
"0 and 1, and the heights of the bars must sum to 1 as well."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:100
msgid ""
"To that end, :numref:`fig-binomSkulls20` plots the binomial probabilities "
"for all possible values of *X* for our dice rolling experiment, from *X* = 0 "
"(no skulls) all the way up to *X* = 20 (all skulls). Note that this is "
"basically a bar chart, and is no different to the “trousers probability” "
"plot I drew in :numref:`fig-pantsDistribution`. On the horizontal axis we "
"have all the possible events, and on the vertical axis we can read off the "
"probability of each of those events. So, the probability of rolling 4 skulls "
"out of 20 is about 0.20 (the actual answer is 0.2022036, as we’ll see in a "
"moment). In other words, you’d expect that to happen about 20% of the times "
"you repeated this experiment."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:111
msgid ""
"To give you a feel for how the binomial distribution changes when we alter "
"the values of *θ* and *N*, let’s suppose that instead of rolling dice I’m "
"actually flipping coins. This time around, my experiment involves flipping a "
"fair coin repeatedly and the outcome that I’m interested in is the number of "
"heads that I observe. In this scenario, the success probability is now *θ* = "
"1/2. Suppose I were to flip the coin *N* = 20 times. In this example, I’ve "
"changed the success probability but kept the size of the experiment the "
"same. What does this do to our binomial distribution? Well, as the left "
"panel of :numref:`fig-binomHeads` shows, the main effect of this is to shift "
"the whole distribution, as you’d expect. Okay, what if we flipped a coin *N* "
"= 100 times? Well, in that case we get what is shown in the right panel. The "
"distribution stays roughly in the middle but there’s a bit more variability "
"in the possible outcomes."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:132
msgid "Binomial distribution: θ = 1/2 and N = 20 (left) or N = 100 (right)"
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:132
msgid ""
"Two binomial distributions, involving a scenario in which I’m flipping a "
"fair coin, so the underlying success probability is θ = 1/2. In the left "
"panel, we assume I’m flipping the coin N = 20 times. In the right panel, we "
"assume that the coin is flipped N = 100 times."
msgstr ""

#: ../../Ch07/Ch07_Probability_4.rst:142
msgid ""
"Note that the term “success” is pretty arbitrary and doesn’t actually imply "
"that the outcome is something to be desired. If *θ* referred to the "
"probability that any one passenger gets injured in a bus crash I’d still "
"call it the success probability, but that doesn’t mean I want people to get "
"hurt in bus crashes!"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:4
msgid "The normal distribution"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:6
msgid ""
"While the binomial distribution is conceptually the simplest distribution to "
"understand, it’s not the most important one. That particular honour goes to "
"the **normal distribution**, also referred to as “the bell curve” or a "
"“Gaussian distribution”. A normal distribution is described using two "
"parameters: the mean of the distribution *µ* and the standard deviation of "
"the distribution *σ*."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:13
msgid ""
"The notation that we sometimes use to say that a variable *X* is normally "
"distributed is as follows:"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:16
msgid "X ~ Normal(µ, σ)"
msgstr "X ~ Normal(µ, σ)"

#: ../../Ch07/Ch07_Probability_5.rst:18
msgid ""
"Of course, that’s just notation. It doesn’t tell us anything interesting "
"about the normal distribution itself. As was the case with the binomial "
"distribution, I have included the formula for the normal distribution in "
"this book, because I think it’s important enough that everyone who learns "
"statistics should at least look at it, but since this is an introductory "
"text I don’t want to focus on it, so I’ve tucked it away in :numref:`tab-"
"distformulas`."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:32
msgid "Normal distribution with mean μ = 0 and standard deviation σ = 1"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:32
msgid ""
"The normal distribution with mean μ = 0 and standard deviation σ = 1. The x-"
"axis corresponds to the value of some variable, and the y-axis tells us "
"something about how likely we are to observe that value. However, notice "
"that the y-axis is labelled “Probability Density” and not “Probability”. "
"There is a subtle and somewhat frustrating characteristic of continuous "
"distributions that makes the y axis behave a bit oddly: the height of the "
"curve here isn’t actually the probability of observing a particular x value. "
"On the other hand, it is true that the heights of the curve tells you which "
"x values are more likely (the higher ones!; see :doc:`Probability density "
"<../Ch07/Ch07_Probability_5>` for all the annoying details)."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:45
msgid ""
"Instead of focusing on the maths, let’s try to get a sense for what it means "
"for a variable to be normally distributed. To that end, have a look at :"
"numref:`fig-standardNormal` which plots a normal distribution with mean *µ* "
"= 0 and standard deviation *σ* = 1. You can see where the name “bell curve” "
"comes from; it looks a bit like a bell. Notice that, unlike the plots that I "
"drew to illustrate the binomial distribution, the picture of the normal "
"distribution in :numref:`fig-standardNormal` shows a smooth curve instead of "
"“histogram-like” bars. This isn’t an arbitrary choice, the normal "
"distribution is continuous whereas the binomial is discrete. For instance, "
"in the die rolling example from the last section it was possible to get 3 "
"skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I "
"drew in the previous section reflected this fact. In :numref:`fig-"
"binomSkulls20`, for instance, there’s a bar located at *X* = 3 and another "
"one at *X* = 4 but there’s nothing in between. Continuous quantities don’t "
"have this constraint. For instance, suppose we’re talking about the weather. "
"The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, "
"23.9 degrees, or anything in between since temperature is a continuous "
"variable |continuous|. And so a normal distribution might be quite "
"appropriate for describing Spring temperatures.\\ [#]_"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:250
msgid "continuous"
msgstr "continuous"

#: ../../Ch07/Ch07_Probability_5.rst:72
msgid "Normal distribution: σ = 1 and µ = 4 (solid) or µ = 7 (dashed)"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:72
msgid ""
"Illustration of what happens when you change the mean of a normal "
"distribution. The solid line depicts a normal distribution with a mean of μ "
"= 4. The dashed line shows a normal distribution with a mean of μ = 7. In "
"both cases, the standard deviation is σ = 1. Not surprisingly, the two "
"distributions have the same shape, but the dashed line is shifted to the "
"right."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:81
msgid ""
"With this in mind, let’s see if we can’t get an intuition for how the normal "
"distribution works. First, let’s have a look at what happens when we play "
"around with the parameters of the distribution. To that end, :numref:`fig-"
"meanShiftNormal` plots normal distributions that have different means but "
"have the same standard deviation. As you might expect, all of these "
"distributions have the same “width”. The only difference between them is "
"that they’ve been shifted to the left or to the right. In every other "
"respect they’re identical. In contrast, if we increase the standard "
"deviation while keeping the mean constant, the peak of the distribution "
"stays in the same place but the distribution gets wider, as you can see in :"
"numref:`fig-scaleShiftNormal`."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:99
msgid "Normal distribution: µ = 5 and σ = 1 (solid) or σ = 2 (dashed)"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:99
msgid ""
"Illustration of what happens when you change the standard deviation of a "
"normal distribution. Both distributions plotted in this figure have a mean "
"of μ = 5, but they have different standard deviations. The solid line plots "
"a distribution with standard deviation σ = 1, and the dashed line shows a "
"distribution with standard deviation σ = 2. As a consequence, both "
"distributions are “centred” on the same spot, but the dashed line is wider "
"than the solid one."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:109
msgid ""
"Notice, though, that when we widen the distribution the height of the peak "
"shrinks. This has to happen, in the same way that the heights of the bars "
"that we used to draw a discrete binomial distribution have to *sum* to 1, "
"the total *area under the curve* for the normal distribution must equal 1. "
"Before moving on, I want to point out one important characteristic of the "
"normal distribution. Irrespective of what the actual mean and standard "
"deviation are, 68.3% of the area falls within 1 standard deviation of the "
"mean. Similarly, 95.4% of the distribution falls within 2 standard "
"deviations of the mean, and 99.7% of the distribution is within 3 standard "
"deviations. This idea is illustrated in :numref:`fig-normAreaSD`."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:127
msgid "Normal distribution: area under the curve for 1 and 2 SD"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:127
msgid ""
"The area under the curve tells you the probability that an observation falls "
"within a particular range. The solid lines plot normal distributions with "
"mean μ = 0 and standard deviation σ = 1. The shaded areas illustrate “areas "
"under the curve” for two important cases. In the left panel, we can see that "
"there is a 68.3% chance that an observation will fall within one standard "
"deviation of the mean. In the right panel, we see that there is a 95.4% "
"chance that an observation will fall within two standard deviations of the "
"mean."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:142
msgid "Area under the curve for 1 SD bordering the mean and at the tails"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:142
msgid ""
"Two more examples of the “area under the curve” idea. There is a 15.9% "
"chance that an observation is one standard deviation below the mean or "
"smaller (left panel), and a 34.1% chance that the observation is somewhere "
"between one standard deviation below the mean and the mean (right panel). "
"Notice that if you add these two numbers together you get 15.9% + 34.1% = "
"50%. For normally distributed data, there is a 50% chance that an "
"observation falls below the mean. And of course that also implies that there "
"is a 50% chance that it falls above the mean."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:154
msgid "Probability density"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:156
msgid ""
"There’s something I’ve been trying to hide throughout my discussion of the "
"normal distribution, something that some introductory textbooks omit "
"completely. They might be right to do so. This “thing” that I’m hiding is "
"weird and counter-intuitive even by the admittedly distorted standards that "
"apply in statistics. Fortunately, it’s not something that you need to "
"understand at a deep level in order to do basic statistics. Rather, it’s "
"something that starts to become important later on when you move beyond the "
"basics. So, if it doesn’t make complete sense, don’t worry too much, but try "
"to make sure that you follow the gist of it."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:166
msgid ""
"Throughout my discussion of the normal distribution there’s been one or two "
"things that don’t quite make sense. Perhaps you noticed that the *y*-axis in "
"these figures is labelled “Probability Density” rather than “Density”. Maybe "
"you noticed that I used *p*\\ (X) instead of *P*\\ (X) when giving the "
"formula for the normal distribution."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:172
msgid ""
"As it turns out, what is presented here isn’t actually a probability, it’s "
"something else. To understand what that something is you have to spend a "
"little time thinking about what it really *means* to say that *X* is a "
"continuous variable |continuous|. Let’s say we’re talking about the "
"temperature outside. The thermometer tells me it’s 23 degrees, but I know "
"that’s not really true. It’s not *exactly* 23 degrees. Maybe it’s \\23.1 "
"degrees, I think to myself. But I know that that’s not really true either "
"because it might actually be 23.09 degrees. But I know that... well, you get "
"the idea. The tricky thing with genuinely continuous quantities is that you "
"never really know exactly what they are."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:183
msgid ""
"Now think about what this implies when we talk about probabilities. Suppose "
"that tomorrow’s maximum temperature is sampled from a normal distribution "
"with mean 23 and standard deviation 1. What’s the probability that the "
"temperature will be *exactly* 23 degrees? The answer is “zero”, or possibly "
"“a number so close to zero that it might as well be zero”. Why is this? It’s "
"like trying to throw a dart at an infinitely small dart board. No matter how "
"good your aim, you’ll never hit it. In real life you’ll never get a value of "
"exactly 23. It’ll always be something like 23.1 or 22.99998 or suchlike. In "
"other words, it’s completely meaningless to talk about the probability that "
"the temperature is exactly 23 degrees. However, in everyday language if I "
"told you that it was 23 degrees outside and it turned out to be 22.9998 "
"degrees you probably wouldn’t call me a liar. Because in everyday language "
"“23 degrees” usually means something like “somewhere between \\22.5 and 23.5 "
"degrees”. And while it doesn’t feel very meaningful to ask about the "
"probability that the temperature is exactly 23 degrees, it does seem "
"sensible to ask about the probability that the temperature lies between 22.5 "
"and 23.5, or between 20 and 30, or any other range of temperatures."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:203
msgid ""
"The point of this discussion is to make clear that when we’re talking about "
"continuous distributions it’s not meaningful to talk about the probability "
"of a specific value. However, what we *can* talk about is the probability "
"that the value lies within a particular range of values. To find out the "
"probability associated with a particular range what you need to do is "
"calculate the “area under the curve”. We’ve seen this concept already, in :"
"numref:`fig-normAreaSD` the shaded areas shown depict genuine probabilities "
"(e.g., in the left panel of :numref:`fig-normAreaSD` it shows the "
"probability of observing a value that falls within 1 standard deviation of "
"the mean)."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:214
msgid ""
"Okay, so that explains part of the story. I’ve explained a little bit about "
"how continuous probability distributions should be interpreted (i.e., area "
"under the curve is the key thing). But what does the formula for *p*\\ (x) "
"that I described earlier actually mean? Obviously, p*\\ (x) doesn’t describe "
"a probability, but what is it? The name for this quantity *p*\\ (x) is a "
"**probability density**, and in terms of the plots we’ve been drawing it "
"corresponds to the *height* of the curve. The densities themselves aren’t "
"meaningful in and of themselves, but they’re “rigged” to ensure that the "
"*area* under the curve is always interpretable as genuine probabilities. To "
"be honest, that’s about as much as you really need to know for now.\\ [#]_"
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:229
msgid ""
"In practice, the normal distribution is so handy that people tend to use it "
"even when the variable isn’t actually continuous. As long as there are "
"enough categories (e.g., Likert scale responses to a questionnaire), it’s "
"pretty standard practice to use the normal distribution as an approximation. "
"This works out much better in practice than you’d think."
msgstr ""

#: ../../Ch07/Ch07_Probability_5.rst:237
msgid ""
"For those readers who know a little calculus, I’ll give a slightly more "
"precise explanation. In the same way that probabilities are non-negative "
"numbers that must sum to 1, probability densities are non-negative numbers "
"that must integrate to 1 (where the integral is taken across all possible "
"values of *X*). To calculate the probability that *X* falls between *a* and "
"*b* we calculate the definite integral of the density function over the "
"corresponding range, :math:`\\int_a^b p(x) \\ dx`. If you don’t remember or "
"never learned calculus, don’t worry about this. It’s not needed for this "
"book."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:4
msgid "Other useful distributions"
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:6
msgid ""
"The normal distribution is the distribution that statistics makes most use "
"of (for reasons to be discussed shortly), and the binomial distribution is a "
"very useful one for lots of purposes. But the world of statistics is filled "
"with probability distributions, some of which we’ll run into in passing. In "
"particular, the three that will appear in this book are the *t* "
"distribution, the *χ²*-distribution and the *F*-distribution. I won’t give "
"formulas for any of these, or talk about them in too much detail, but I will "
"show you some pictures."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:15
msgid ""
"The **t-distribution** is a continuous distribution that looks very similar "
"to a normal distribution, see :numref:`fig-tdist`. Note that the “tails” of "
"the *t*-distribution are “heavier” (i.e., extend further outwards) than the "
"tails of the normal distribution). That’s the important difference between "
"the two. This distribution tends to arise in situations where you think that "
"the data actually follow a normal distribution, but you don’t know the mean "
"or standard deviation. We’ll run into this distribution again in chapter :"
"doc:`Comparing two means <../Ch11/Ch11_tTest>`."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:30
msgid "t-distribution with df = 3 in comparison to a normal distribution"
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:30
msgid ""
"t-distribution with 3 degrees of freedom (solid line). It looks similar to a "
"normal distribution, but it’s not quite the same. For comparison purposes "
"I’ve plotted a standard normal distribution as the dashed line."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:36
msgid ""
"The **χ²-distribution** is another distribution that turns up in lots of "
"different places. The situation in which we’ll see it is when doing :doc:"
"`Categorical data analysis <../Ch10/Ch10_ChiSquare>`, but it’s one of those "
"things that actually pops up all over the place. When you dig into the maths "
"(and who doesn’t love doing that?), it turns out that the main reason why "
"the *χ²*-distribution turns up all over the place is that if you have a "
"bunch of variables that are normally distributed, square their values and "
"then add them up (a procedure referred to as taking a “sum of squares”), "
"this sum has a *χ²*-distribution. You’d be amazed how often this fact turns "
"out to be useful. Anyway, :numref:`fig-chiSqDist` illustrates what a χ²-"
"distribution looks like."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:54
msgid "χ²-distribution with df = 3"
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:54
msgid ""
"χ²-distribution with 3 degrees of freedom. Notice that the observed values "
"must always be greater than zero, and that the distribution is pretty "
"skewed. These are the key features of a χ²-distribution."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:60
msgid ""
"The **F-distribution** looks a bit like a *χ²*-distribution, and it arises "
"whenever you need to compare two *χ²*-distributions to one another. "
"Admittedly, this doesn’t exactly sound like something that any sane person "
"would want to do, but it turns out to be very important in real world data "
"analysis. Remember when I said that *χ²* turns out to be the key "
"distribution when we’re taking a “sum of squares”? Well, what that means is "
"if you want to compare two different “sums of squares”, you’re probably "
"talking about something that has an *F*-distribution. Of course, as yet I "
"still haven’t given you an example of anything that involves a sum of "
"squares, but I will in chapter :doc:`Comparing several means (one-way ANOVA) "
"<../Ch13/Ch13_ANOVA>`. And that’s where we’ll run into the *F*-distribution. "
"Oh, and there’s a picture in :numref:`fig-Fdist`."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:79
msgid "F-distribution with df = 3 and df = 5"
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:79
msgid ""
"F-distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it "
"looks pretty similar to a χ²-distribution, but they’re not quite the same in "
"general."
msgstr ""

#: ../../Ch07/Ch07_Probability_6.rst:85
msgid ""
"Okay, time to wrap this section up. We’ve seen three new distributions: "
"*χ²*, *t* and *F*. They’re all continuous distributions, and they’re all "
"closely related to the normal distribution. The main thing for our purposes "
"is that you grasp the basic idea that these distributions are all deeply "
"related to one another, and to the normal distribution. Later on in this "
"book we’re going to run into data that are normally distributed, or at least "
"assumed to be normally distributed. What I want you to understand right now "
"is that, if you make the assumption that your data are normally distributed, "
"you shouldn’t be surprised to see *χ²*-, *t*-and *F*-distributions popping "
"up all over the place when you start trying to do your data analysis."
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:4
msgid "Summary"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:6
msgid ""
"In this chapter we’ve talked about probability. We’ve talked about what "
"probability means and why statisticians can’t agree on what it means. We "
"talked about the rules that probabilities have to obey. And we introduced "
"the idea of a probability distribution and spent a good chunk of the chapter "
"talking about some of the more important probability distributions that "
"statisticians work with. The section by section breakdown looks like this:"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:14
msgid ""
":doc:`Probability theory versus statistics <../Ch07/Ch07_Probability_1>`"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:16
msgid ""
":doc:`Frequentist versus Bayesian views of probability <../Ch07/"
"Ch07_Probability_2>`"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:18
msgid ":doc:`Basics of probability theory <../Ch07/Ch07_Probability_3>`"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:20
msgid ""
":doc:`Binomial distribution <../Ch07/Ch07_Probability_4>`, :doc:`Normal "
"distribution <../Ch07/Ch07_Probability_5>`, and :doc:`Other useful "
"distributions <../Ch07/Ch07_Probability_6>`"
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:24
msgid ""
"As you’d expect, my coverage is by no means exhaustive. Probability theory "
"is a large branch of mathematics in its own right, entirely separate from "
"its application to statistics and data analysis. As such, there are "
"thousands of books written on the subject and universities generally offer "
"multiple classes devoted entirely to probability theory. Even the “simpler” "
"task of documenting standard probability distributions is a big topic. I’ve "
"described five standard probability distributions in this chapter, but "
"sitting on my bookshelf I have a 45-chapter book called “Statistical "
"Distributions” (`Evans, 2000 <../Other/References.html#evans-2000>`__\\ ) "
"that lists a *lot* more than that. Fortunately for you, very little of this "
"is necessary. You’re unlikely to need to know dozens of statistical "
"distributions when you go out and do real world data analysis, and you "
"definitely won’t need them for this book, but it never hurts to know that "
"there’s other possibilities out there."
msgstr ""

#: ../../Ch07/Ch07_Probability_7.rst:38
msgid ""
"Picking up on that last point, there’s a sense in which this whole chapter "
"is something of a digression. Many undergraduate psychology classes on "
"statistics skim over this content very quickly (I know mine did), and even "
"the more advanced classes will often “forget” to revisit the basic "
"foundations of the field. Most academic psychologists would not know the "
"difference between probability and density, and until recently very few "
"would have been aware of the difference between Bayesian and frequentist "
"probability. However, I think it’s important to understand these things "
"before moving onto the applications. For example, there are a lot of rules "
"about what you’re “allowed” to say when doing statistical inference and many "
"of these can seem arbitrary and weird. However, they start to make sense if "
"you understand that there is this Bayesian / frequentist distinction. "
"Similarly, in chapter :doc:`Comparing two means <../Ch11/Ch11_tTest>` we’re "
"going to talk about something called the *t*-test, and if you really want to "
"have a grasp of the mechanics of the *t*-test it really helps to have a "
"sense of what a *t*-distribution actually looks like. You get the idea, I "
"hope."
msgstr ""
