msgid ""
msgstr ""
"Project-Id-Version: Learning statistics with jamovi\n"
"Report-Msgid-Bugs-To: sebastian.jentschke@uib.no\n"
"POT-Creation-Date: 2025-06-12 11:37+0200\n"
"PO-Revision-Date: 2025-09-02 18:01+0000\n"
"Last-Translator: Максим Горпиніч <gorpinicmaksim0@gmail.com>\n"
"Language-Team: Ukrainian <https://hosted.weblate.org/projects/lsjdocs/ch07/"
"uk/>\n"
"Language: uk\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=3; plural=(n%10==1 && n%100!=11 ? 0 : n%10>=2 && "
"n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2);\n"
"X-Generator: Weblate 5.13.1-dev\n"
"Generated-By: Babel 2.10.3\n"

#: ../../Ch07/Ch07_Probability.rst:4
msgid "Introduction to probability"
msgstr "Вступ до теорії ймовірності"

#: ../../Ch07/Ch07_Probability.rst:0
msgid "*[God] has afforded us only the twilight … of Probability.*"
msgstr "*[Бог] дарував нам лише сутінки … Ймовірності.*"

#: ../../Ch07/Ch07_Probability.rst:23
msgid "John Locke"
msgstr "Джон Локк"

#: ../../Ch07/Ch07_Probability.rst:25
msgid ""
"Up to this point in the book we’ve discussed some of the key ideas in "
"experimental design, and we’ve talked a little about how you can summarise a "
"data set. To a lot of people this is all there is to statistics: collecting "
"all the numbers, calculating averages, drawing pictures, and putting them "
"all in a report somewhere. Kind of like stamp collecting but with numbers. "
"However, statistics covers much more than that. In fact, descriptive "
"statistics is one of the smallest parts of statistics and one of the least "
"powerful. The bigger and more useful part of statistics is that it provides "
"information that lets you make inferences about data."
msgstr ""
"До цього моменту в книзі ми обговорили деякі ключові ідеї експериментального "
"дизайну та трохи поговорили про те, як можна узагальнити набір даних. Для "
"багатьох людей це все, що стосується статистики: збір усіх чисел, обчислення "
"середніх значень, малювання графіків і розміщення їх у звіті. Щось на зразок "
"колекціонування марок, але з цифрами. Однак статистика охоплює набагато "
"більше. Насправді, описова статистика є однією з найменших частин статистики "
"і однією з найменш потужних. Більша і корисніша частина статистики полягає в "
"тому, що вона надає інформацію, яка дозволяє робити висновки про дані."

#: ../../Ch07/Ch07_Probability.rst:36
msgid ""
"Once you start thinking about statistics in these terms, that statistics is "
"there to help us draw inferences from data, you start seeing examples of it "
"everywhere. For instance, here’s a tiny extract from a newspaper article in "
"the Sydney Morning Herald (30 Oct 2010):"
msgstr ""
"Як тільки ви почнете думати про статистику в таких термінах, що статистика "
"існує для того, щоб допомогти нам робити висновки на основі даних, ви "
"почнете бачити приклади цього всюди. Наприклад, ось невеликий уривок із "
"статті в газеті Sydney Morning Herald (30 жовтня 2010 р.):"

#: ../../Ch07/Ch07_Probability.rst:41
msgid ""
"“I have a tough job,” the Premier said in response to a poll which found her "
"government is now the most unpopular Labor administration in polling "
"history, with a primary vote of just 23 per cent."
msgstr ""
"«У мене важка робота», – сказала прем’єр-міністр у відповідь на опитування, "
"яке показало, що її уряд зараз є найнепопулярнішою адміністрацією "
"лейбористів в історії опитувань, маючи лише 23 відсотки голосів на праймеріз."

#: ../../Ch07/Ch07_Probability.rst:45
msgid ""
"This kind of remark is entirely unremarkable in the papers or in everyday "
"life, but let’s have a think about what it entails. A polling company has "
"conducted a survey, usually a pretty big one because they can afford it. I’m "
"too lazy to track down the original survey so let’s just imagine that they "
"called 1000 New South Wales (NSW) voters at random, and 230 (23\\%) of those "
"claimed that they intended to vote for the Australian Labor Party (ALP). For "
"the 2010 Federal election the Australian Electoral Commission reported "
"4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 "
"voters (about 99.98\\% of voters) remain unknown to us. Even assuming that "
"no-one lied to the polling company the only thing we can say with 100\\% "
"confidence is that the true ALP primary vote is somewhere between "
"230/4610795 (about \\0.005\\%) and 4610025/4610795 (about 99.83\\%). So, on "
"what basis is it legitimate for the polling company, the newspaper, and the "
"readership to conclude that the ALP primary vote is only about 23\\%?"
msgstr ""
"Такі зауваження є цілком звичайними в газетах або в повсякденному житті, але "
"давайте поміркуємо, що вони означають. Опитувальна компанія провела "
"опитування, як правило, досить велике, оскільки вона може собі це дозволити. "
"Я занадто лінивий, щоб відшукати оригінальне опитування, тож давайте просто "
"уявимо, що вони зателефонували 1000 виборців Нового Південного Уельсу (NSW) "
"навмання, і 230 (23 %) з них заявили, що мають намір голосувати за "
"Австралійську лейбористську партію (ALP). За даними Австралійської виборчої "
"комісії, на федеральних виборах 2010 року в штаті Новий Південний Уельс було "
"зареєстровано 4 610 795 виборців, тому думка решти 4 609 795 виборців ("
"близько 99,98 % виборців) нам невідома. Навіть якщо припустити, що ніхто не "
"брехав опитувальній компанії, єдине, що ми можемо сказати зі 100\\% "
"впевненістю, це те, що справжній первинний результат голосування за АЛП "
"знаходиться десь між 230/4610795 (близько 0,005\\% ) і 4610025/4610795 ("
"близько 99,83\\% ). Тож на якій підставі опитувальна компанія, газета та "
"читацька аудиторія можуть зробити висновок, що первинна кількість голосів за "
"АЛП становить лише близько 23\\%?"

#: ../../Ch07/Ch07_Probability.rst:61
msgid ""
"The answer to the question is pretty obvious. If I call 1000 people at "
"random, and 230 of them say they intend to vote for the ALP, then it seems "
"very unlikely that these are the *only* 230 people out of the entire voting "
"public who actually intend to vote ALP. In other words, we assume that the "
"data collected by the polling company is pretty representative of the "
"population at large. But how representative? Would we be surprised to "
"discover that the true ALP primary vote is actually 24\\%? 29\\%? 37\\%? At "
"this point everyday intuition starts to break down a bit. No-one would be "
"surprised by 24\\%, and everybody would be surprised by 37\\%, but it’s a "
"bit hard to say whether 29\\% is plausible. We need some more powerful tools "
"than just looking at the numbers and guessing."
msgstr ""
"Відповідь на це питання досить очевидна. Якщо я зателефоную 1000 випадково "
"обраних людей, і 230 з них скажуть, що мають намір голосувати за АЛП, то "
"здається дуже малоймовірним, що це *єдині* 230 людей з усіх виборців, які "
"насправді мають намір голосувати за АЛП. Іншими словами, ми припускаємо, що "
"дані, зібрані компанією, яка проводить опитування, є досить "
"репрезентативними для населення в цілому. Але наскільки репрезентативні? Чи "
"будемо ми здивовані, якщо виявимо, що справжній первинний результат "
"голосування за АЛП насправді становить 24 %? 29 %? 37 %? На цьому етапі "
"повсякденна інтуїція починає трохи давати збій. Ніхто не буде здивований "
"результатом 24 %, і всі будуть здивовані результатом 37 %, але трохи важко "
"сказати, чи є результат 29 % правдоподібним. Нам потрібні більш потужні "
"інструменти, ніж просто перегляд цифр і припущення."

#: ../../Ch07/Ch07_Probability.rst:73
msgid ""
"**Inferential statistics** provides the tools that we need to answer these "
"sorts of questions, and since these kinds of questions lie at the heart of "
"the scientific enterprise, they take up the lions share of every "
"introductory course on statistics and research methods. However, the theory "
"of statistical inference is built on top of **probability theory**. And it "
"is to probability theory that we must now turn. This discussion of "
"probability theory is basically background detail. There’s not a lot of "
"statistics per se in this chapter, and you don’t need to understand this "
"material in as much depth as the other chapters in this part of the book. "
"Nevertheless, because probability theory does underpin so much of "
"statistics, it’s worth covering some of the basics."
msgstr ""
"**Інференційна статистика** надає інструменти, необхідні для відповіді на "
"такі питання, і оскільки вони лежать в основі наукової діяльності, вони "
"займають левову частку кожного вступного курсу зі статистики та методів "
"дослідження. Однак теорія статистичної інференції побудована на основі **"
"теорії ймовірностей**. І саме до теорії ймовірностей ми зараз і звернемося. "
"Ця дискусія про теорію ймовірностей є, в основному, додатковою інформацією. "
"У цьому розділі не так багато статистики як такої, і вам не потрібно "
"розуміти цей матеріал так глибоко, як інші розділи цієї частини книги. "
"Проте, оскільки теорія ймовірності є основою статистики, варто розглянути "
"деякі її основи."

#: ../../Ch07/Ch07_Probability_1.rst:4
msgid "How are probability and statistics different?"
msgstr "Чим відрізняються ймовірність та статистика?"

#: ../../Ch07/Ch07_Probability_1.rst:6
msgid ""
"Before we start talking about probability theory, it’s helpful to spend a "
"moment thinking about the relationship between probability and statistics. "
"The two disciplines are closely related but they’re not identical. "
"Probability theory is “the doctrine of chances”. It’s a branch of "
"mathematics that tells you how often different kinds of events will happen. "
"For example, all of these questions are things you can answer using "
"probability theory:"
msgstr ""
"Перш ніж почати говорити про теорію ймовірностей, варто задуматися над "
"взаємозв'язком між ймовірністю та статистикою. Ці дві дисципліни тісно "
"пов'язані між собою, але не є ідентичними. Теорія ймовірностей — це «наука "
"про шанси». Це розділ математики, який вивчає, як часто відбуваються різні "
"події. Наприклад, на всі ці питання можна відповісти за допомогою теорії "
"ймовірностей:"

#: ../../Ch07/Ch07_Probability_1.rst:14
msgid "What are the chances of a fair coin coming up heads 10 times in a row?"
msgstr "Яка ймовірність того, що чесний джекпот випаде орлом 10 разів поспіль?"

#: ../../Ch07/Ch07_Probability_1.rst:17
msgid ""
"If I roll a six sided dice twice, how likely is it that I’ll roll two sixes?"
msgstr ""
"Якщо я двічі кину шестигранний кубик, яка ймовірність того, що випаде дві "
"шістки?"

#: ../../Ch07/Ch07_Probability_1.rst:20
msgid ""
"How likely is it that five cards drawn from a perfectly shuffled deck will "
"all be hearts?"
msgstr ""
"Яка ймовірність того, що всі п'ять карт, витягнутих з ідеально перетасованої "
"колоди, будуть червами?"

#: ../../Ch07/Ch07_Probability_1.rst:23
msgid "What are the chances that I’ll win the lottery?"
msgstr "Які в мене шанси виграти в лотерею?"

#: ../../Ch07/Ch07_Probability_1.rst:25
msgid ""
"Notice that all of these questions have something in common. In each case "
"the “truth of the world” is known and my question relates to the “what kind "
"of events” will happen. In the first question I *know* that the coin is fair "
"so there’s a 50\\% chance that any individual coin flip will come up heads. "
"In the second question I *know* that the chance of rolling a 6 on a single "
"die is 1 in 6. In the third question I *know* that the deck is shuffled "
"properly. And in the fourth question I *know* that the lottery follows "
"specific rules. You get the idea. The critical point is that probabilistic "
"questions start with a known **model** of the world, and we use that model "
"to do some calculations. The underlying model can be quite simple. For "
"instance, in the coin flipping example we can write down the model like this:"
msgstr ""
"Зверніть увагу, що всі ці питання мають щось спільне. У кожному випадку «"
"правда світу» відома, і моє питання стосується того, «які події» "
"відбудуться. У першому питанні я *знаю*, що монета чесна, тому ймовірність "
"того, що при будь-якому окремому підкиданні монети випаде решка, становить "
"50 %. У другому питанні я *знаю*, що ймовірність випадання 6 на одному "
"кубику становить 1 до 6. У третьому питанні я *знаю*, що колода карт "
"правильно перемішана. А в четвертому питанні я *знаю*, що лотерея "
"відбувається за певними правилами. Ви зрозуміли суть. Ключовим моментом є "
"те, що ймовірнісні питання починаються з відомої **моделі** світу, і ми "
"використовуємо цю модель для деяких розрахунків. Базова модель може бути "
"досить простою. Наприклад, у прикладі з підкиданням монети ми можемо "
"записати модель так:"

#: ../../Ch07/Ch07_Probability_1.rst:38
msgid "*P*\\ (heads) = *0.5*"
msgstr "*P*\\ (голови) = *0,5*"

#: ../../Ch07/Ch07_Probability_1.rst:40
msgid ""
"which you can read as “the probability of heads is 0.5”. As we’ll see later, "
"in the same way that percentages are numbers that range from 0\\% to 100\\%, "
"probabilities are just numbers that range from 0 to 1. When using this "
"probability model to answer the first question I don’t actually know exactly "
"what’s going to happen. Maybe I’ll get 10 heads, like the question says. But "
"maybe I’ll get three heads. That’s the key thing. In probability theory the "
"*model* is known but the *data* are not."
msgstr ""
"що можна прочитати як «ймовірність випадання решки становить 0,5». Як ми "
"побачимо пізніше, так само як відсотки є числами в діапазоні від 0 % до 100 "
"%, ймовірності є просто числами в діапазоні від 0 до 1. Використовуючи цю "
"модель ймовірності для відповіді на перше питання, я насправді не знаю, що "
"саме станеться. Можливо, я отримаю 10 решт, як зазначено в питанні. Але, "
"можливо, я отримаю три решти. Це є ключовим моментом. У теорії ймовірностей "
"*модель* є відомою, але *дані* — ні."

#: ../../Ch07/Ch07_Probability_1.rst:49
msgid ""
"So that’s probability. What about statistics? Statistical questions work the "
"other way around. In statistics we do not know the truth about the world. "
"All we have is the data and it is from the data that we want to *learn* the "
"truth about the world. Statistical questions tend to look more like these:"
msgstr ""
"Отже, це ймовірність. А що щодо статистики? Статистичні питання працюють "
"навпаки. У статистиці ми не знаємо істини про світ. Все, що ми маємо, — це "
"дані, і саме з цих даних ми хочемо *дізнатися* істину про світ. Статистичні "
"питання, як правило, виглядають так:"

#: ../../Ch07/Ch07_Probability_1.rst:55
msgid ""
"If my friend flips a coin 10 times and gets 10 heads are they playing a "
"trick on me?"
msgstr ""
"Якщо мій друг підкидає монету 10 разів і отримує 10 орлів, чи він мене "
"обманює?"

#: ../../Ch07/Ch07_Probability_1.rst:58
msgid ""
"If five cards off the top of the deck are all hearts how likely is it that "
"the deck was shuffled?"
msgstr ""
"Якщо п'ять карт зверху колоди — це всі черви, яка ймовірність того, що "
"колоду перетасували?"

#: ../../Ch07/Ch07_Probability_1.rst:61
msgid ""
"If the lottery commissioner’s spouse wins the lottery how likely is it that "
"the lottery was rigged?"
msgstr ""
"Якщо дружина комісара лотереї виграє в лотерею, наскільки ймовірно, що "
"лотерея була підтасована?"

#: ../../Ch07/Ch07_Probability_1.rst:64
msgid ""
"This time around the only thing we have are data. What I *know* is that I "
"saw my friend flip the coin 10 times and it came up heads every time. And "
"what I want to **infer** is whether or not I should conclude that what I "
"just saw was actually a fair coin being flipped 10 times in a row, or "
"whether I should suspect that my friend is playing a trick on me. The data I "
"have look like this:"
msgstr ""
"Цього разу все, що ми маємо, це дані. Я *знаю*, що бачив, як мій друг 10 "
"разів підкидав монету, і вона щоразу випадала решкою. І я хочу **зробити "
"висновок**, чи слід мені вважати, що я бачив, як справді 10 разів поспіль "
"підкидали чесну монету, чи слід мені підозрювати, що мій друг мене обдурив. "
"Дані, які я маю, виглядають так:"

#: ../../Ch07/Ch07_Probability_1.rst:75
msgid ""
"and what I’m trying to do is work out which “model of the world” I should "
"put my trust in. If the coin is fair then the model I should adopt is one "
"that says that the probability of heads is 0.5, that is *P*\\ (heads) = "
"*0.5*. If the coin is not fair then I should conclude that the probability "
"of heads is *not* 0.5, which we would write as *P*\\ (heads) ≠ *0.5*. In "
"other words, the statistical inference problem is to figure out which of "
"these probability models is right. Clearly, the statistical question isn’t "
"the same as the probability question, but they’re deeply connected to one "
"another. Because of this, a good introduction to statistical theory will "
"start with a discussion of what probability is and how it works."
msgstr ""
"і я намагаюся з'ясувати, якій «моделі світу» я повинен довіряти. Якщо монета "
"чесна, то я повинен прийняти модель, яка стверджує, що ймовірність випадання "
"решки становить 0,5, тобто *P*\\ (решка) = *0,5*. Якщо монета нечесна, то я "
"повинен зробити висновок, що ймовірність випадання решки *не* дорівнює 0,5, "
"що ми запишемо як *P*\\ (решка) ≠ *0,5*. Іншими словами, проблема "
"статистичного висновку полягає в тому, щоб з'ясувати, яка з цих моделей "
"ймовірності є правильною. Очевидно, що статистичне питання не є таким самим, "
"як питання ймовірності, але вони глибоко пов'язані між собою. Тому хороший "
"вступ до статистичної теорії починається з обговорення того, що таке "
"ймовірність і як вона працює."

#: ../../Ch07/Ch07_Probability_2.rst:4
msgid "What does probability mean?"
msgstr "Що означає ймовірність?"

#: ../../Ch07/Ch07_Probability_2.rst:6
msgid ""
"Let’s start with the first of these questions. What is “probability”? It "
"might seem surprising to you but while statisticians and mathematicians "
"(mostly) agree on what the *rules* of probability are, there’s much less of "
"a consensus on what the word really *means*. It seems weird because we’re "
"all very comfortable using words like “chance”, “likely”, “possible” and "
"“probable”, and it doesn’t seem like it should be a very difficult question "
"to answer. But if you’ve ever had that experience in real life you might "
"walk away from the conversation feeling like you didn’t quite get it right, "
"and that (like many everyday concepts) it turns out that you don’t *really* "
"know what it’s all about."
msgstr ""
"Почнемо з першого з цих питань. Що таке «ймовірність»? Це може здатися вам "
"дивним, але хоча статистики та математики (в основному) згодні з тим, що "
"таке *правила* ймовірності, набагато менше згоди існує щодо того, що "
"насправді *означає* це слово. Це здається дивним, оскільки ми всі дуже "
"звикли вживати такі слова, як «шанс», «ймовірний», «можливий» і «ймовірний», "
"і здається, що відповісти на це питання не повинно бути дуже складно. Але "
"якщо ви коли-небудь стикалися з цим у реальному житті, ви, можливо, пішли з "
"розмови з відчуттям, що не зовсім все зрозуміли, і що (як і у випадку з "
"багатьма повсякденними поняттями) ви *насправді* не знаєте, про що йдеться."

#: ../../Ch07/Ch07_Probability_2.rst:17
msgid ""
"So I’ll have a go at it. Let’s suppose I want to bet on a soccer game "
"between two teams of robots, *Arduino Arsenal* and *C Milan*. After thinking "
"about it, I decide that there is an 80\\% probability of *Arduino Arsenal* "
"winning. What do I mean by that? Here are three possibilities:"
msgstr ""
"Тож я спробую. Припустимо, я хочу зробити ставку на футбольний матч між "
"двома командами роботів, *Arduino Arsenal* і *C Milan*. Подумавши, я "
"вирішую, що ймовірність перемоги *Arduino Arsenal* становить 80 %. Що я маю "
"на увазі? Ось три можливості:"

#: ../../Ch07/Ch07_Probability_2.rst:22
msgid ""
"They’re robot teams so I can make them play over and over again, and if I "
"did that *Arduino Arsenal* would win 8 out of every 10 games on average."
msgstr ""
"Це команди-роботи, тому я можу змушувати їх грати знову і знову, і якби я це "
"робив, *Arduino Arsenal* вигравав би в середньому 8 з кожних 10 ігор."

#: ../../Ch07/Ch07_Probability_2.rst:26
msgid ""
"For any given game, I would agree that betting on this game is only “fair” "
"if a $1 bet on *C Milan* gives a $5 payoff (i.e. I get my $1 back plus a $4 "
"reward for being correct), as would a $4 bet on *Arduino Arsenal* (i.e., my "
"$4 bet plus a $1 reward)."
msgstr ""
"Для будь-якої гри я б погодився, що ставка на цю гру є «справедливою» лише в "
"тому випадку, якщо ставка в розмірі 1 долара на *C Milan* приносить виграш у "
"розмірі 5 доларів (тобто я отримую назад свої 1 долар плюс 4 долари "
"винагороди за правильний прогноз), так само як і ставка в розмірі 4 доларів "
"на *Arduino Arsenal* (тобто мої 4 долари ставки плюс 1 долар винагороди)."

#: ../../Ch07/Ch07_Probability_2.rst:31
msgid ""
"My subjective “belief” or “confidence” in an *Arduino Arsenal* victory is "
"four times as strong as my belief in a *C Milan* victory."
msgstr ""
"Моя суб'єктивна «віра» або «впевненість» у перемозі *Arduino Arsenal* у "
"чотири рази сильніша, ніж моя віра у перемогу *C Milan*."

#: ../../Ch07/Ch07_Probability_2.rst:34
msgid ""
"Each of these seems sensible. However, they’re not identical and not every "
"statistician would endorse all of them. The reason is that there are "
"different statistical ideologies (yes, really!) and depending on which one "
"you subscribe to, you might say that some of those statements are "
"meaningless or irrelevant. In this section I give a brief introduction the "
"two main approaches that exist in the literature. These are by no means the "
"only approaches, but they’re the two big ones."
msgstr ""
"Кожен з них здається розумним. Однак вони не є ідентичними, і не кожен "
"статистик погодиться з усіма ними. Причина полягає в тому, що існують різні "
"статистичні ідеології (так, справді!), і залежно від того, яку з них ви "
"підтримуєте, ви можете сказати, що деякі з цих тверджень є безглуздими або "
"нерелевантними. У цьому розділі я даю короткий огляд двох основних підходів, "
"що існують у літературі. Це аж ніяк не єдині підходи, але вони є двома "
"основними."

#: ../../Ch07/Ch07_Probability_2.rst:43
msgid "The frequentist view"
msgstr "Частотний погляд"

#: ../../Ch07/Ch07_Probability_2.rst:45
msgid ""
"The first of the two major approaches to probability, and the more dominant "
"one in statistics, is referred to as the **frequentist view** and it defines "
"probability as a **long-run frequency**. Suppose we were to try flipping a "
"fair coin over and over again. By definition this is a coin that has "
"*P*\\(H) = *0.5*. What might we observe? One possibility is that the first "
"20 flips might look like this:"
msgstr ""
"Перший з двох основних підходів до ймовірності, який є більш домінуючим у "
"статистиці, називається **фреквентистським підходом** і визначає ймовірність "
"як **довгострокову частоту**. Припустимо, що ми намагаємося кидати "
"справедливу монету знову і знову. За визначенням, це монета, яка має *P*\\(H)"
" = *0,5*. Що ми можемо спостерігати? Один із можливих варіантів — перші 20 "
"підкидань можуть виглядати так:"

#: ../../Ch07/Ch07_Probability_2.rst:56
msgid ""
"In this case 11 of these 20 coin flips (55\\%) came up heads. Now suppose "
"that I’d been keeping a running tally of the number of heads (which I’ll "
"call *N*\\ :sub:`H`\\ ) that I’ve seen, across the first *N* flips, and "
"calculate the proportion of heads *N*\\ :sub:`H` / *N* every time. Here’s "
"what I’d get (I did literally flip coins to produce this!):"
msgstr ""
"У цьому випадку 11 з 20 підкидань монети (55\\%) випали «решкою». Тепер "
"припустимо, що я вела підрахунок кількості «решок» (яку я назву *N*\\ "
":sub:`H`\\ ), які я бачила, протягом перших *N* підкидань, і щоразу "
"обчислювала пропорцію «решок» *N*\\ :sub:`H` / *N*. Ось що я отримав (я "
"буквально підкидав монети, щоб це обчислити!):"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:71
msgid "number of flips"
msgstr "кількість перекидів"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "1"
msgstr "1"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "2"
msgstr "2"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "3"
msgstr "3"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "4"
msgstr "4"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "5"
msgstr "5"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "6"
msgstr "6"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:65
msgid "7"
msgstr "7"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "8"
msgstr "8"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "9"
msgstr "9"

#: ../../Ch07/Ch07_Probability_2.rst:63 ../../Ch07/Ch07_Probability_2.rst:73
msgid "10"
msgstr "10"

#: ../../Ch07/Ch07_Probability_2.rst:65 ../../Ch07/Ch07_Probability_2.rst:73
msgid "**number of heads**"
msgstr "**кількість голів**"

#: ../../Ch07/Ch07_Probability_2.rst:65
msgid "0"
msgstr "0"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "**proportion**"
msgstr "**пропорція**"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.00"
msgstr "0.00"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.50"
msgstr "0.50"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.67"
msgstr "0.67"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.75"
msgstr "0.75"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.80"
msgstr "0.80"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.57"
msgstr "0.57"

#: ../../Ch07/Ch07_Probability_2.rst:67 ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.63"
msgstr "0.63"

#: ../../Ch07/Ch07_Probability_2.rst:67
msgid "0.70"
msgstr "0.70"

#: ../../Ch07/Ch07_Probability_2.rst:71 ../../Ch07/Ch07_Probability_2.rst:73
msgid "11"
msgstr "11"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "12"
msgstr "12"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "13"
msgstr "13"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "14"
msgstr "14"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "15"
msgstr "15"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "16"
msgstr "16"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "17"
msgstr "17"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "18"
msgstr "18"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "19"
msgstr "19"

#: ../../Ch07/Ch07_Probability_2.rst:71
msgid "20"
msgstr "20"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.73"
msgstr "0.73"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.69"
msgstr "0.69"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.71"
msgstr "0.71"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.59"
msgstr "0.59"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.56"
msgstr "0.56"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.53"
msgstr "0.53"

#: ../../Ch07/Ch07_Probability_2.rst:75
msgid "0.55"
msgstr "0.55"

#: ../../Ch07/Ch07_Probability_2.rst:78
msgid ""
"Notice that at the start of the sequence the *proportion* of heads "
"fluctuates wildly, starting at 0.00 and rising as high as 0.80. Later on, "
"one gets the impression that it dampens out a bit, with more and more of the "
"values actually being pretty close to the “right” answer of 0.50. This is "
"the frequentist definition of probability in a nutshell. Flip a fair coin "
"over and over again, and as *N* grows large (approaches infinity, denoted "
"*N* → ∞) the proportion of heads will converge to 50\\%. There are some "
"subtle technicalities that the mathematicians care about, but qualitatively "
"speaking that’s how the frequentists define probability. Unfortunately, I "
"don’t have an infinite number of coins or the infinite patience required to "
"flip a coin an infinite number of times. However, I do have a computer and "
"computers excel at mindless repetitive tasks. So I asked my computer to "
"simulate flipping a coin 1000 times and then drew a picture of what happens "
"to the proportion *N*\\ :sub:`H` / *N* as *N* increases. Actually, I did it "
"four times just to make sure it wasn’t a fluke. The results are shown in :"
"numref:`fig-frequentistProb`. As you can see, the *proportion of observed "
"heads* eventually stops fluctuating and settles down. When it does, the "
"number at which it finally settles is the true probability of heads."
msgstr ""
"Зверніть увагу, що на початку послідовності *пропорція* решт сильно "
"коливається, починаючи з 0,00 і піднімаючись до 0,80. Пізніше складається "
"враження, що вона трохи зменшується, і все більше значень наближаються до "
"«правильної» відповіді 0,50. Це, коротко кажучи, частотне визначення "
"ймовірності. Кидайте справедливу монету знову і знову, і коли *N* стає "
"великим (наближається до нескінченності, позначається *N* → ∞), пропорція "
"решт наближатиметься до 50\\%. Є деякі тонкі технічні нюанси, які цікавлять "
"математиків, але якісно кажучи, саме так частотники визначають ймовірність. "
"На жаль, я не маю нескінченної кількості монет або нескінченного терпіння, "
"необхідного для того, щоб кидати монету нескінченну кількість разів. Однак у "
"мене є комп'ютер, а комп'ютери чудово справляються з бездумними "
"повторюваними завданнями. Тому я попросив свій комп'ютер змоделювати кидання "
"монети 1000 разів, а потім намалював, що відбувається з часткою *N*\\ "
":sub:`H` / *N* при збільшенні *N*. Насправді я зробив це чотири рази, щоб "
"переконатися, що це не випадковість. Результати показані в :numref:`fig-"
"frequentistProb`. Як ви можете бачити, *частка спостережуваних решт* зрештою "
"перестає коливатися і стабілізується. Коли це відбувається, число, на якому "
"вона остаточно стабілізується, є справжньою ймовірністю решти."

#: ../../Ch07/Ch07_Probability_2.rst:100
msgid "Illustration of how frequentist probability works"
msgstr "Ілюстрація того, як працює частотна ймовірність"

#: ../../Ch07/Ch07_Probability_2.rst:104
msgid ""
"Illustration of how frequentist probability works: If you flip a fair coin "
"over and over again the proportion of heads that you’ve seen eventually "
"settles down and converges to the true probability of 0.5. Each panel shows "
"four different simulated experiments. In each case we pretend we flipped a "
"coin 1000 times and kept track of the proportion of flips that were heads as "
"we went along. Although none of these sequences actually ended up with an "
"exact value of 0.5, if we’d extended the experiment for an infinite number "
"of coin flips they would have."
msgstr ""
"Ілюстрація того, як працює частотна ймовірність: якщо ви кидаєте чесну "
"монету знову і знову, частка решт, яку ви бачите, зрештою стабілізується і "
"наближається до справжньої ймовірності 0,5. Кожна панель показує чотири "
"різні модельовані експерименти. У кожному випадку ми уявляємо, що підкинули "
"монету 1000 разів і відстежували частку підкидань, які були решками. Хоча "
"жодна з цих послідовностей насправді не закінчилася точним значенням 0,5, "
"якби ми продовжили експеримент до нескінченної кількості підкидань монети, "
"вони б закінчилися саме так."

#: ../../Ch07/Ch07_Probability_2.rst:115
msgid ""
"The frequentist definition of probability has some desirable "
"characteristics. First, it is objective. The probability of an event is "
"*necessarily* grounded in the world. The only way that probability "
"statements can make sense is if they refer to (a sequence of) events that "
"occur in the physical universe.\\ [#]_ Secondly, it is unambiguous. Any two "
"people watching the same sequence of events unfold, trying to calculate the "
"probability of an event, must inevitably come up with the same answer."
msgstr ""
"Частотне визначення ймовірності має деякі бажані характеристики. По-перше, "
"воно є об'єктивним. Ймовірність події *необхідно* ґрунтується на реальному "
"світі. Єдиний спосіб, у який твердження про ймовірність можуть мати сенс, — "
"це якщо вони стосуються (послідовності) подій, що відбуваються у фізичному "
"всесвіті. [#]_ По-друге, воно є однозначним. Дві людини, які спостерігають "
"за однією і тією ж послідовністю подій і намагаються обчислити ймовірність "
"події, неминуче дійдуть одного і того ж висновку."

#: ../../Ch07/Ch07_Probability_2.rst:124
msgid ""
"However, it also has undesirable characteristics. First, infinite sequences "
"don’t exist in the physical world. Suppose you picked up a coin from your "
"pocket and started to flip it. Every time it lands it impacts on the ground. "
"Each impact wears the coin down a bit. Eventually the coin will be "
"destroyed. So, one might ask whether it really makes sense to pretend that "
"an “infinite” sequence of coin flips is even a meaningful concept, or an "
"objective one. We can’t say that an “infinite sequence” of events is a real "
"thing in the physical universe, because the physical universe doesn’t allow "
"infinite anything. More seriously, the frequentist definition has a narrow "
"scope. There are lots of things out there that human beings are happy to "
"assign probability to in everyday language, but cannot (even in theory) be "
"mapped onto a hypothetical sequence of events. For instance, if a "
"meteorologist comes on TV and says “the probability of rain in Adelaide on 2 "
"November 2048 is 60\\%” we humans are happy to accept this. But it’s not "
"clear how to define this in frequentist terms. There’s only one city of "
"Adelaide, and only one 2 November 2048. There’s no infinite sequence of "
"events here, just a one-off thing. Frequentist probability genuinely "
"*forbids* us from making probability statements about a single event. From "
"the frequentist perspective it will either rain tomorrow or it will not. "
"There is no “probability” that attaches to a single non-repeatable event. "
"Now, it should be said that there are some very clever tricks that "
"frequentists can use to get around this. One possibility is that what the "
"meteorologist means is something like “There is a category of days for which "
"I predict a 60\\% chance of rain, and if we look only across those days for "
"which I make this prediction, then on 60\\% of those days it will actually "
"rain”. It’s very weird and counterintuitive to think of it this way, but you "
"do see frequentists do this sometimes. And it *will* come up later in this "
"book (see :doc:`../Ch08/Ch08_Estimation_5`)."
msgstr ""
"Однак вона також має небажані характеристики. По-перше, нескінченні "
"послідовності не існують у фізичному світі. Припустимо, ви дістали монету з "
"кишені і почали її підкидати. Кожного разу, коли вона падає, вона вдаряється "
"об землю. Кожен удар трохи зношує монету. Зрештою монета буде знищена. Отже, "
"можна запитати, чи дійсно має сенс вважати, що «нескінченна» послідовність "
"підкидання монети є значущим або об'єктивним поняттям. Ми не можемо сказати, "
"що «нескінченна послідовність» подій є реальною річчю у фізичному всесвіті, "
"оскільки фізичний всесвіт не допускає нічого нескінченного. Більш серйозно, "
"частотне визначення має вузьку сферу застосування. Є багато речей, яким люди "
"з радістю присвоюють ймовірність у повсякденній мові, але які не можуть ("
"навіть теоретично) бути відображені на гіпотетичній послідовності подій. "
"Наприклад, якщо метеоролог з'являється на телебаченні і каже: «ймовірність "
"дощу в Аделаїді 2 листопада 2048 року становить 60 %», ми, люди, з радістю "
"приймаємо це. Але не ясно, як визначити це в термінах частоти. Є тільки одне "
"місто Аделаїда і тільки один 2 листопада 2048 року. Тут немає нескінченної "
"послідовності подій, а є тільки одноразова подія. Частотна ймовірність "
"дійсно *забороняє* нам робити ймовірнісні твердження про окрему подію. З "
"частотної точки зору, завтра або буде дощ, або не буде. Немає «ймовірності», "
"яка б стосувалася окремої неповторної події. Слід зазначити, що існують дуже "
"хитрі прийоми, які частотисти можуть використовувати, щоб обійти це. Один із "
"можливих варіантів — метеоролог має на увазі щось на кшталт «Є категорія "
"днів, для яких я прогнозую 60 % ймовірність дощу, і якщо ми розглянемо "
"тільки ті дні, для яких я роблю цей прогноз, то в 60 % цих днів насправді "
"буде дощ». Думати про це таким чином дуже дивно і суперечить інтуїції, але "
"частотисти іноді так роблять. І це *буде* згадуватися пізніше в цій книзі ("
"див. :doc:`../Ch08/Ch08_Estimation_5`)."

#: ../../Ch07/Ch07_Probability_2.rst:155
msgid "The Bayesian view"
msgstr "Байєсівський погляд"

#: ../../Ch07/Ch07_Probability_2.rst:157
msgid ""
"The **Bayesian view** of probability is often called the subjectivist view, "
"and although it has been a minority view among statisticians it has been "
"steadily gaining traction for the last several decades. There are many "
"flavours of Bayesianism, making it hard to say exactly what “the” Bayesian "
"view is. The most common way of thinking about subjective probability is to "
"define the probability of an event as the **degree of belief** that an "
"intelligent and rational agent assigns to that truth of that event. From "
"that perspective, probabilities don’t exist in the world but rather in the "
"thoughts and assumptions of people and other intelligent beings."
msgstr ""
"**Байєсівський погляд** на ймовірність часто називають суб'єктивістським, і "
"хоча він є меншості серед статистиків, протягом останніх десятиліть він "
"постійно набирає популярність. Існує багато різновидів байєсіанства, тому "
"важко точно сказати, що таке «байєсіанська точка зору». Найпоширеніший "
"спосіб мислення про суб'єктивну ймовірність полягає в тому, щоб визначити "
"ймовірність події як **ступінь віри**, яку розумний і раціональний агент "
"приділяє істинності цієї події. З цієї точки зору, ймовірності не існують у "
"світі, а існують у думках і припущеннях людей та інших розумних істот."

#: ../../Ch07/Ch07_Probability_2.rst:168
msgid ""
"However, in order for this approach to work we need some way of "
"operationalising “degree of belief”. One way that you can do this is to "
"formalise it in terms of “rational gambling”, though there are many other "
"ways. Suppose that I believe that there’s a 60\\% probability of rain "
"tomorrow. If someone offers me a bet that if it rains tomorrow then I win "
"$5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is "
"a pretty good bet. On the other hand, if I think that the probability of "
"rain is only 40\\% then it’s a bad bet to take. So we can operationalise the "
"notion of a “subjective probability” in terms of what bets I’m willing to "
"accept."
msgstr ""
"Однак, щоб цей підхід працював, нам потрібно якось операціоналізувати «"
"ступінь віри». Один із способів зробити це — формалізувати його в термінах «"
"раціонального азартного гри», хоча є й багато інших способів. Припустимо, я "
"вірю, що ймовірність дощу завтра становить 60 %. Якщо хтось пропонує мені "
"парі, що якщо завтра буде дощ, я виграю 5 доларів, а якщо дощу не буде, я "
"втрачу 5 доларів. З моєї точки зору, це досить вигідне парі. З іншого боку, "
"якщо я вважаю, що ймовірність дощу становить лише 40 %, то це невигідне "
"парі. Отже, ми можемо операціоналізувати поняття «суб'єктивної ймовірності» "
"з точки зору того, на які парі я готовий погодитися."

#: ../../Ch07/Ch07_Probability_2.rst:179
msgid ""
"What are the advantages and disadvantages to the Bayesian approach? The main "
"advantage is that it allows you to assign probabilities to any event you "
"want to. You don’t need to be limited to those events that are repeatable. "
"The main disadvantage (to many people) is that we can’t be purely objective. "
"Specifying a probability requires us to specify an entity that has the "
"relevant degree of belief. This entity might be a human, an alien, a robot, "
"or even a statistician. But there has to be an intelligent agent out there "
"that believes in things. To many people this is uncomfortable, it seems to "
"make probability arbitrary. Whilst the Bayesian approach requires that the "
"agent in question be rational (i.e., obey the rules of probability), it does "
"allow everyone to have their own beliefs. I can believe the coin is fair and "
"you don’t have to, even though we’re both rational. The frequentist view "
"doesn’t allow any two observers to attribute different probabilities to the "
"same event. When that happens then at least one of them must be wrong. The "
"Bayesian view does not prevent this from occurring. Two observers with "
"different background knowledge can legitimately hold different beliefs about "
"the same event. In short, where the frequentist view is sometimes considered "
"to be too narrow (forbids lots of things that that we want to assign "
"probabilities to), the Bayesian view is sometimes thought to be too broad "
"(allows too many differences between observers)."
msgstr ""
"Які переваги та недоліки має байєсівський підхід? Головна перевага полягає в "
"тому, що він дозволяє присвоювати ймовірності будь-яким подіям. Не потрібно "
"обмежуватися лише тими подіями, які можна повторити. Головний недолік (для "
"багатьох людей) полягає в тому, що ми не можемо бути суто об'єктивними. Щоб "
"визначити ймовірність, нам потрібно вказати суб'єкта, який має відповідний "
"ступінь переконаності. Цим суб'єктом може бути людина, інопланетянин, робот "
"або навіть статистик. Але має бути розумний агент, який вірить у щось. "
"Багатьом людям це не подобається, оскільки це робить ймовірність довільною. "
"Хоча байєсівський підхід вимагає, щоб даний агент був раціональним (тобто "
"дотримувався правил ймовірності), він дозволяє кожному мати власні "
"переконання. Я можу вірити, що монета чесна, а ви не мусите, навіть якщо ми "
"обоє раціональні. Фреквентистський погляд не дозволяє двом спостерігачам "
"приписати різні ймовірності одній і тій самій події. Коли це відбувається, "
"принаймні один з них повинен помилятися. Байєсівський погляд не заважає "
"цьому. Два спостерігачі з різними знаннями можуть законно мати різні "
"переконання щодо однієї і тієї ж події. Коротко кажучи, якщо частотний "
"погляд іноді вважається занадто вузьким (забороняє багато речей, яким ми "
"хочемо присвоїти ймовірності), то байєсівський погляд іноді вважається "
"занадто широким (дозволяє занадто багато розбіжностей між спостерігачами)."

#: ../../Ch07/Ch07_Probability_2.rst:202
msgid "What’s the difference? And who is right?"
msgstr "Яка різниця? І хто правий?"

#: ../../Ch07/Ch07_Probability_2.rst:204
msgid ""
"Now that you’ve seen each of these two views independently it’s useful to "
"make sure you can compare the two. Go back to the hypothetical robot soccer "
"game at the start of the section. What do you think a frequentist and a "
"Bayesian would say about these three statements? Which statement would a "
"frequentist say is the correct definition of probability? Which one would a "
"Bayesian opt for? Would some of these statements be meaningless to a "
"frequentist or a Bayesian? If you’ve understood the two perspectives you "
"should have some sense of how to answer those questions."
msgstr ""
"Тепер, коли ви ознайомилися з кожним із цих двох поглядів окремо, корисно "
"переконатися, що ви можете порівняти їх між собою. Поверніться до "
"гіпотетичної гри в футбол між роботами на початку розділу. Як ви думаєте, що "
"б сказали прихильники частотного та байєсівського підходів про ці три "
"твердження? Яке твердження прихильники частотного підходу вважали б "
"правильним визначенням ймовірності? Яке твердження обрали б прихильники "
"байєсівського підходу? Чи були б деякі з цих тверджень безглуздими для "
"прихильників частотного або байєсівського підходів? Якщо ви зрозуміли обидва "
"погляди, ви повинні мати уявлення про те, як відповісти на ці питання."

#: ../../Ch07/Ch07_Probability_2.rst:214
msgid ""
"Okay, assuming you understand the difference then you might be wondering "
"which of them is *right*? Honestly, I don’t know that there is a right "
"answer. As far as I can tell there’s nothing mathematically incorrect about "
"the way frequentists think about sequences of events, and there’s nothing "
"mathematically incorrect about the way that Bayesians define the beliefs of "
"a rational agent. In fact, when you dig down into the details Bayesians and "
"frequentists actually agree about a lot of things. Many frequentist methods "
"lead to decisions that Bayesians agree a rational agent would make. Many "
"Bayesian methods have very good frequentist properties."
msgstr ""
"Гаразд, припустимо, ви розумієте різницю, і тепер вам цікаво, який з них "
"*правильний*? Чесно кажучи, я не знаю, чи є правильна відповідь. Наскільки я "
"розумію, в тому, як частотисти розглядають послідовності подій, немає нічого "
"математично неправильного, так само як і в тому, як баєсіанці визначають "
"переконання раціонального агента. Насправді, якщо заглибитися в деталі, "
"баєсіанці та частотники насправді погоджуються з багатьма речами. Багато "
"частотницьких методів призводять до рішень, які, на думку баєсіанців, "
"прийняв би раціональний агент. Багато баєсіанських методів мають дуже хороші "
"частотницькі властивості."

#: ../../Ch07/Ch07_Probability_2.rst:225
msgid ""
"For the most part, I’m a pragmatist so I’ll use any statistical method that "
"I trust. As it turns out, that makes me prefer Bayesian methods for reasons "
"I’ll explain towards the end of the book. But I’m not fundamentally opposed "
"to frequentist methods. Not everyone is quite so relaxed. For instance, "
"consider Sir Ronald Fisher, one of the towering figures of 20th century "
"statistics and a vehement opponent to all things Bayesian, whose paper on "
"the mathematical foundations of statistics referred to Bayesian probability "
"as “an impenetrable jungle [that] arrests progress towards precision of "
"statistical concepts” (:ref:`Fisher, 1922b <Fisher_1922b>`). Or the "
"psychologist Paul Meehl, who suggests that relying on frequentist methods "
"could turn you into “a potent but sterile intellectual rake who leaves in "
"his merry path a long train of ravished maidens but no viable scientific "
"offspring” (:ref:`Meehl, 1967 <Meehl_1967>`; p. 114). The history of "
"statistics, as you might gather, is not devoid of entertainment."
msgstr ""
"В основному я прагматик, тому використовую будь-який статистичний метод, "
"якому довіряю. Як виявляється, це змушує мене віддавати перевагу "
"байєсівським методам з причин, які я поясню в кінці книги. Але я не є "
"принциповим противником частотних методів. Не всі так спокійно ставляться до "
"цього питання. Наприклад, візьмемо сера Рональда Фішера, одну з видатних "
"постатей статистики 20 століття і запеклого противника всього байєсівського, "
"чия стаття про математичні основи статистики називала байєсівську "
"ймовірність «непроникною джунглями, що гальмують прогрес у напрямку точності "
"статистичних понять» (:ref:`Fisher, 1922b <Fisher_1922b>`). Або психолога "
"Пола Міла, який вважає, що покладання на частотні методи може перетворити "
"вас на «потужного, але безплідного інтелектуального розпусника, який залишає "
"за собою довгий шлейф зґвалтованих дівчат, але жодного життєздатного "
"наукового потомства» (:ref:`Meehl, 1967 <Meehl_1967>`; с. 114). Як ви можете "
"зрозуміти, історія статистики не позбавлена розважальних моментів."

#: ../../Ch07/Ch07_Probability_2.rst:241
msgid ""
"In any case, whilst I personally prefer the Bayesian view, the majority of "
"statistical analyses are based on the frequentist approach. My reasoning is "
"pragmatic. The goal of this book is to cover roughly the same territory as a "
"typical undergraduate stats class in psychology, and if you want to "
"understand the statistical tools used by most psychologists you’ll need a "
"good grasp of frequentist methods. I promise you that this isn’t wasted "
"effort. Even if you end up wanting to switch to the Bayesian perspective, "
"you really should read through at least one book on the “orthodox” "
"frequentist view. Besides, I won’t completely ignore the Bayesian "
"perspective. Every now and then I’ll add some commentary from a Bayesian "
"point of view, and I’ll revisit the topic in more depth in chapter :doc:`../"
"Ch16/Ch16_Bayes`."
msgstr ""
"У будь-якому випадку, хоча я особисто віддаю перевагу байєсівському підходу, "
"більшість статистичних аналізів базуються на частотному підході. Моє "
"міркування є прагматичним. Мета цієї книги — охопити приблизно ту саму "
"територію, що й типовий курс статистики для студентів-психологів, і якщо ви "
"хочете зрозуміти статистичні інструменти, які використовують більшість "
"психологів, вам потрібно добре опанувати частотні методи. Я обіцяю, що це не "
"буде марною тратою часу. Навіть якщо ви в кінцевому підсумку захочете "
"перейти на байєсівську перспективу, вам дійсно варто прочитати хоча б одну "
"книгу про «ортодоксальний» частотний підхід. Крім того, я не буду повністю "
"ігнорувати байєсівську перспективу. Час від часу я додаватиму коментарі з "
"байєсівської точки зору, а в розділі :doc:`../Ch16/Ch16_Bayes` я повернуся "
"до цієї теми більш детально."

#: ../../Ch07/Ch07_Probability_2.rst:257
msgid ""
"This doesn’t mean that frequentists can’t make hypothetical statements, of "
"course. It’s just that if you want to make a statement about probability "
"then it must be possible to redescribe that statement in terms of a sequence "
"of potentially observable events, together with the relative frequencies of "
"different outcomes that appear within that sequence."
msgstr ""
"Це, звичайно, не означає, що частотисти не можуть робити гіпотетичних "
"тверджень. Просто якщо ви хочете зробити твердження про ймовірність, то воно "
"повинно бути переписане у вигляді послідовності потенційно спостережуваних "
"подій разом з відносною частотою різних результатів, що з'являються в цій "
"послідовності."

#: ../../Ch07/Ch07_Probability_3.rst:4
msgid "Basic probability theory"
msgstr "Основна теорія ймовірностей"

#: ../../Ch07/Ch07_Probability_3.rst:6
msgid ""
"Ideological arguments between Bayesians and frequentists notwithstanding, it "
"turns out that people mostly agree on the rules that probabilities should "
"obey. There are lots of different ways of arriving at these rules. The most "
"commonly used approach is based on the work of Andrey Kolmogorov, one of the "
"great Soviet mathematicians of the 20th century. I won’t go into a lot of "
"detail, but I’ll try to give you a bit of a sense of how it works. And in "
"order to do so I’m going to have to talk about my trousers."
msgstr ""
"Незважаючи на ідеологічні суперечки між баєсіанцями та частотистами, "
"виявляється, що люди в основному згодні з правилами, яким повинні "
"підкорятися ймовірності. Існує безліч різних способів визначення цих правил. "
"Найпоширеніший підхід базується на роботах Андрія Колмогорова, одного з "
"видатних радянських математиків 20 століття. Я не буду вдаватися в "
"подробиці, але спробую дати вам уявлення про те, як це працює. А для цього "
"мені доведеться поговорити про свої штани."

#: ../../Ch07/Ch07_Probability_3.rst:16
msgid "Introducing probability distributions"
msgstr "Введення розподілів ймовірностей"

#: ../../Ch07/Ch07_Probability_3.rst:18
msgid ""
"One of the disturbing truths about my life is that I only own 5 pairs of "
"trousers. Three pairs of jeans, the bottom half of a suit, and a pair of "
"tracksuit pants. Even sadder, I’ve given them names: I call them *X*\\ :sub:"
"`1`\\ , *X*\\ :sub:`2`\\ , *X*\\ :sub:`3`\\ , *X*\\ :sub:`4`  and *X*\\ :sub:"
"`5`\\ . I really have, that’s why they call me Mister Imaginative. Now, on "
"any given day, I pick out exactly one of pair of trousers to wear. Not even "
"I’m so stupid as to try to wear two pairs of trousers, and thanks to years "
"of training I never go outside without wearing trousers anymore. If I were "
"to describe this situation using the language of probability theory, I would "
"refer to each pair of trousers (i.e., each *X*) as an **elementary event**. "
"The key characteristic of elementary events is that every time we make an "
"observation (e.g., every time I put on a pair of trousers) then the outcome "
"will be one and only one of these events. Like I said, these days I always "
"wear exactly one pair of trousers so my trousers satisfy this constraint. "
"Similarly, the set of all possible events is called a **sample space**. "
"Granted, some people would call it a “wardrobe”, but that’s because they’re "
"refusing to think about my trousers in probabilistic terms. Sad."
msgstr ""
"Однією з тривожних правд про моє життя є те, що я маю лише 5 пар штанів. Три "
"пари джинсів, нижню частину костюма та пару спортивних штанів. Ще сумніше "
"те, що я дав їм імена: я називаю їх *X*\\ :sub:`1`\\ , *X*\\ :sub:`2`\\ , *X*"
"\\ :sub:`3`\\ , *X*\\ :sub:`4`  і *X*\\ :sub:`5`\\ . Це справді так, тому "
"мене називають Містером Фантазером. Тепер кожного дня я вибираю саме одну "
"пару штанів, щоб одягти. Навіть я не настільки дурний, щоб намагатися одягти "
"дві пари штанів, і завдяки багаторічному тренуванню я більше ніколи не "
"виходжу на вулицю без штанів. Якби я мав описати цю ситуацію мовою теорії "
"ймовірностей, я б назвав кожну пару штанів (тобто кожну *X*) **елементарною "
"подією**. Ключовою характеристикою елементарних подій є те, що кожного разу, "
"коли ми робимо спостереження (наприклад, кожного разу, коли я одягаю штани), "
"результат буде одним і тільки одним із цих подій. Як я вже сказав, останнім "
"часом я завжди ношу саме одну пару штанів, тому мої штани відповідають цій "
"умові. Аналогічно, сукупність усіх можливих подій називається **просторовим "
"зразком**. Звісно, деякі люди називали б це «гардеробом», але це тому, що "
"вони відмовляються думати про мої штани в термінах ймовірності. Сумно."

#: ../../Ch07/Ch07_Probability_3.rst:37
msgid ""
"Okay, now that we have a sample space (a wardrobe), which is built from lots "
"of possible elementary events (trousers), what we want to do is assign a "
"**probability** of one of these elementary events. For an event *X*, the "
"probability of that event *P*\\ (X) is a number that lies between 0 and 1. "
"The bigger the value of *P*\\ (X), the more likely the event is to occur. "
"So, for example, if *P*\\ (X) = 0 it means the event *X* is impossible (i."
"e., I never wear those trousers). On the other hand, if *P*\\ (X) = 1 it "
"means that event *X* is certain to occur (i.e., I always wear those "
"trousers). For probability values in the middle it means that I sometimes "
"wear those trousers. For instance, if *P*\\ (X) = 0.5 it means that I wear "
"those trousers half of the time."
msgstr ""
"Отже, тепер, коли ми маємо пробний простір (гардероб), який складається з "
"безлічі можливих елементарних подій (штани), нам потрібно призначити "
"**ймовірність** однієї з цих елементарних подій. Для події *X* ймовірність "
"цієї події *P*\\ (X) є числом, яке лежить в діапазоні від 0 до 1. Чим більше "
"значення *P*\\ (X), тим більша ймовірність того, що подія відбудеться. Так, "
"наприклад, якщо *P*\\ (X) = 0, це означає, що подія *X* неможлива (тобто я "
"ніколи не ношу ці штани). З іншого боку, якщо *P*\\ (X) = 1, це означає, що "
"подія *X* обов'язково відбудеться (тобто я завжди ношу ці штани). Значення "
"ймовірності в середині означає, що я іноді ношу ці штани. Наприклад, якщо *P*"
"\\ (X) = 0,5, це означає, що я ношу ці штани половину часу."

#: ../../Ch07/Ch07_Probability_3.rst:50
msgid ""
"At this point, we’re almost done. The last thing we need to recognise is "
"that “something always happens”. Every time I put on trousers, I really do "
"end up wearing trousers (crazy, right?). What this somewhat trite statement "
"means, in probabilistic terms, is that the probabilities of the elementary "
"events need to add up to 1. This is known as the **law of total "
"probability**, not that any of us really care. More importantly, if these "
"requirements are satisfied then what we have is a **probability "
"distribution**. For example, this is an example of a probability "
"distribution:"
msgstr ""
"На цьому ми майже закінчили. Останнє, що нам потрібно усвідомити, це те, що «"
"щось завжди трапляється». Кожного разу, коли я одягаю штани, я дійсно в "
"кінцевому підсумку ношу штани (божевільно, правда?). Що це дещо банальне "
"твердження означає з точки зору ймовірності, так це те, що ймовірності "
"елементарних подій повинні складати в сумі 1. Це відоме як **закон повної "
"ймовірності**, але нас це не дуже цікавить. Більш важливо те, що якщо ці "
"вимоги виконуються, то ми маємо **розподіл ймовірностей**. Наприклад, ось "
"приклад розподілу ймовірностей:"

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Which trousers?"
msgstr "Які штани?"

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Label"
msgstr "Мітка"

#: ../../Ch07/Ch07_Probability_3.rst:61
msgid "Probability"
msgstr "Ймовірність"

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "Blue jeans"
msgstr "Сині джинси"

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "*X*\\ :sub:`1`"
msgstr "*X*\\ :sub:`1`"

#: ../../Ch07/Ch07_Probability_3.rst:63
msgid "*P*\\ (X\\ :sub:`1`\\ ) = 0.5"
msgstr "*P*\\ (X\\ :sub:`1`\\ ) = 0.5"

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "Grey jeans"
msgstr "Сірі джинси"

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "*X*\\ :sub:`2`"
msgstr "*X*\\ :sub:`2`"

#: ../../Ch07/Ch07_Probability_3.rst:65
msgid "*P*\\ (X\\ :sub:`2`\\ ) = 0.3"
msgstr "*P*\\ (X\\ :sub:`2`\\ ) = 0.3"

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "Black jeans"
msgstr "Чорні джинси"

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "*X*\\ :sub:`3`"
msgstr "*X*\\ :sub:`3`"

#: ../../Ch07/Ch07_Probability_3.rst:67
msgid "*P*\\ (X\\ :sub:`3`\\ ) = 0.1"
msgstr "*P*\\ (X\\ :sub:`3`\\ ) = 0.1"

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "Black suit"
msgstr "Чорний костюм"

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "*X*\\ :sub:`4`"
msgstr "*X*\\ :sub:`4`"

#: ../../Ch07/Ch07_Probability_3.rst:69
msgid "*P*\\ (X\\ :sub:`4`\\ ) = 0"
msgstr "*P*\\ (X\\ :sub:`4`\\ ) = 0"

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "Blue tracksuit"
msgstr "Синій спортивний костюм"

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "*X*\\ :sub:`5`"
msgstr "*X*\\ :sub:`5`"

#: ../../Ch07/Ch07_Probability_3.rst:71
msgid "*P*\\ (X\\ :sub:`5`\\ ) = 0.1"
msgstr "*P*\\ (X\\ :sub:`5`\\ ) = 0.1"

#: ../../Ch07/Ch07_Probability_3.rst:74
msgid ""
"Each of the events has a probability that lies between 0 and 1, and if we "
"add up the probability of all events they sum to 1. Awesome. We can even "
"draw a nice bar graph (see :doc:`../Ch05/Ch05_Graphics_3`) to visualise this "
"distribution, as shown in :numref:`fig-pantsDistribution`. And, at this "
"point, we’ve all achieved something. You’ve learned what a probability "
"distribution is, and I’ve finally managed to find a way to create a graph "
"that focuses entirely on my trousers. Everyone wins!"
msgstr ""
"Кожна з подій має ймовірність, що лежить між 0 і 1, і якщо ми додамо "
"ймовірність всіх подій, їх сума дорівнюватиме 1. Чудово. Ми навіть можемо "
"намалювати гарний гістограму (див. :doc:`../Ch05/Ch05_Graphics_3`), щоб "
"візуалізувати цей розподіл, як показано на :numref:`fig-pantsDistribution`. "
"І на цьому етапі ми всі чогось досягли. Ви дізналися, що таке розподіл "
"ймовірностей, а я нарешті знайшов спосіб створити графік, який повністю "
"зосереджений на моїх штанах. Усі виграли!"

#: ../../Ch07/Ch07_Probability_3.rst:84
msgid "“Trousers” probability distribution"
msgstr "Розподіл ймовірностей «штанів»"

#: ../../Ch07/Ch07_Probability_3.rst:88
msgid ""
"Visual depiction of the “trousers” probability distribution. There are five "
"“elementary events”, corresponding to the five pairs of trousers that I own. "
"Each event has some probability of occurring: this probability is a number "
"between 0 to 1. The sum of these probabilities is 1."
msgstr ""
"Візуальне зображення розподілу ймовірностей «штанів». Існує п'ять «"
"елементарних подій», що відповідають п'яти парам штанів, які я маю. Кожна "
"подія має певну ймовірність відбутися: ця ймовірність є числом від 0 до 1. "
"Сума цих ймовірностей дорівнює 1."

#: ../../Ch07/Ch07_Probability_3.rst:95
msgid ""
"The only other thing that I need to point out is that probability theory "
"allows you to talk about **non elementary events** as well as elementary "
"ones. The easiest way to illustrate the concept is with an example. In the "
"trousers example it’s perfectly legitimate to refer to the probability that "
"I wear jeans. In this scenario, the “Dani wears jeans” event is said to have "
"happened as long as the elementary event that actually did occur is one of "
"the appropriate ones. In this case “blue jeans”, “black jeans” or “grey "
"jeans”. In mathematical terms we defined the “jeans” event *E* to correspond "
"to the set of elementary events (X\\ :sub:`1`\\ , X\\ :sub:`2`\\ , X\\ :sub:"
"`3`\\ )`. If any of these elementary events occurs then *E* is also said to "
"have occurred. Having decided to write down the definition of the *E* this "
"way, it’s pretty straightforward to state what the probability *P*\\ (E) is: "
"we just add everything up. In this particular case"
msgstr ""
"Єдине, на що я ще хочу звернути увагу, це те, що теорія ймовірностей "
"дозволяє говорити як про **неелементарні події**, так і про елементарні. "
"Найпростіший спосіб проілюструвати це поняття — навести приклад. У прикладі "
"з брюками цілком правомірно говорити про ймовірність того, що я ношу джинси. "
"У цьому сценарії подія «Дані носить джинси» вважається такою, що відбулася, "
"якщо елементарна подія, яка фактично відбулася, є однією з відповідних. У "
"цьому випадку це «сині джинси», «чорні джинси» або «сірі джинси». У "
"математичних термінах ми визначили подію «джинси» *E* як таку, що відповідає "
"множині елементарних подій (X\\ :sub:`1`\\ , X\\ :sub:`2`\\ , X\\ :sub:`3`\\ "
")`. Якщо будь-яка з цих елементарних подій відбувається, то вважається, що "
"*E* також відбулася. Вирішивши записати визначення *E* таким чином, досить "
"просто визначити, що таке ймовірність *P*\\ (E): ми просто додаємо все "
"разом. У цьому конкретному випадку"

#: ../../Ch07/Ch07_Probability_3.rst:110
msgid ""
"*P*\\ (E) = *P*\\ (X\\ :sub:`1`\\ ) + *P*\\ (X\\ :sub:`2`\\ ) + *P*\\ (X\\ :"
"sub:`3`\\ )"
msgstr ""
"*P*\\ (E) = *P*\\ (X\\ :sub:`1`\\ ) + *P*\\ (X\\ :sub:`2`\\ ) + *P*\\ (X\\ "
":sub:`3`\\ )"

#: ../../Ch07/Ch07_Probability_3.rst:112
msgid ""
"and, since the probabilities of blue, grey and black jeans respectively are "
"0.5, 0.3 and 0.1, the probability that I wear jeans is equal to 0.9."
msgstr ""
"а оскільки ймовірності синіх, сірих та чорних джинсів відповідно становлять "
"0,5, 0,3 та 0,1, ймовірність того, що я ношу джинси, дорівнює 0,9."

#: ../../Ch07/Ch07_Probability_3.rst:115
msgid ""
"At this point you might be thinking that this is all terribly obvious and "
"simple and you’d be right. All we’ve really done is wrap some basic "
"mathematics around a few common sense intuitions. However, from these simple "
"beginnings it’s possible to construct some extremely powerful mathematical "
"tools. I’m definitely not going to go into the details in this book, but "
"what I will do is list, in :numref:`tab-probrules`, some of the other rules "
"that probabilities satisfy. These rules can be derived from the simple "
"assumptions that I’ve outlined above, but since we don’t actually use these "
"rules for anything in this book I won’t do so here."
msgstr ""
"На цьому етапі ви, можливо, думаєте, що все це надзвичайно очевидно і "
"просто, і ви маєте рацію. Все, що ми насправді зробили, це об'єднали деякі "
"базові математичні поняття з кількома інтуїтивними уявленнями, що базуються "
"на здоровому глузді. Однак, виходячи з цих простих початків, можна "
"побудувати надзвичайно потужні математичні інструменти. Я точно не буду "
"вдаватися в подробиці в цій книзі, але я перерахую в :numref:`tab-probrules` "
"деякі інші правила, яким підкоряються ймовірності. Ці правила можна вивести "
"з простих припущень, які я виклав вище, але оскільки ми насправді не "
"використовуємо ці правила для чогось у цій книзі, я не буду цього робити тут."

#: ../../Ch07/Ch07_Probability_3.rst:125
msgid ""
"Some basic rules that probabilities must satisfy. You don’t really need to "
"know these rules in order to understand the analyses that we’ll talk about "
"later in the book, but they are important if you want to understand "
"probability theory a bit more deeply."
msgstr ""
"Деякі основні правила, яким повинні відповідати ймовірності. Вам не "
"обов'язково знати ці правила, щоб зрозуміти аналітичні результати, про які "
"ми поговоримо далі в цій книзі, але вони важливі, якщо ви хочете трохи "
"глибше зрозуміти теорію ймовірностей."

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "English"
msgstr "Англійська"

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "Notation"
msgstr "Нотація"

#: ../../Ch07/Ch07_Probability_3.rst:134
msgid "Formula"
msgstr "Формула"

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "not *A*"
msgstr "не *A*"

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "*P*\\ (¬ A)"
msgstr "*P*\\ (¬ A)"

#: ../../Ch07/Ch07_Probability_3.rst:136 ../../Ch07/Ch07_Probability_3.rst:138
#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "="
msgstr "="

#: ../../Ch07/Ch07_Probability_3.rst:136
msgid "1 - *P*\\ (A)"
msgstr "1 - *P*\\ (A)"

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*A* or *B*"
msgstr "*A* або *B*"

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*P*\\ (A ∪ B)"
msgstr "*P*\\ (A ∪ B)"

#: ../../Ch07/Ch07_Probability_3.rst:138
msgid "*P*\\ (A) + *P*\\ (B) - *P*\\ (A ∩ B)"
msgstr "*P*\\ (A) + *P*\\ (B) - *P*\\ (A ∩ B)"

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*A* and *B*"
msgstr "*A* і *B*"

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*P*\\ (A ∩ B)"
msgstr "*P*\\ (A ∩ B)"

#: ../../Ch07/Ch07_Probability_3.rst:140
msgid "*P*\\ (A|B) *P*\\ (B)"
msgstr "*P*\\ (A|B) *P*\\ (B)"

#: ../../Ch07/Ch07_Probability_4.rst:4
msgid "The binomial distribution"
msgstr "Біноміальний розподіл"

#: ../../Ch07/Ch07_Probability_4.rst:6
msgid ""
"As you might imagine, probability distributions vary enormously and there’s "
"an enormous range of distributions out there. However, they aren’t all "
"equally important. In fact, the vast majority of the content in this book "
"relies on one of five distributions: the binomial distribution, the normal "
"distribution, the *t*-distribution, the χ²-distribution (chi-square) and the "
"*F*-distribution. Given this, what I’ll do over the next few sections is "
"provide a brief introduction to all five of these, paying special attention "
"to the binomial and the normal. I’ll start with the binomial distribution "
"since it’s the simplest of the five."
msgstr ""
"Як ви можете собі уявити, розподіли ймовірностей дуже різняться між собою, і "
"існує величезна кількість різних розподілів. Однак не всі вони однаково "
"важливі. Насправді, переважна більшість змісту цієї книги базується на "
"одному з п'яти розподілів: біноміальному розподілі, нормальному розподілі, "
"*t*-розподілі, χ²-розподілі (хі-квадрат) та *F*-розподілі. З огляду на це, у "
"наступних розділах я коротко розповім про всі п'ять розподілів, приділяючи "
"особливу увагу біноміальному та нормальному. Почну з біноміального "
"розподілу, оскільки він є найпростішим з п'яти."

#: ../../Ch07/Ch07_Probability_4.rst:17
msgid "Introducing the binomial"
msgstr "Введення бінома"

#: ../../Ch07/Ch07_Probability_4.rst:19
msgid ""
"The theory of probability originated in the attempt to describe how games of "
"chance work, so it seems fitting that our discussion of the **binomial "
"distribution** should involve a discussion of rolling dice and flipping "
"coins. Let’s imagine a simple “experiment”. In my hot little hand I’m "
"holding 20 identical six-sided dice. On one face of each die there’s a "
"picture of a skull, the other five faces are all blank. If I proceed to roll "
"all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming "
"that the dice are fair, we know that the chance of any one die coming up "
"skulls is 1 in 6. To say this another way, the skull probability for a "
"single die is approximately 0.167. This is enough information to answer our "
"question, so let’s have a look at how it’s done."
msgstr ""
"Теорія ймовірності виникла в результаті спроби описати, як працюють азартні "
"ігри, тому здається доречним, що наше обговорення **біноміального розподілу**"
" повинно включати обговорення кидання кубиків і підкидання монет. Давайте "
"уявимо собі простий «експеримент». У моїй гарячій руці я тримаю 20 однакових "
"шестигранних кубиків. На одній грані кожного кубика зображений череп, інші "
"п'ять граней порожні. Якщо я кину всі 20 кубиків, яка ймовірність того, що я "
"отримаю саме 4 черепа? Припускаючи, що кубики чесні, ми знаємо, що "
"ймовірність випадання черепа на будь-якому кубику становить 1 до 6. Іншими "
"словами, ймовірність випадання черепа на одному кубику становить приблизно "
"0,167. Цього достатньо, щоб відповісти на наше запитання, тож давайте "
"подивимося, як це зробити."

#: ../../Ch07/Ch07_Probability_4.rst:32
msgid ""
"Formulas for the binomial and normal distributions. We don’t really use "
"these formulas for anything in this book, but they’re pretty important for "
"more advanced work, so I thought it might be best to put them here in a "
"table, where they can’t get in the way of the text. In the equation for the "
"binomial, *X!* is the factorial function (i.e., multiply all whole numbers "
"from 1 to *X*), and for the normal distribution “exp” refers to the "
"exponential function, which we discussed in chapter :doc:`../Ch06/"
"Ch06_DataHandling`. If these equations don’t make a lot of sense to you, "
"don’t worry too much about them."
msgstr ""
"Формули для біноміального та нормального розподілів. Ми не використовуємо ці "
"формули в цій книзі, але вони є досить важливими для більш складних завдань, "
"тому я вирішив, що найкраще буде розмістити їх у таблиці, де вони не "
"заважатимуть тексту. У рівнянні для біноміального розподілу *X!* є "
"факторіальною функцією (тобто множенням усіх цілих чисел від 1 до *X*), а "
"для нормального розподілу «exp» означає експоненційну функцію, яку ми "
"обговорювали в розділі :doc:`../Ch06/Ch06_DataHandling`. Якщо ці рівняння не "
"мають для вас великого сенсу, не переймайтеся ними надто."

#: ../../Ch07/Ch07_Probability_4.rst:44
msgid "Binomial"
msgstr "Binomial"

#: ../../Ch07/Ch07_Probability_4.rst:44
msgid "Normal"
msgstr "Normal"

#: ../../Ch07/Ch07_Probability_4.rst:46
msgid "|binomial|"
msgstr "|binomial|"

#: ../../Ch07/Ch07_Probability_4.rst:46
msgid "|normal|"
msgstr "|normal|"

#: ../../Ch07/Ch07_Probability_4.rst:49
msgid ""
"As usual, we’ll want to introduce some names and some notation. We’ll let "
"*N* denote the number of dice rolls in our experiment, which is often "
"referred to as the **size parameter** of our binomial distribution. We’ll "
"also use *θ* to refer to the the probability that a single die comes up "
"skulls, a quantity that is usually called the **success probability** of the "
"binomial.\\ [#]_ Finally, we’ll use *X* to refer to the results of our "
"experiment, namely the number of skulls I get when I roll the dice. Since "
"the actual value of *X* is due to chance we refer to it as a **random "
"variable**. In any case, now that we have all this terminology and notation "
"we can use it to state the problem a little more precisely. The quantity "
"that we want to calculate is the probability that *X* = 4 given that we know "
"that *θ* = 0.167 and *N* = 20. The general “form” of the thing I’m "
"interested in calculating could be written as"
msgstr ""
"Як завжди, ми хочемо ввести деякі назви та позначення. Позначимо *N* "
"кількість кидків кубика в нашому експерименті, що часто називають **"
"параметром розміру** нашого біноміального розподілу. Також будемо "
"використовувати *θ* для позначення ймовірності того, що на одному кубику "
"випаде череп, величина, яку зазвичай називають **ймовірністю успіху** "
"біноміального розподілу. [#]_ Нарешті, ми будемо використовувати *X* для "
"позначення результатів нашого експерименту, а саме кількості випадання "
"«черепа», яку я отримую, коли кидаю кубик. Оскільки фактичне значення *X* "
"залежить від випадковості, ми називаємо його **випадковою величиною**. У "
"будь-якому випадку, тепер, коли ми маємо всю цю термінологію та нотацію, ми "
"можемо використовувати її, щоб сформулювати задачу трохи точніше. Величина, "
"яку ми хочемо обчислити, — це ймовірність того, що *X* = 4, якщо ми знаємо, "
"що *θ* = 0,167 і *N* = 20. Загальна «форма» того, що я хочу обчислити, може "
"бути записана так"

#: ../../Ch07/Ch07_Probability_4.rst:65
msgid "*P*\\ (X | θ, N)"
msgstr "*P*\\ (X | θ, N)"

#: ../../Ch07/Ch07_Probability_4.rst:67
msgid ""
"and we’re interested in the special case where *X* = 4, *θ* = 0.167 and *N* "
"= 20. There’s only one more piece of notation I want to refer to before "
"moving on to discuss the solution to the problem. If I want to say that *X* "
"is generated randomly from a binomial distribution with parameters *θ* and "
"*N*, the notation I would use is as follows:"
msgstr ""
"і нас цікавить особливий випадок, коли *X* = 4, *θ* = 0,167 і *N* = 20. Є ще "
"одна позначка, на яку я хочу звернути увагу, перш ніж перейти до обговорення "
"рішення задачі. Якщо я хочу сказати, що *X* генерується випадково з "
"біноміального розподілу з параметрами *θ* і *N*, я використовую таку "
"позначку:"

#: ../../Ch07/Ch07_Probability_4.rst:74
msgid "*X* ~ Binomial(θ, N)"
msgstr "*X* ~ Binomial(θ, N)"

#: ../../Ch07/Ch07_Probability_4.rst:76
msgid ""
"Yeah, yeah. I know what you’re thinking: notation, notation, notation. "
"Really, who cares? Very few readers of this book are here for the notation, "
"so I should probably move on and talk about how to use the binomial "
"distribution. I’ve included the formula for the binomial distribution in :"
"numref:`tab-distformulas`, since some readers may want to play with it "
"themselves, but since most people probably don’t care that much and because "
"we don’t need the formula in this book, I won’t talk about it in any detail. "
"Instead, I just want to show you what the binomial distribution looks like."
msgstr ""
"Так, так. Я знаю, про що ви думаєте: нотація, нотація, нотація. Насправді, "
"кому це цікаво? Дуже мало читачів цієї книги цікавляться нотацією, тому, "
"мабуть, я повинен перейти до того, як використовувати біноміальний розподіл. "
"Я включив формулу біноміального розподілу в :numref:`tab-distformulas`, "
"оскільки деякі читачі можуть захотіти погратися з нею самостійно, але "
"оскільки більшість людей, ймовірно, не дуже цим цікавляться, а нам ця "
"формула в цій книзі не потрібна, я не буду про неї детально розповідати. "
"Натомість я просто хочу показати вам, як виглядає біноміальний розподіл."

#: ../../Ch07/Ch07_Probability_4.rst:88
msgid "Binomial distribution for *N* = 20 and θ = 1/6"
msgstr "Біноміальний розподіл для *N* = 20 та θ = 1/6"

#: ../../Ch07/Ch07_Probability_4.rst:92
msgid ""
"Binomial distribution with size parameter of *N* = 20 and an underlying "
"success probability of θ = 1/6. Each vertical bar depicts the probability of "
"one specific outcome (i.e., one possible value of X). Because this is a "
"probability distribution, each of the probabilities must be a number between "
"0 and 1, and the heights of the bars must sum to 1 as well."
msgstr ""
"Біноміальний розподіл з параметром розміру *N* = 20 і базовою ймовірністю "
"успіху θ = 1/6. Кожна вертикальна смуга відображає ймовірність одного "
"конкретного результату (тобто одного можливого значення X). Оскільки це "
"розподіл ймовірностей, кожна з ймовірностей повинна бути числом від 0 до 1, "
"а висота смуг також повинна дорівнювати 1."

#: ../../Ch07/Ch07_Probability_4.rst:100
msgid ""
"To that end, :numref:`fig-binomSkulls20` plots the binomial probabilities "
"for all possible values of *X* for our dice rolling experiment, from *X* = 0 "
"(no skulls) all the way up to *X* = 20 (all skulls). Note that this is "
"basically a bar chart, and is no different to the “trousers probability” "
"plot I drew in :numref:`fig-pantsDistribution`. On the horizontal axis we "
"have all the possible events, and on the vertical axis we can read off the "
"probability of each of those events. So, the probability of rolling 4 skulls "
"out of 20 is about 0.20 (the actual answer is 0.2022036, as we’ll see in a "
"moment). In other words, you’d expect that to happen about 20\\% of the "
"times you repeated this experiment."
msgstr ""
"З цією метою :numref:`fig-binomSkulls20` відображає біноміальні ймовірності "
"для всіх можливих значень *X* для нашого експерименту з киданням кубиків, "
"від *X* = 0 (жодної черепа) до *X* = 20 (всі черепи). Зверніть увагу, що це, "
"по суті, гістограма, яка не відрізняється від графіка «ймовірності штанів», "
"який я намалював у :numref:`fig-pantsDistribution`. На горизонтальній осі ми "
"маємо всі можливі події, а на вертикальній осі ми можемо прочитати "
"ймовірність кожної з цих подій. Отже, ймовірність випадання 4 черепів з 20 "
"становить приблизно 0,20 (фактична відповідь — 0,2022036, як ми побачимо за "
"мить). Іншими словами, ви можете очікувати, що це трапиться приблизно в 20 % "
"випадків, коли ви повторите цей експеримент."

#: ../../Ch07/Ch07_Probability_4.rst:111
msgid ""
"To give you a feel for how the binomial distribution changes when we alter "
"the values of *θ* and *N*, let’s suppose that instead of rolling dice I’m "
"actually flipping coins. This time around, my experiment involves flipping a "
"fair coin repeatedly and the outcome that I’m interested in is the number of "
"heads that I observe. In this scenario, the success probability is now *θ* = "
"1/2. Suppose I were to flip the coin *N* = 20 times. In this example, I’ve "
"changed the success probability but kept the size of the experiment the "
"same. What does this do to our binomial distribution? Well, as the left "
"panel of :numref:`fig-binomHeads` shows, the main effect of this is to shift "
"the whole distribution, as you’d expect. Okay, what if we flipped a coin *N* "
"= 100 times? Well, in that case we get what is shown in the right panel. The "
"distribution stays roughly in the middle but there’s a bit more variability "
"in the possible outcomes."
msgstr ""
"Щоб ви могли зрозуміти, як змінюється біноміальний розподіл при зміні "
"значень *θ* і *N*, давайте уявимо, що замість кидання кубиків я кидаю "
"монети. Цього разу мій експеримент полягає в багаторазовому киданні чесної "
"монети, і результат, який мене цікавить, — це кількість випадів «решки», які "
"я спостерігаю. У цьому сценарії ймовірність успіху тепер дорівнює *θ* = 1/2. "
"Припустимо, я підкидаю монету *N* = 20 разів. У цьому прикладі я змінив "
"ймовірність успіху, але залишив розмір експерименту незмінним. Як це впливає "
"на наш біноміальний розподіл? Як показано на лівій панелі :numref:`fig-"
"binomHeads`, головним ефектом цього є зміщення всього розподілу, як і слід "
"було очікувати. Гаразд, а що буде, якщо ми підкинемо монету *N* = 100 разів? "
"У цьому випадку ми отримаємо те, що показано на правій панелі. Розподіл "
"залишається приблизно в середині, але можливі результати мають трохи більшу "
"мінливість."

#: ../../Ch07/Ch07_Probability_4.rst:128
msgid "Binomial distribution: θ = 1/2 and *N* = 20 (left) or *N* = 100 (right)"
msgstr "Біноміальний розподіл: θ = 1/2 and *N* = 20 (left) or *N* = 100 (right)"

#: ../../Ch07/Ch07_Probability_4.rst:132
msgid ""
"Two binomial distributions, involving a scenario in which I’m flipping a "
"fair coin, so the underlying success probability is θ = 1/2. In the left "
"panel, we assume I’m flipping the coin *N* = 20 times. In the right panel, "
"we assume that the coin is flipped *N* = 100 times."
msgstr ""
"Два біноміальні розподіли, що передбачають сценарій, в якому я підкидаю "
"чесну монету, тому базова ймовірність успіху становить θ = 1/2. На лівій "
"панелі ми припускаємо, що я підкидаю монету *N* = 20 разів. На правій панелі "
"ми припускаємо, що монету підкидають *N* = 100 разів."

#: ../../Ch07/Ch07_Probability_4.rst:142
msgid ""
"Note that the term “success” is pretty arbitrary and doesn’t actually imply "
"that the outcome is something to be desired. If *θ* referred to the "
"probability that any one passenger gets injured in a bus crash I’d still "
"call it the success probability, but that doesn’t mean I want people to get "
"hurt in bus crashes!"
msgstr ""
"Зверніть увагу, що термін «успіх» є досить довільним і насправді не означає, "
"що результат є бажаним. Якщо *θ* позначає ймовірність того, що будь-який "
"пасажир отримає травму в результаті аварії автобуса, я все одно називатиму "
"це ймовірністю успіху, але це не означає, що я хочу, щоб люди травмувалися в "
"аваріях автобусів!"

#: ../../Ch07/Ch07_Probability_5.rst:4
msgid "The normal distribution"
msgstr "Нормальний розподіл"

#: ../../Ch07/Ch07_Probability_5.rst:6
msgid ""
"While the binomial distribution is conceptually the simplest distribution to "
"understand, it’s not the most important one. That particular honour goes to "
"the **normal distribution**, also referred to as “the bell curve” or a "
"“Gaussian distribution”. A normal distribution is described using two "
"parameters: the mean of the distribution µ and the standard deviation of the "
"distribution σ."
msgstr ""
"Хоча біноміальний розподіл є концептуально найпростішим для розуміння, він "
"не є найважливішим. Ця честь належить **нормальному розподілу**, який також "
"називають «кривою дзвона» або «гаусовим розподілом». Нормальний розподіл "
"описується за допомогою двох параметрів: середнього значення розподілу µ та "
"стандартного відхилення розподілу σ."

#: ../../Ch07/Ch07_Probability_5.rst:13
msgid ""
"The notation that we sometimes use to say that a variable *X* is normally "
"distributed is as follows:"
msgstr ""
"Позначення, яке ми іноді використовуємо для того, щоб сказати, що змінна *X* "
"розподілена нормально, виглядає так:"

#: ../../Ch07/Ch07_Probability_5.rst:16
msgid "X ~ Normal(µ, σ)"
msgstr "X ~ Normal(µ, σ)"

#: ../../Ch07/Ch07_Probability_5.rst:18
msgid ""
"Of course, that’s just notation. It doesn’t tell us anything interesting "
"about the normal distribution itself. As was the case with the binomial "
"distribution, I have included the formula for the normal distribution in "
"this book, because I think it’s important enough that everyone who learns "
"statistics should at least look at it, but since this is an introductory "
"text I don’t want to focus on it, so I’ve tucked it away in :numref:`tab-"
"distformulas`."
msgstr ""
"Звичайно, це лише позначення. Воно не дає нам ніякої цікавої інформації про "
"сам нормальний розподіл. Як і у випадку з біноміальним розподілом, я включив "
"формулу нормального розподілу в цю книгу, тому що вважаю, що вона достатньо "
"важлива, щоб кожен, хто вивчає статистику, хоча б поглянув на неї, але "
"оскільки це вступний текст, я не хочу зосереджуватися на ній, тому я сховав "
"її в :numref:`tab-distformulas`."

#: ../../Ch07/Ch07_Probability_5.rst:28
msgid "Normal distribution with mean μ = 0 and standard deviation σ = 1"
msgstr ""
"Нормальний розподіл із середнім значенням μ = 0 та стандартним відхиленням σ "
"= 1"

#: ../../Ch07/Ch07_Probability_5.rst:32
msgid ""
"The normal distribution with mean μ = 0 and standard deviation σ = 1. The x-"
"axis corresponds to the value of some variable, and the y-axis tells us "
"something about how likely we are to observe that value. However, notice "
"that the y-axis is labelled “Probability Density” and not “Probability”. "
"There is a subtle and somewhat frustrating characteristic of continuous "
"distributions that makes the y axis behave a bit oddly: the height of the "
"curve here isn’t actually the probability of observing a particular x value. "
"On the other hand, it is true that the heights of the curve tells you which "
"x values are more likely (the higher ones!; see :ref:`Probability density "
"<probability_density>` for all the annoying details)."
msgstr ""
"Нормальний розподіл із середнім значенням μ = 0 і стандартним відхиленням σ "
"= 1. Вісь x відповідає значенню деякої змінної, а вісь y показує, наскільки "
"ймовірно ми спостерігатимемо це значення. Однак зверніть увагу, що вісь y "
"має назву «Щільність ймовірності», а не «Ймовірність». Існує тонкий і дещо "
"дратівливий характерний риса безперервних розподілів, який змушує вісь y "
"поводитися дещо дивно: висота кривої тут насправді не є ймовірністю "
"спостереження певного значення x. З іншого боку, висота кривої дійсно "
"вказує, які значення x є більш імовірними (ті, що вищі!; див. :ref:`"
"Probability density <probability_density>` для всіх дратівливих деталей)."

#: ../../Ch07/Ch07_Probability_5.rst:45
msgid ""
"Instead of focusing on the maths, let’s try to get a sense for what it means "
"for a variable to be normally distributed. To that end, have a look at :"
"numref:`fig-standardNormal` which plots a normal distribution with mean µ = "
"0 and standard deviation σ = 1. You can see where the name “bell curve” "
"comes from; it looks a bit like a bell. Notice that, unlike the plots that I "
"drew to illustrate the binomial distribution, the picture of the normal "
"distribution in :numref:`fig-standardNormal` shows a smooth curve instead of "
"“histogram-like” bars. This isn’t an arbitrary choice, the normal "
"distribution is continuous whereas the binomial is discrete. For instance, "
"in the die rolling example from the last section it was possible to get 3 "
"skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I "
"drew in the previous section reflected this fact. In :numref:`fig-"
"binomSkulls20`, for instance, there’s a bar located at *X* = 3 and another "
"one at *X* = 4 but there’s nothing in between. Continuous quantities don’t "
"have this constraint. For instance, suppose we’re talking about the weather. "
"The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, "
"23.9 degrees, or anything in between since temperature is a continuous "
"variable |continuous|. And so a normal distribution might be quite "
"appropriate for describing Spring temperatures.\\ [#]_"
msgstr ""
"Замість того, щоб зосередитися на математиці, спробуємо зрозуміти, що "
"означає нормальний розподіл змінної. Для цього погляньте на :numref:`fig-"
"standardNormal`, де зображено нормальний розподіл із середнім значенням µ = "
"0 і стандартним відхиленням σ = 1. Ви можете побачити, звідки походить назва "
"«крива дзвона»; вона трохи нагадує дзвін. Зверніть увагу, що на відміну від "
"графіків, які я намалював для ілюстрації біноміального розподілу, зображення "
"нормального розподілу в :numref:`fig-standardNormal` показує плавну криву "
"замість «гістограмоподібних» стовпчиків. Це не випадковий вибір, нормальний "
"розподіл є безперервним, тоді як біноміальний — дискретним. Наприклад, у "
"прикладі з киданням кубика з попереднього розділу можна було отримати 3 або "
"4 черепи, але неможливо було отримати 3,9 черепа. Рисунки, які я намалював у "
"попередньому розділі, відображали цей факт. Наприклад, у :numref:`fig-"
"binomSkulls20` є стовпчик, розташований на *X* = 3, і ще один на *X* = 4, "
"але між ними нічого немає. Безперервні величини не мають такого обмеження. "
"Наприклад, припустимо, що ми говоримо про погоду. Температура в приємний "
"весняний день може бути 23 градуси, 24 градуси, 23,9 градуси або будь-яка "
"інша величина між ними, оскільки температура є безперервною змінною "
"|безперервною|. Отже, нормальний розподіл може бути цілком відповідним для "
"опису весняних температур.\\ [#]_"

#: ../../Ch07/Ch07_Probability_5.rst:252
msgid "continuous"
msgstr "continuous"

#: ../../Ch07/Ch07_Probability_5.rst:68
msgid "Normal distribution: σ = 1 and µ = 4 (solid) or µ = 7 (dashed)"
msgstr "Нормальний розподіл: σ = 1 і µ = 4 (суцільний) або µ = 7 (штриховий)"

#: ../../Ch07/Ch07_Probability_5.rst:72
msgid ""
"Illustration of what happens when you change the mean of a normal "
"distribution. The solid line depicts a normal distribution with a mean of μ "
"= 4. The dashed line shows a normal distribution with a mean of μ = 7. In "
"both cases, the standard deviation is σ = 1. Not surprisingly, the two "
"distributions have the same shape, but the dashed line is shifted to the "
"right."
msgstr ""
"Ілюстрація того, що відбувається, коли ви змінюєте середнє значення "
"нормального розподілу. Суцільна лінія зображує нормальний розподіл із "
"середнім значенням μ = 4. Пунктирна лінія показує нормальний розподіл із "
"середнім значенням μ = 7. В обох випадках стандартне відхилення становить σ "
"= 1. Не дивно, що обидва розподіли мають однакову форму, але пунктирна лінія "
"зміщена вправо."

#: ../../Ch07/Ch07_Probability_5.rst:81
msgid ""
"With this in mind, let’s see if we can’t get an intuition for how the normal "
"distribution works. First, let’s have a look at what happens when we play "
"around with the parameters of the distribution. To that end, :numref:`fig-"
"meanShiftNormal` plots normal distributions that have different means but "
"have the same standard deviation. As you might expect, all of these "
"distributions have the same “width”. The only difference between them is "
"that they’ve been shifted to the left or to the right. In every other "
"respect they’re identical. In contrast, if we increase the standard "
"deviation while keeping the mean constant, the peak of the distribution "
"stays in the same place but the distribution gets wider, as you can see in :"
"numref:`fig-scaleShiftNormal`."
msgstr ""
"Маючи це на увазі, давайте подивимося, чи зможемо ми інтуїтивно зрозуміти, "
"як працює нормальний розподіл. Спочатку давайте подивимося, що відбувається, "
"коли ми змінюємо параметри розподілу. Для цього :numref:`fig-meanShiftNormal`"
" будує нормальні розподіли, які мають різні середні значення, але однакове "
"стандартне відхилення. Як і слід було очікувати, всі ці розподіли мають "
"однакову «ширину». Єдина різниця між ними полягає в тому, що вони зміщені "
"вліво або вправо. В усьому іншому вони ідентичні. Натомість, якщо ми "
"збільшимо стандартне відхилення, зберігаючи середнє значення незмінним, пік "
"розподілу залишиться на тому самому місці, але розподіл стане ширшим, як ви "
"можете бачити на :numref:`fig-scaleShiftNormal`."

#: ../../Ch07/Ch07_Probability_5.rst:95
msgid "Normal distribution: µ = 5 and σ = 1 (solid) or σ = 2 (dashed)"
msgstr "Нормальний розподіл: µ = 5 і σ = 1 (суцільний) або σ = 2 (штриховий)"

#: ../../Ch07/Ch07_Probability_5.rst:99
msgid ""
"Illustration of what happens when you change the standard deviation of a "
"normal distribution. Both distributions plotted in this figure have a mean "
"of μ = 5, but they have different standard deviations. The solid line plots "
"a distribution with standard deviation σ = 1, and the dashed line shows a "
"distribution with standard deviation σ = 2. As a consequence, both "
"distributions are “centred” on the same spot, but the dashed line is wider "
"than the solid one."
msgstr ""
"Ілюстрація того, що відбувається, коли ви змінюєте стандартне відхилення "
"нормального розподілу. Обидва розподіли, зображені на цьому рисунку, мають "
"середнє значення μ = 5, але вони мають різні стандартні відхилення. Суцільна "
"лінія зображує розподіл зі стандартним відхиленням σ = 1, а пунктирна лінія "
"показує розподіл зі стандартним відхиленням σ = 2. Як наслідок, обидва "
"розподіли «зосереджені» в одній точці, але пунктирна лінія ширша за суцільну."

#: ../../Ch07/Ch07_Probability_5.rst:109
msgid ""
"Notice, though, that when we widen the distribution the height of the peak "
"shrinks. This has to happen, in the same way that the heights of the bars "
"that we used to draw a discrete binomial distribution have to *sum* to 1, "
"the total *area under the curve* for the normal distribution must equal 1. "
"Before moving on, I want to point out one important characteristic of the "
"normal distribution. Irrespective of what the actual mean and standard "
"deviation are, 68.3\\% of the area falls within 1 standard deviation of the "
"mean. Similarly, 95.4\\% of the distribution falls within 2 standard "
"deviations of the mean, and 99.7\\% of the distribution is within 3 standard "
"deviations. This idea is illustrated in :numref:`fig-normAreaSD`."
msgstr ""
"Зверніть увагу, що коли ми розширюємо розподіл, висота піку зменшується. Це "
"має відбуватися, так само як висоти стовпчиків, які ми використовували для "
"побудови дискретного біноміального розподілу, мають *дорівнювати* 1, "
"загальна *площа під кривою* для нормального розподілу повинна дорівнювати 1. "
"Перш ніж продовжувати, я хочу вказати на одну важливу характеристику "
"нормального розподілу. Незалежно від того, якими є фактичне середнє значення "
"та стандартне відхилення, 68,3 % площі припадає на 1 стандартне відхилення "
"від середнього значення. Аналогічно, 95,4 % розподілу припадає на 2 "
"стандартні відхилення від середнього значення, а 99,7 % розподілу — на 3 "
"стандартні відхилення. Ця ідея проілюстрована на малюнку :numref:`fig-"
"normAreaSD`."

#: ../../Ch07/Ch07_Probability_5.rst:123
msgid "Normal distribution: area under the curve for 1 and 2 SD"
msgstr "Нормальний розподіл: площа під кривою для 1 та 2 стандартних відхилень"

#: ../../Ch07/Ch07_Probability_5.rst:127
msgid ""
"The area under the curve tells you the probability that an observation falls "
"within a particular range. The solid lines plot normal distributions with "
"mean μ = 0 and standard deviation σ = 1. The shaded areas illustrate “areas "
"under the curve” for two important cases. In the left panel, we can see that "
"there is a 68.3\\% chance that an observation will fall within one standard "
"deviation of the mean. In the right panel, we see that there is a 95.4\\% "
"chance that an observation will fall within two standard deviations of the "
"mean."
msgstr ""
"Площа під кривою показує ймовірність того, що спостереження потрапить у "
"певний діапазон. Суцільні лінії відображають нормальний розподіл із середнім "
"значенням μ = 0 і стандартним відхиленням σ = 1. Зафарбовані області "
"ілюструють «площі під кривою» для двох важливих випадків. На лівій панелі ми "
"бачимо, що ймовірність того, що спостереження потрапить у межах одного "
"стандартного відхилення від середнього значення, становить 68,3 %. На правій "
"панелі ми бачимо, що ймовірність того, що спостереження потрапить у межі "
"двох стандартних відхилень від середнього значення, становить 95,4 %."

#: ../../Ch07/Ch07_Probability_5.rst:138
msgid "Area under the curve for 1 SD bordering the mean and at the tails"
msgstr ""
"Площа під кривою для 1 стандартного відхилення на межі середнього значення "
"та на хвостах"

#: ../../Ch07/Ch07_Probability_5.rst:142
msgid ""
"Two more examples of the “area under the curve” idea. There is a 15.9\\% "
"chance that an observation is one standard deviation below the mean or "
"smaller (left panel), and a 34.1\\% chance that the observation is somewhere "
"between one standard deviation below the mean and the mean (right panel). "
"Notice that if you add these two numbers together you get 15.9\\% + 34.1\\% "
"= 50\\%. For normally distributed data, there is a 50\\% chance that an "
"observation falls below the mean. And of course that also implies that there "
"is a 50\\% chance that it falls above the mean."
msgstr ""
"Ще два приклади ідеї «площі під кривою». Існує 15,9 % ймовірності, що "
"спостереження буде на одне стандартне відхилення нижче середнього значення "
"або менше (ліва панель), і 34,1 % ймовірності, що спостереження буде десь "
"між одним стандартним відхиленням нижче середнього значення і середнім "
"значенням (права панель). Зверніть увагу, що якщо додати ці два числа, "
"отримаємо 15,9 % + 34,1 % = 50 %. Для нормально розподілених даних існує 50 "
"% ймовірність, що спостереження буде нижче середнього значення. І, звичайно, "
"це також означає, що існує 50 % ймовірність, що воно буде вище середнього "
"значення."

#: ../../Ch07/Ch07_Probability_5.rst:156
msgid "Probability density"
msgstr "Щільність ймовірності"

#: ../../Ch07/Ch07_Probability_5.rst:158
msgid ""
"There’s something I’ve been trying to hide throughout my discussion of the "
"normal distribution, something that some introductory textbooks omit "
"completely. They might be right to do so. This “thing” that I’m hiding is "
"weird and counter-intuitive even by the admittedly distorted standards that "
"apply in statistics. Fortunately, it’s not something that you need to "
"understand at a deep level in order to do basic statistics. Rather, it’s "
"something that starts to become important later on when you move beyond the "
"basics. So, if it doesn’t make complete sense, don’t worry too much, but try "
"to make sure that you follow the gist of it."
msgstr ""
"Є дещо, що я намагався приховати протягом усього обговорення нормального "
"розподілу, дещо, що деякі вступні підручники повністю опускають. Можливо, "
"вони мають рацію. Це «дещо», що я приховую, є дивним і суперечить інтуїції "
"навіть за визнано спотвореними стандартами, що застосовуються в статистиці. "
"На щастя, це не те, що потрібно глибоко розуміти для виконання базових "
"статистичних операцій. Це те, що стає важливим пізніше, коли ви виходите за "
"межі основ. Тож, якщо це не зовсім зрозуміло, не переймайтеся, але "
"постарайтеся зрозуміти суть."

#: ../../Ch07/Ch07_Probability_5.rst:168
msgid ""
"Throughout my discussion of the normal distribution there’s been one or two "
"things that don’t quite make sense. Perhaps you noticed that the *y*-axis in "
"these figures is labelled “Probability Density” rather than “Density”. Maybe "
"you noticed that I used *p*\\ (X) instead of *P*\\ (X) when giving the "
"formula for the normal distribution."
msgstr ""
"Протягом мого обговорення нормального розподілу було одне-два моменти, які "
"не зовсім зрозумілі. Можливо, ви помітили, що вісь *y* на цих рисунках має "
"назву «Щільність ймовірності», а не «Щільність». Можливо, ви помітили, що я "
"використовував *p*\\ (X) замість *P*\\ (X) при наведенні формули нормального "
"розподілу."

#: ../../Ch07/Ch07_Probability_5.rst:174
msgid ""
"As it turns out, what is presented here isn’t actually a probability, it’s "
"something else. To understand what that something is you have to spend a "
"little time thinking about what it really *means* to say that *X* is a "
"continuous variable |continuous|. Let’s say we’re talking about the "
"temperature outside. The thermometer tells me it’s 23 degrees, but I know "
"that’s not really true. It’s not *exactly* 23 degrees. Maybe it’s \\23.1 "
"degrees, I think to myself. But I know that that’s not really true either "
"because it might actually be 23.09 degrees. But I know that… well, you get "
"the idea. The tricky thing with genuinely continuous quantities is that you "
"never really know exactly what they are."
msgstr ""
"Як виявляється, те, що тут представлено, насправді не є ймовірністю, а "
"чимось іншим. Щоб зрозуміти, що це таке, потрібно трохи подумати над тим, що "
"насправді *означає* твердження, що *X* є безперервною змінною |безперервною|"
". Припустимо, ми говоримо про температуру на вулиці. Термометр показує 23 "
"градуси, але я знаю, що це не зовсім так. Це не *точно* 23 градуси. Можливо, "
"це 23,1 градуса, думаю я. Але я знаю, що це теж не зовсім правда, бо "
"насправді може бути 23,09 градуса. Але я знаю, що... ну, ви зрозуміли. "
"Складність із справді безперервними величинами полягає в тому, що ви ніколи "
"не знаєте точно, якими вони є."

#: ../../Ch07/Ch07_Probability_5.rst:185
msgid ""
"Now think about what this implies when we talk about probabilities. Suppose "
"that tomorrow’s maximum temperature is sampled from a normal distribution "
"with mean 23 and standard deviation 1. What’s the probability that the "
"temperature will be *exactly* 23 degrees? The answer is “zero”, or possibly "
"“a number so close to zero that it might as well be zero”. Why is this? It’s "
"like trying to throw a dart at an infinitely small dart board. No matter how "
"good your aim, you’ll never hit it. In real life you’ll never get a value of "
"exactly 23. It’ll always be something like 23.1 or 22.99998 or suchlike. In "
"other words, it’s completely meaningless to talk about the probability that "
"the temperature is exactly 23 degrees. However, in everyday language if I "
"told you that it was 23 degrees outside and it turned out to be 22.9998 "
"degrees you probably wouldn’t call me a liar. Because in everyday language "
"“23 degrees” usually means something like “somewhere between \\22.5 and 23.5 "
"degrees”. And while it doesn’t feel very meaningful to ask about the "
"probability that the temperature is exactly 23 degrees, it does seem "
"sensible to ask about the probability that the temperature lies between 22.5 "
"and 23.5, or between 20 and 30, or any other range of temperatures."
msgstr ""
"Тепер подумайте, що це означає, коли ми говоримо про ймовірності. "
"Припустимо, що максимальна температура завтрашнього дня відбирається з "
"нормального розподілу із середнім значенням 23 і стандартним відхиленням 1. "
"Яка ймовірність того, що температура буде *точно* 23 градуси? Відповідь — "
"«нуль» або, можливо, «число, настільки близьке до нуля, що його можна "
"вважати нулем». Чому так? Це як спроба кинути дротик у нескінченно малу "
"мішень. Як би добре ви не цілилися, ви ніколи не влучите. У реальному житті "
"ви ніколи не отримаєте значення рівне 23. Воно завжди буде десь 23,1 або "
"22,99998 тощо. Іншими словами, абсолютно безглуздо говорити про ймовірність "
"того, що температура буде рівно 23 градуси. Однак, якщо в повсякденній мові "
"я скажу вам, що на вулиці 23 градуси, а насправді виявиться, що 22,9998 "
"градуси, ви, мабуть, не назвете мене брехуном. Тому що в повсякденній мові «"
"23 градуси» зазвичай означає щось на зразок «десь між 22,5 і 23,5 градусами»"
". І хоча не має сенсу запитувати про ймовірність того, що температура буде "
"рівно 23 градуси, цілком розумно запитувати про ймовірність того, що "
"температура буде між 22,5 і 23,5, або між 20 і 30, або в будь-якому іншому "
"діапазоні температур."

#: ../../Ch07/Ch07_Probability_5.rst:205
msgid ""
"The point of this discussion is to make clear that when we’re talking about "
"continuous distributions it’s not meaningful to talk about the probability "
"of a specific value. However, what we *can* talk about is the probability "
"that the value lies within a particular range of values. To find out the "
"probability associated with a particular range what you need to do is "
"calculate the “area under the curve”. We’ve seen this concept already, in :"
"numref:`fig-normAreaSD` the shaded areas shown depict genuine probabilities "
"(e.g., in the left panel of :numref:`fig-normAreaSD` it shows the "
"probability of observing a value that falls within 1 standard deviation of "
"the mean)."
msgstr ""
"Мета цієї дискусії полягає в тому, щоб пояснити, що коли ми говоримо про "
"безперервні розподіли, не має сенсу говорити про ймовірність конкретного "
"значення. Однак ми *можемо* говорити про ймовірність того, що значення "
"лежить в певному діапазоні значень. Щоб дізнатися ймовірність, пов'язану з "
"певним діапазоном, потрібно обчислити «площу під кривою». Ми вже бачили це "
"поняття в :numref:`fig-normAreaSD`: зафарбовані області зображують справжні "
"ймовірності (наприклад, на лівій панелі :numref:`fig-normAreaSD` показано "
"ймовірність спостереження значення, яке знаходиться в межах 1 стандартного "
"відхилення від середнього значення)."

#: ../../Ch07/Ch07_Probability_5.rst:216
msgid ""
"Okay, so that explains part of the story. I’ve explained a little bit about "
"how continuous probability distributions should be interpreted (i.e., area "
"under the curve is the key thing). But what does the formula for *p*\\ (x) "
"that I described earlier actually mean? Obviously, p*\\ (x) doesn’t describe "
"a probability, but what is it? The name for this quantity *p*\\ (x) is a "
"**probability density**, and in terms of the plots we’ve been drawing it "
"corresponds to the *height* of the curve. The densities themselves aren’t "
"meaningful in and of themselves, but they’re “rigged” to ensure that the "
"*area* under the curve is always interpretable as genuine probabilities. To "
"be honest, that’s about as much as you really need to know for now.\\ [#]_"
msgstr ""
"Гаразд, це пояснює частину історії. Я трохи розповів про те, як слід "
"інтерпретувати безперервні розподіли ймовірностей (тобто, ключовим моментом "
"є площа під кривою). Але що насправді означає формула для *p*\\ (x), яку я "
"описав раніше? Очевидно, що p*\\ (x) не описує ймовірність, але що ж це "
"таке? Ця величина *p*\\ (x) називається **щільністю ймовірності**, і в "
"контексті графіків, які ми малювали, вона відповідає *висоті* кривої. Самі "
"по собі щільності не мають значення, але вони «налагоджені» таким чином, щоб "
"*площа* під кривою завжди інтерпретувалася як справжні ймовірності. Чесно "
"кажучи, це все, що вам потрібно знати на даний момент.\\ [#]_"

#: ../../Ch07/Ch07_Probability_5.rst:231
msgid ""
"In practice, the normal distribution is so handy that people tend to use it "
"even when the variable isn’t actually continuous. As long as there are "
"enough categories (e.g., Likert scale responses to a questionnaire), it’s "
"pretty standard practice to use the normal distribution as an approximation. "
"This works out much better in practice than you’d think."
msgstr ""
"На практиці нормальний розподіл настільки зручний, що люди схильні "
"використовувати його навіть тоді, коли змінна насправді не є безперервною. "
"Якщо є достатня кількість категорій (наприклад, відповіді за шкалою Лікерта "
"в анкеті), то цілком стандартною практикою є використання нормального "
"розподілу як наближення. На практиці це працює набагато краще, ніж можна "
"було б подумати."

#: ../../Ch07/Ch07_Probability_5.rst:239
msgid ""
"For those readers who know a little calculus, I’ll give a slightly more "
"precise explanation. In the same way that probabilities are non-negative "
"numbers that must sum to 1, probability densities are non-negative numbers "
"that must integrate to 1 (where the integral is taken across all possible "
"values of *X*). To calculate the probability that *X* falls between *a* and "
"*b* we calculate the definite integral of the density function over the "
"corresponding range, :math:`\\int_a^b p(x) \\ dx`. If you don’t remember or "
"never learned calculus, don’t worry about this. It’s not needed for this "
"book."
msgstr ""
"Для тих читачів, які трохи знаються на математичному аналізі, я дам трохи "
"більш точне пояснення. Так само, як ймовірності є невід'ємними числами, сума "
"яких дорівнює 1, щільності ймовірностей є невід'ємними числами, інтеграл "
"яких дорівнює 1 (де інтеграл береться по всіх можливих значеннях *X*). Щоб "
"обчислити ймовірність того, що *X* потрапить між *a* і *b*, ми обчислюємо "
"визначений інтеграл функції щільності по відповідному діапазону: :math:`"
"\\int_a^b p(x) \\ dx`. Якщо ви не пам'ятаєте або ніколи не вивчали "
"математичний аналіз, не хвилюйтеся. Це не потрібно для цієї книги."

#: ../../Ch07/Ch07_Probability_6.rst:4
msgid "Other useful distributions"
msgstr "Інші корисні дистрибутиви"

#: ../../Ch07/Ch07_Probability_6.rst:6
msgid ""
"The normal distribution is the distribution that statistics makes most use "
"of (for reasons to be discussed shortly), and the binomial distribution is a "
"very useful one for lots of purposes. But the world of statistics is filled "
"with probability distributions, some of which we’ll run into in passing. In "
"particular, the three that will appear in this book are the *t*-"
"distribution, the χ²-distribution and the *F*-distribution. I won’t give "
"formulas for any of these, or talk about them in too much detail, but I will "
"show you some pictures."
msgstr ""
"Нормальний розподіл є розподілом, який найчастіше використовується в "
"статистиці (причини цього будуть розглянуті трохи пізніше), а біноміальний "
"розподіл є дуже корисним для багатьох цілей. Але світ статистики наповнений "
"розподілами ймовірностей, деякі з яких ми зустрінемо мимохідь. Зокрема, три, "
"які з'являться в цій книзі, це *t*-розподіл, χ²-розподіл і *F*-розподіл. Я "
"не буду наводити формули для жодного з них і не буду говорити про них надто "
"детально, але покажу вам кілька малюнків."

#: ../../Ch07/Ch07_Probability_6.rst:15
msgid ""
"The **t-distribution** is a continuous distribution that looks very similar "
"to a normal distribution, see :numref:`fig-tdist`. Note that the “tails” of "
"the *t*-distribution are “heavier” (i.e., extend further outwards) than the "
"tails of the normal distribution). That’s the important difference between "
"the two. This distribution tends to arise in situations where you think that "
"the data actually follow a normal distribution, but you don’t know the mean "
"or standard deviation. We’ll run into this distribution again in chapter :"
"doc:`../Ch11/Ch11_tTest`."
msgstr ""
"**t-розподіл** — це безперервний розподіл, який дуже схожий на нормальний "
"розподіл, див. :numref:`fig-tdist`. Зверніть увагу, що «хвости» *t*-"
"розподілу «важчі» (тобто простягаються далі назовні), ніж хвости нормального "
"розподілу. Це важлива відмінність між цими двома розподілами. Цей розподіл, "
"як правило, виникає в ситуаціях, коли ви вважаєте, що дані насправді "
"підпорядковуються нормальному розподілу, але не знаєте середнього значення "
"або стандартного відхилення. Ми знову зустрінемося з цим розподілом у "
"розділі :doc:`../Ch11/Ch11_tTest`."

#: ../../Ch07/Ch07_Probability_6.rst:26
msgid "*t*-distribution with *df* = 3 in comparison to a normal distribution"
msgstr "*t*-розподіл з *df* = 3 у порівнянні з нормальним розподілом"

#: ../../Ch07/Ch07_Probability_6.rst:30
msgid ""
"*t*-distribution with 3 degrees of freedom (solid line). It looks similar to "
"a normal distribution, but it’s not quite the same. For comparison purposes "
"I’ve plotted a standard normal distribution as the dashed line."
msgstr ""
"*t*-розподіл з 3 ступенями свободи (суцільна лінія). Він схожий на "
"нормальний розподіл, але не зовсім такий самий. Для порівняння я зобразив "
"стандартний нормальний розподіл у вигляді пунктирної лінії."

#: ../../Ch07/Ch07_Probability_6.rst:36
msgid ""
"The **χ²-distribution** is another distribution that turns up in lots of "
"different places. The situation in which we’ll see it is when doing :doc:`../"
"Ch10/Ch10_ChiSquare`, but it’s one of those things that actually pops up all "
"over the place. When you dig into the maths (and who doesn’t love doing "
"that?), it turns out that the main reason why the χ²-distribution turns up "
"all over the place is that if you have a bunch of variables that are "
"normally distributed, square their values and then add them up (a procedure "
"referred to as taking a “sum of squares”), this sum has a χ²-distribution. "
"You’d be amazed how often this fact turns out to be useful. Anyway, :numref:"
"`fig-chiSqDist` illustrates what a χ²-distribution looks like."
msgstr ""
"**Розподіл χ²** — це ще один розподіл, який зустрічається в багатьох різних "
"місцях. Ми побачимо його під час виконання :doc:`../Ch10/Ch10_ChiSquare`, "
"але насправді він зустрічається повсюди. Якщо заглибитися в математику (а "
"хто ж цього не любить?), то виявляється, що головна причина, чому χ²-"
"розподіл зустрічається всюди, полягає в тому, що якщо ви маєте набір "
"змінних, які мають нормальний розподіл, піднесете їх значення до квадрата, а "
"потім додасте їх (процедура, яка називається «сума квадратів»), то ця сума "
"матиме χ²-розподіл. Ви здивуєтеся, як часто цей факт виявляється корисним. У "
"будь-якому разі, :numref:`fig-chiSqDist` ілюструє, як виглядає χ²-розподіл."

#: ../../Ch07/Ch07_Probability_6.rst:50
msgid "χ²-distribution with *df* = 3"
msgstr "χ²-розподіл з *df* = 3"

#: ../../Ch07/Ch07_Probability_6.rst:54
msgid ""
"χ²-distribution with 3 degrees of freedom. Notice that the observed values "
"must always be greater than zero, and that the distribution is pretty "
"skewed. These are the key features of a χ²-distribution."
msgstr ""
"χ²-розподіл з 3 ступенями свободи. Зверніть увагу, що спостережувані "
"значення завжди повинні бути більшими за нуль, і що розподіл досить "
"асиметричний. Це ключові характеристики χ²-розподілу."

#: ../../Ch07/Ch07_Probability_6.rst:60
msgid ""
"The **F-distribution** looks a bit like a χ²-distribution, and it arises "
"whenever you need to compare two χ²-distributions to one another. "
"Admittedly, this doesn’t exactly sound like something that any sane person "
"would want to do, but it turns out to be very important in real world data "
"analysis. Remember when I said that χ² turns out to be the key distribution "
"when we’re taking a “sum of squares”? Well, what that means is if you want "
"to compare two different “sums of squares”, you’re probably talking about "
"something that has an *F*-distribution. Of course, as yet I still haven’t "
"given you an example of anything that involves a sum of squares, but I will "
"in chapter :doc:`../Ch13/Ch13_ANOVA`. And that’s where we’ll run into the "
"*F*-distribution. Oh, and there’s a picture in :numref:`fig-Fdist`."
msgstr ""
"**F-розподіл** дещо схожий на χ²-розподіл і виникає, коли потрібно порівняти "
"два χ²-розподіли між собою. Звісно, це не звучить як щось, що хотів би "
"робити будь-яка розсудлива людина, але виявляється дуже важливим у реальному "
"аналізі даних. Пам'ятаєте, я казав, що χ² виявляється ключовим розподілом, "
"коли ми беремо «суму квадратів»? Це означає, що якщо ви хочете порівняти дві "
"різні «суми квадратів», ви, ймовірно, маєте на увазі щось, що має *F*-"
"розподіл. Звичайно, я ще не наводив прикладів, що стосуються суми квадратів, "
"але зроблю це в розділі :doc:`../Ch13/Ch13_ANOVA`. І саме там ми зіткнемося "
"з *F*-розподілом. О, і в :numref:`fig-Fdist` є ілюстрація."

#: ../../Ch07/Ch07_Probability_6.rst:74
msgid "*F*-distribution with *df* = 3 and *df* = 5"
msgstr "*F*-розподіл з *df* = 3 та *df* = 5"

#: ../../Ch07/Ch07_Probability_6.rst:78
msgid ""
"*F*-distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it "
"looks pretty similar to a χ²-distribution, but they’re not quite the same in "
"general."
msgstr ""
"*F*-розподіл з 3 та 5 ступенями свободи. Якісно кажучи, він досить схожий на "
"χ²-розподіл, але загалом вони не зовсім однакові."

#: ../../Ch07/Ch07_Probability_6.rst:84
msgid ""
"Okay, time to wrap this section up. We’ve seen three new distributions: *t*, "
"χ² and *F*. They’re all continuous distributions, and they’re all closely "
"related to the normal distribution. The main thing for our purposes is that "
"you grasp the basic idea that these distributions are all deeply related to "
"one another, and to the normal distribution. Later on in this book we’re "
"going to run into data that are normally distributed, or at least assumed to "
"be normally distributed. What I want you to understand right now is that, if "
"you make the assumption that your data are normally distributed, you "
"shouldn’t be surprised to see *t*-, χ²- and *F*-distributions popping up all "
"over the place when you start trying to do your data analysis."
msgstr ""
"Гаразд, час завершувати цей розділ. Ми розглянули три нові розподіли: *t*, "
"χ² та *F*. Всі вони є неперервними розподілами і тісно пов'язані з "
"нормальним розподілом. Головне для наших цілей — це зрозуміти основну ідею, "
"що всі ці розподіли тісно пов'язані між собою та з нормальним розподілом. "
"Далі в цій книзі ми зустрінемо дані, які мають нормальний розподіл або, "
"принаймні, вважаються такими, що мають нормальний розподіл. Зараз я хочу, "
"щоб ви зрозуміли: якщо ви припускаєте, що ваші дані мають нормальний "
"розподіл, не дивуйтеся, коли під час аналізу даних ви побачите *t*-, χ²- та "
"*F*-розподіли."

#: ../../Ch07/Ch07_Probability_7.rst:4
msgid "Summary"
msgstr "Короткий зміст"

#: ../../Ch07/Ch07_Probability_7.rst:6
msgid ""
"In this chapter we’ve talked about probability. We’ve talked about what "
"probability means and why statisticians can’t agree on what it means. We "
"talked about the rules that probabilities have to obey. And we introduced "
"the idea of a probability distribution and spent a good chunk of the chapter "
"talking about some of the more important probability distributions that "
"statisticians work with. The section by section breakdown looks like this:"
msgstr ""
"У цьому розділі ми говорили про ймовірність. Ми говорили про те, що означає "
"ймовірність і чому статистики не можуть дійти згоди щодо її значення. Ми "
"говорили про правила, яким повинна підкорятися ймовірність. Ми також "
"представили поняття розподілу ймовірностей і присвятили значну частину "
"розділу обговоренню деяких з найбільш важливих розподілів ймовірностей, з "
"якими працюють статистики. Розподіл по розділах виглядає наступним чином:"

#: ../../Ch07/Ch07_Probability_7.rst:14
msgid ":doc:`Probability theory versus statistics <Ch07_Probability_1>`"
msgstr ":doc:`Probability theory versus statistics <Ch07_Probability_1>`"

#: ../../Ch07/Ch07_Probability_7.rst:16
msgid ""
":doc:`Frequentist versus Bayesian views of probability <Ch07_Probability_2>`"
msgstr ""
":doc:`Frequentist versus Bayesian views of probability <Ch07_Probability_2>`"

#: ../../Ch07/Ch07_Probability_7.rst:18
msgid ":doc:`Basics of probability theory <Ch07_Probability_3>`"
msgstr ":doc:`Basics of probability theory <Ch07_Probability_3>`"

#: ../../Ch07/Ch07_Probability_7.rst:20
msgid ""
":doc:`Binomial distribution <Ch07_Probability_4>`, :doc:`normal distribution "
"<Ch07_Probability_5>`, and :doc:`other useful distributions "
"<Ch07_Probability_6>`"
msgstr ""
":doc:`Binomial distribution <Ch07_Probability_4>`, :doc:`normal distribution "
"<Ch07_Probability_5>`, і :doc:`other useful distributions "
"<Ch07_Probability_6>`"

#: ../../Ch07/Ch07_Probability_7.rst:24
msgid ""
"As you’d expect, my coverage is by no means exhaustive. Probability theory "
"is a large branch of mathematics in its own right, entirely separate from "
"its application to statistics and data analysis. As such, there are "
"thousands of books written on the subject and universities generally offer "
"multiple classes devoted entirely to probability theory. Even the “simpler” "
"task of documenting standard probability distributions is a big topic. I’ve "
"described five standard probability distributions in this chapter, but "
"sitting on my bookshelf I have a 47-chapter book called “Statistical "
"Distributions” (:ref:`Forbes et al., 2010 <Forbes_2010>`) that lists a *lot* "
"more than that. Fortunately for you, very little of this is necessary. "
"You’re unlikely to need to know dozens of statistical distributions when you "
"go out and do real world data analysis, and you definitely won’t need them "
"for this book, but it never hurts to know that there’s other possibilities "
"out there."
msgstr ""
"Як і слід було очікувати, моє висвітлення цієї теми аж ніяк не є вичерпним. "
"Теорія ймовірностей — це велика галузь математики, яка повністю відокремлена "
"від її застосування в статистиці та аналізі даних. Як така, вона є предметом "
"тисяч книг, а університети зазвичай пропонують кілька курсів, повністю "
"присвячених теорії ймовірностей. Навіть *простіше* завдання документування "
"стандартних розподілів ймовірностей є великою темою. У цьому розділі я "
"описав п'ять стандартних розподілів ймовірностей, але на моїй книжковій "
"полиці лежить 47-роздільна книга під назвою «Статистичні розподіли» (:ref:`"
"Forbes et al., 2010 <Forbes_2010>`), в якій перелічено набагато більше. На "
"щастя для вас, знати все це майже не потрібно. Вам навряд чи знадобиться "
"знати десятки статистичних розподілів, коли ви будете займатися аналізом "
"реальних даних, і вам точно не знадобляться вони для цієї книги, але ніколи "
"не завадить знати, що існують й інші можливості."

#: ../../Ch07/Ch07_Probability_7.rst:38
msgid ""
"Picking up on that last point, there’s a sense in which this whole chapter "
"is something of a digression. Many undergraduate psychology classes on "
"statistics skim over this content very quickly (I know mine did), and even "
"the more advanced classes will often “forget” to revisit the basic "
"foundations of the field. Most academic psychologists would not know the "
"difference between probability and density, and until recently very few "
"would have been aware of the difference between Bayesian and frequentist "
"probability. However, I think it’s important to understand these things "
"before moving onto the applications. For example, there are a lot of rules "
"about what you’re “allowed” to say when doing statistical inference and many "
"of these can seem arbitrary and weird. However, they start to make sense if "
"you understand that there is this Bayesian / frequentist distinction. "
"Similarly, in chapter :doc:`../Ch11/Ch11_tTest` we’re going to talk about "
"something called the *t*-test, and if you really want to have a grasp of the "
"mechanics of the *t*-test it really helps to have a sense of what a *t*-"
"distribution actually looks like. You get the idea, I hope."
msgstr ""
"Повертаючись до останнього пункту, можна сказати, що весь цей розділ є "
"певним відступом від теми. Багато курсів статистики для студентів-психологів "
"дуже швидко пробігають цей матеріал (я знаю, що так було на моєму курсі), і "
"навіть на більш просунутих курсах часто «забувають» повернутися до основних "
"основ цієї галузі. Більшість академічних психологів не знають різниці між "
"ймовірністю та щільністю, і донедавна дуже мало хто знав про різницю між "
"байєсівською та частотною ймовірністю. Однак я вважаю, що важливо зрозуміти "
"ці речі, перш ніж переходити до практичного застосування. Наприклад, існує "
"багато правил щодо того, що «дозволено» говорити під час статистичного "
"висновку, і багато з них можуть здаватися довільними та дивними. Однак вони "
"починають набувати сенсу, якщо ви розумієте, що існує різниця між "
"байєсівським і частотним підходами. Аналогічно, у розділі :doc:`../Ch11/"
"Ch11_tTest` ми поговоримо про те, що називається *t*-тестом, і якщо ви "
"дійсно хочете зрозуміти механізм *t*-тесту, то дуже корисно мати уявлення "
"про те, як насправді виглядає *t*-розподіл. Сподіваюся, ви зрозуміли суть."
